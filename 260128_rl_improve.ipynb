{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "5f6d8db1",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import geopandas as gpd"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "04eec354",
            "metadata": {},
            "source": [
                "# Process the data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "b2f4c053",
            "metadata": {},
            "outputs": [],
            "source": [
                "# move_dt = pd.read_csv(\"data/foood_mob_cover.csv\")\n",
                "# move_dt = move_dt.assign(\n",
                "#     who = (move_dt['quitaz_1'] * 8e3 + move_dt['quitaz_2']).astype(int),\n",
                "#     seq = move_dt['seiqd'],\n",
                "#     lon_o = (move_dt['mean_dur'] - 200)/1e3 + 114,\n",
                "#     lon_d = (move_dt['std_dur'] - 200)/1e3 + 114,\n",
                "#     lat_o = (move_dt['mean_volm'])/10 + 22.65,\n",
                "#     lat_d = (move_dt['std_volm'])/10 + 22.65,\n",
                "#     date = move_dt['date'],\n",
                "#     poi_o = move_dt['district_o'],\n",
                "#     poi_d = move_dt['district_d'],\n",
                "#     inplace=True\n",
                "# ) \n",
                "\n",
                "# select_columns = ['who', 'seq', 'lon_o', 'lat_o', 'lon_d', 'lat_d', 'date', 'poi_o', 'poi_d']\n",
                "# move_dt = move_dt[select_columns].copy()\n",
                "# # sort the values of the move_dt dataframe by who, date and seq\n",
                "# move_dt = move_dt.sort_values(by=['who', 'date', 'seq']).reset_index(drop=True)\n",
                "# move_dt.to_csv(\"data/processed_moves.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "91c38a45",
            "metadata": {},
            "outputs": [],
            "source": [
                "# move_dt = pd.read_csv(\"data/processed_moves.csv\")\n",
                "# persons = move_dt.who.drop_duplicates().reset_index(drop=True)\n",
                "# persons.to_csv(\"data/processed_users.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "66a551a9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# process stay data\n",
                "# read all the data as series, because they only contain one column\n",
                "t_start = pd.read_csv('data/st/reptoire.csv').iloc[:, 0]\n",
                "t_end = pd.read_csv('data/st/nif.csv').iloc[:, 0]\n",
                "ptype = pd.read_csv('data/st/model.csv').iloc[:, 0]\n",
                "poi = pd.read_csv('data/st/iop.csv').iloc[:, 0]\n",
                "who = pd.read_csv('data/st/est.csv').iloc[:, 0]\n",
                "date = pd.read_csv('data/st/aoz.csv').iloc[:, 0]\n",
                "lon_p1 = pd.read_csv('data/st/mean_log_p1.csv').iloc[:, 0]\n",
                "lon_p2 = pd.read_csv('data/st/mean_log_p2.csv').iloc[:, 0]\n",
                "lat_p1 = pd.read_csv('data/st/std_log_p1.csv').iloc[:, 0]\n",
                "lat_p2 = pd.read_csv('data/st/std_log_p2.csv').iloc[:, 0]\n",
                "\n",
                "# concatenate the longitude and latitude parts\n",
                "lon = pd.concat([lon_p1, lon_p2], ignore_index=True)\n",
                "lat = pd.concat([lat_p1, lat_p2], ignore_index=True)\n",
                "\n",
                "st_data = pd.DataFrame({\n",
                "    't_start': t_start,\n",
                "    't_end': t_end,\n",
                "    'ptype': ptype,\n",
                "    'poi': poi,\n",
                "    'who': who,\n",
                "    'date': date,\n",
                "    'lon': lon,\n",
                "    'lat': lat\n",
                "})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "61b9e834",
            "metadata": {},
            "outputs": [],
            "source": [
                "st_sample = st_data.assign(\n",
                "    # turn to the time stamp by adding the base time 1677600000\n",
                "    who = st_data['who'].astype(int),\n",
                "    date = st_data['date'].astype(int) + 20202020,\n",
                "    t_start = pd.to_datetime(st_data['t_start'] + 1677600000, unit='s'),\n",
                "    t_end = pd.to_datetime(st_data['t_end'] + 1677600000, unit='s'),\n",
                "    lon = st_data['lon'] / 4,\n",
                "    lat = st_data['lat'] * 4,\n",
                "    ptype = st_data['ptype'].astype(int),\n",
                "    poi = st_data['poi'].astype(int)\n",
                ")\n",
                "\n",
                "st_sample = st_sample[['who', 'date', 't_start', 't_end', 'lon', 'lat', 'ptype', 'poi']].\\\n",
                "    sort_values(by=['who', 'date', 't_start'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "a0f50edd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original data: 720427 records\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Cover\\AppData\\Local\\Temp\\ipykernel_9636\\438820480.py:81: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
                        "  st_processed = st_processed.groupby('who', group_keys=False).apply(process_single_person)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "After Step 2 (merge consecutive stays): 652777 records\n",
                        "After Step 3 (filter short stays): 575138 records\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Cover\\AppData\\Local\\Temp\\ipykernel_9636\\438820480.py:89: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
                        "  st_processed = st_processed.groupby('who', group_keys=False).apply(process_single_person)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "After Step 4 (merge again): 567555 records\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>who</th>\n",
                            "      <th>t_start</th>\n",
                            "      <th>t_end</th>\n",
                            "      <th>lon</th>\n",
                            "      <th>lat</th>\n",
                            "      <th>ptype</th>\n",
                            "      <th>poi</th>\n",
                            "      <th>date</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-01 00:28:58</td>\n",
                            "      <td>2019-01-01 10:11:49</td>\n",
                            "      <td>113.833530</td>\n",
                            "      <td>22.689639</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>20190101</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-01 10:19:15</td>\n",
                            "      <td>2019-01-01 17:13:30</td>\n",
                            "      <td>113.948257</td>\n",
                            "      <td>22.529631</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>20190101</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-01 17:26:58</td>\n",
                            "      <td>2019-01-01 18:47:28</td>\n",
                            "      <td>113.889444</td>\n",
                            "      <td>22.773309</td>\n",
                            "      <td>0</td>\n",
                            "      <td>14</td>\n",
                            "      <td>20190101</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-01 18:59:06</td>\n",
                            "      <td>2019-01-01 21:26:17</td>\n",
                            "      <td>113.867473</td>\n",
                            "      <td>22.573858</td>\n",
                            "      <td>0</td>\n",
                            "      <td>11</td>\n",
                            "      <td>20190101</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-02 10:05:07</td>\n",
                            "      <td>2019-01-02 19:41:13</td>\n",
                            "      <td>114.095561</td>\n",
                            "      <td>22.553550</td>\n",
                            "      <td>0</td>\n",
                            "      <td>2</td>\n",
                            "      <td>20190102</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>567550</th>\n",
                            "      <td>78623283</td>\n",
                            "      <td>2019-12-30 11:00:45</td>\n",
                            "      <td>2019-12-30 18:46:13</td>\n",
                            "      <td>113.912155</td>\n",
                            "      <td>22.534095</td>\n",
                            "      <td>2</td>\n",
                            "      <td>0</td>\n",
                            "      <td>20191230</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>567551</th>\n",
                            "      <td>78623283</td>\n",
                            "      <td>2019-12-30 19:58:40</td>\n",
                            "      <td>2019-12-30 23:15:45</td>\n",
                            "      <td>113.908330</td>\n",
                            "      <td>22.518993</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>20191230</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>567552</th>\n",
                            "      <td>78623283</td>\n",
                            "      <td>2019-12-31 09:02:22</td>\n",
                            "      <td>2019-12-31 18:30:06</td>\n",
                            "      <td>114.026052</td>\n",
                            "      <td>22.625212</td>\n",
                            "      <td>0</td>\n",
                            "      <td>9</td>\n",
                            "      <td>20191231</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>567553</th>\n",
                            "      <td>78623283</td>\n",
                            "      <td>2019-12-31 18:30:40</td>\n",
                            "      <td>2019-12-31 20:05:57</td>\n",
                            "      <td>113.945381</td>\n",
                            "      <td>22.556100</td>\n",
                            "      <td>2</td>\n",
                            "      <td>0</td>\n",
                            "      <td>20191231</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>567554</th>\n",
                            "      <td>78623283</td>\n",
                            "      <td>2019-12-31 20:10:26</td>\n",
                            "      <td>2019-12-31 23:43:04</td>\n",
                            "      <td>113.908330</td>\n",
                            "      <td>22.518993</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>20191231</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>567555 rows × 8 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "             who             t_start               t_end         lon  \\\n",
                            "0         126272 2019-01-01 00:28:58 2019-01-01 10:11:49  113.833530   \n",
                            "1         126272 2019-01-01 10:19:15 2019-01-01 17:13:30  113.948257   \n",
                            "2         126272 2019-01-01 17:26:58 2019-01-01 18:47:28  113.889444   \n",
                            "3         126272 2019-01-01 18:59:06 2019-01-01 21:26:17  113.867473   \n",
                            "4         126272 2019-01-02 10:05:07 2019-01-02 19:41:13  114.095561   \n",
                            "...          ...                 ...                 ...         ...   \n",
                            "567550  78623283 2019-12-30 11:00:45 2019-12-30 18:46:13  113.912155   \n",
                            "567551  78623283 2019-12-30 19:58:40 2019-12-30 23:15:45  113.908330   \n",
                            "567552  78623283 2019-12-31 09:02:22 2019-12-31 18:30:06  114.026052   \n",
                            "567553  78623283 2019-12-31 18:30:40 2019-12-31 20:05:57  113.945381   \n",
                            "567554  78623283 2019-12-31 20:10:26 2019-12-31 23:43:04  113.908330   \n",
                            "\n",
                            "              lat  ptype  poi      date  \n",
                            "0       22.689639      1    0  20190101  \n",
                            "1       22.529631      0    3  20190101  \n",
                            "2       22.773309      0   14  20190101  \n",
                            "3       22.573858      0   11  20190101  \n",
                            "4       22.553550      0    2  20190102  \n",
                            "...           ...    ...  ...       ...  \n",
                            "567550  22.534095      2    0  20191230  \n",
                            "567551  22.518993      1    1  20191230  \n",
                            "567552  22.625212      0    9  20191231  \n",
                            "567553  22.556100      2    0  20191231  \n",
                            "567554  22.518993      1    1  20191231  \n",
                            "\n",
                            "[567555 rows x 8 columns]"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# ====== Stay Record Processing Algorithm ======\n",
                "def merge_consecutive_stays(df, gap_minutes=30):\n",
                "    \"\"\"\n",
                "    Merge consecutive stay records with the same location and gap <= gap_minutes\n",
                "    OR same location AND adjacent calendar dates\n",
                "    \n",
                "    Parameters:\n",
                "    - df: DataFrame sorted by who and t_start\n",
                "    - gap_minutes: Maximum gap threshold (minutes)\n",
                "    \n",
                "    Returns:\n",
                "    - Merged DataFrame\n",
                "    \"\"\"\n",
                "    if len(df) == 0:\n",
                "        return df.copy()\n",
                "    \n",
                "    # Calculate time gap to next record (in minutes)\n",
                "    time_diff = (df['t_start'].shift(-1) - df['t_end']).dt.total_seconds() / 60\n",
                "    \n",
                "    # Check if location changed (use np.isclose for floating-point comparison)\n",
                "    lon_same = np.isclose(df['lon'].shift(-1), df['lon'], rtol=1e-9)\n",
                "    lat_same = np.isclose(df['lat'].shift(-1), df['lat'], rtol=1e-9)\n",
                "    pos_same = lon_same & lat_same\n",
                "    \n",
                "    # Check if dates are adjacent\n",
                "    date_adjacent = (df['date'].shift(-1) - df['date']) == 1\n",
                "    \n",
                "    # Determine if merge is needed:\n",
                "    # Option 1: same location AND time gap <= threshold\n",
                "    # Option 2: same location AND adjacent calendar dates\n",
                "    need_merge = pos_same & ((time_diff <= gap_minutes) | date_adjacent)\n",
                "    need_merge.iloc[-1] = False  # Last record doesn't need merge\n",
                "    \n",
                "    # Create group identifier: new group whenever merge is not needed\n",
                "    group_id = (~need_merge).cumsum()\n",
                "    \n",
                "    # Aggregate by group\n",
                "    # Add group_id as a column first to avoid FutureWarning\n",
                "    df = df.copy()\n",
                "    df['group_id'] = group_id\n",
                "    merged = df.groupby(['who', 'group_id'], as_index=False).agg({\n",
                "        't_start': 'min',\n",
                "        't_end': 'max',\n",
                "        'lon': 'first',\n",
                "        'lat': 'first',\n",
                "        'ptype': 'first',\n",
                "        'poi': 'first',\n",
                "        'date': 'first'\n",
                "    })\n",
                "    merged = merged.drop(columns=['group_id'])\n",
                "    \n",
                "    return merged\n",
                "\n",
                "\n",
                "def filter_short_stays(df, min_minutes=30):\n",
                "    \"\"\"\n",
                "    Filter out stay records shorter than min_minutes\n",
                "    \n",
                "    Parameters:\n",
                "    - df: DataFrame\n",
                "    - min_minutes: Minimum stay duration (minutes)\n",
                "    \n",
                "    Returns:\n",
                "    - Filtered DataFrame\n",
                "    \"\"\"\n",
                "    stay_duration = (df['t_end'] - df['t_start']).dt.total_seconds() / 60\n",
                "    return df[stay_duration >= min_minutes].reset_index(drop=True)\n",
                "\n",
                "\n",
                "# ====== Processing Pipeline ======\n",
                "print(f\"Original data: {len(st_sample)} records\")\n",
                "\n",
                "# Step 1: Group by who (data is already sorted by who, date, t_start)\n",
                "st_processed = st_sample.copy()\n",
                "\n",
                "# Step 2: Merge consecutive stays for each individual\n",
                "def process_single_person(group):\n",
                "    \"\"\"Process data for a single individual\"\"\"\n",
                "    return merge_consecutive_stays(group, gap_minutes=30)\n",
                "\n",
                "st_processed = st_processed.groupby('who', group_keys=False).apply(process_single_person)\n",
                "print(f\"After Step 2 (merge consecutive stays): {len(st_processed)} records\")\n",
                "\n",
                "# Step 3: Filter out stays shorter than 30 minutes\n",
                "st_processed = filter_short_stays(st_processed, min_minutes=30)\n",
                "print(f\"After Step 3 (filter short stays): {len(st_processed)} records\")\n",
                "\n",
                "# Step 4: Merge again\n",
                "st_processed = st_processed.groupby('who', group_keys=False).apply(process_single_person)\n",
                "print(f\"After Step 4 (merge again): {len(st_processed)} records\")\n",
                "\n",
                "# Re-sort\n",
                "st_processed = st_processed.sort_values(by=['who', 't_start']).reset_index(drop=True)\n",
                "\n",
                "st_processed"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "39a91e21",
            "metadata": {},
            "source": [
                "# Format the trajectories"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "8b74fde9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Applying random jitter (300m) and HDBSCAN clustering...\n",
                        "Applying random jitter (300m) and HDBSCAN clustering...\n",
                        "Total users: 441, Total records: 567555\n",
                        "  User 395753: 1 missing, 1199 valid coordinates\n",
                        "  User 2436270: 5 missing, 1385 valid coordinates\n",
                        "  User 2590264: 3 missing, 1588 valid coordinates\n",
                        "  User 3087457: 2 missing, 1023 valid coordinates\n",
                        "  User 3549549: 2 missing, 1409 valid coordinates\n",
                        "  User 3803542: 1 missing, 1028 valid coordinates\n",
                        "  User 3932007: 1 missing, 1169 valid coordinates\n",
                        "  User 4717957: 1 missing, 751 valid coordinates\n",
                        "  User 4975553: 1 missing, 1271 valid coordinates\n",
                        "  User 6071595: 3 missing, 1640 valid coordinates\n",
                        "  User 6854307: 1 missing, 1322 valid coordinates\n",
                        "  User 7477809: 5 missing, 2034 valid coordinates\n",
                        "  User 7823042: 1 missing, 1262 valid coordinates\n",
                        "  User 8614296: 2 missing, 1388 valid coordinates\n",
                        "  User 12521378: 7 missing, 1696 valid coordinates\n",
                        "  User 13976993: 3 missing, 1176 valid coordinates\n",
                        "  User 14921919: 1 missing, 1372 valid coordinates\n",
                        "  User 16983956: 5 missing, 1192 valid coordinates\n",
                        "  User 17669488: 2 missing, 1230 valid coordinates\n",
                        "  User 18409644: 2 missing, 1630 valid coordinates\n",
                        "  User 34920163: 1 missing, 1580 valid coordinates\n",
                        "  User 35401357: 3 missing, 1100 valid coordinates\n",
                        "  User 36531062: 4 missing, 1333 valid coordinates\n",
                        "  User 36980397: 2 missing, 1702 valid coordinates\n",
                        "  User 38588760: 2 missing, 1452 valid coordinates\n",
                        "  User 38622107: 1 missing, 1420 valid coordinates\n",
                        "  User 39317715: 7 missing, 1636 valid coordinates\n",
                        "  User 39557557: 1 missing, 1168 valid coordinates\n",
                        "  User 40046477: 3 missing, 1660 valid coordinates\n",
                        "  User 40073552: 1 missing, 2056 valid coordinates\n",
                        "  User 50423252: 1 missing, 1488 valid coordinates\n",
                        "  User 51169921: 1 missing, 1541 valid coordinates\n",
                        "  User 52655018: 1 missing, 1329 valid coordinates\n",
                        "  User 63435990: 2 missing, 1678 valid coordinates\n",
                        "  User 63449372: 3 missing, 1592 valid coordinates\n",
                        "  User 64988739: 3 missing, 1507 valid coordinates\n",
                        "  User 65332038: 10 missing, 1550 valid coordinates\n",
                        "  User 65731195: 6 missing, 1880 valid coordinates\n",
                        "  User 67410950: 5 missing, 1663 valid coordinates\n",
                        "  User 67590697: 1 missing, 1283 valid coordinates\n",
                        "  User 68545896: 1 missing, 1172 valid coordinates\n",
                        "  User 68790880: 4 missing, 1524 valid coordinates\n",
                        "  User 75391906: 2 missing, 1295 valid coordinates\n",
                        "  User 75728232: 4 missing, 1544 valid coordinates\n",
                        "  User 76087683: 4 missing, 1130 valid coordinates\n",
                        "  User 76635485: 2 missing, 1722 valid coordinates\n",
                        "  User 76828832: 2 missing, 1533 valid coordinates\n",
                        "  User 78315489: 1 missing, 1437 valid coordinates\n",
                        "  User 78396593: 5 missing, 1673 valid coordinates\n",
                        "  User 78594883: 2 missing, 1204 valid coordinates\n",
                        "\n",
                        "Clustering complete!\n",
                        "Total records: 567555\n",
                        "Missing coordinates (cluster_id=0): 134\n",
                        "HDBSCAN noise (cluster_id=-1): 185565\n",
                        "Records in clusters (cluster_id>=1): 381856\n",
                        "Number of unique clusters: 65\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Cover\\AppData\\Local\\Temp\\ipykernel_9636\\1787329715.py:132: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
                        "  df_clustered = df.groupby('who', group_keys=False).apply(cluster_one_person)\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>who</th>\n",
                            "      <th>t_start</th>\n",
                            "      <th>t_end</th>\n",
                            "      <th>lon</th>\n",
                            "      <th>lat</th>\n",
                            "      <th>ptype</th>\n",
                            "      <th>poi</th>\n",
                            "      <th>date</th>\n",
                            "      <th>cluster_id</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-01 00:28:58</td>\n",
                            "      <td>2019-01-01 10:11:49</td>\n",
                            "      <td>113.834643</td>\n",
                            "      <td>22.690710</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>20190101</td>\n",
                            "      <td>3</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-01 10:19:15</td>\n",
                            "      <td>2019-01-01 17:13:30</td>\n",
                            "      <td>113.948529</td>\n",
                            "      <td>22.530949</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>20190101</td>\n",
                            "      <td>20</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-01 17:26:58</td>\n",
                            "      <td>2019-01-01 18:47:28</td>\n",
                            "      <td>113.889141</td>\n",
                            "      <td>22.773277</td>\n",
                            "      <td>0</td>\n",
                            "      <td>14</td>\n",
                            "      <td>20190101</td>\n",
                            "      <td>9</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-01 18:59:06</td>\n",
                            "      <td>2019-01-01 21:26:17</td>\n",
                            "      <td>113.866880</td>\n",
                            "      <td>22.573279</td>\n",
                            "      <td>0</td>\n",
                            "      <td>11</td>\n",
                            "      <td>20190101</td>\n",
                            "      <td>-1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-02 10:05:07</td>\n",
                            "      <td>2019-01-02 19:41:13</td>\n",
                            "      <td>114.098086</td>\n",
                            "      <td>22.554694</td>\n",
                            "      <td>0</td>\n",
                            "      <td>2</td>\n",
                            "      <td>20190102</td>\n",
                            "      <td>26</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-02 19:56:04</td>\n",
                            "      <td>2019-01-02 22:37:02</td>\n",
                            "      <td>114.097994</td>\n",
                            "      <td>22.598955</td>\n",
                            "      <td>0</td>\n",
                            "      <td>9</td>\n",
                            "      <td>20190102</td>\n",
                            "      <td>10</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-03 00:46:34</td>\n",
                            "      <td>2019-01-03 07:51:17</td>\n",
                            "      <td>113.831582</td>\n",
                            "      <td>22.688753</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>20190103</td>\n",
                            "      <td>3</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-03 08:11:55</td>\n",
                            "      <td>2019-01-03 14:11:15</td>\n",
                            "      <td>113.842961</td>\n",
                            "      <td>22.607576</td>\n",
                            "      <td>2</td>\n",
                            "      <td>1</td>\n",
                            "      <td>20190103</td>\n",
                            "      <td>24</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-03 14:11:19</td>\n",
                            "      <td>2019-01-03 16:40:00</td>\n",
                            "      <td>113.829167</td>\n",
                            "      <td>22.737915</td>\n",
                            "      <td>0</td>\n",
                            "      <td>10</td>\n",
                            "      <td>20190103</td>\n",
                            "      <td>-1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>126272</td>\n",
                            "      <td>2019-01-03 16:53:00</td>\n",
                            "      <td>2019-01-03 21:15:07</td>\n",
                            "      <td>114.023939</td>\n",
                            "      <td>22.531267</td>\n",
                            "      <td>0</td>\n",
                            "      <td>2</td>\n",
                            "      <td>20190103</td>\n",
                            "      <td>22</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "      who             t_start               t_end         lon        lat  \\\n",
                            "0  126272 2019-01-01 00:28:58 2019-01-01 10:11:49  113.834643  22.690710   \n",
                            "1  126272 2019-01-01 10:19:15 2019-01-01 17:13:30  113.948529  22.530949   \n",
                            "2  126272 2019-01-01 17:26:58 2019-01-01 18:47:28  113.889141  22.773277   \n",
                            "3  126272 2019-01-01 18:59:06 2019-01-01 21:26:17  113.866880  22.573279   \n",
                            "4  126272 2019-01-02 10:05:07 2019-01-02 19:41:13  114.098086  22.554694   \n",
                            "5  126272 2019-01-02 19:56:04 2019-01-02 22:37:02  114.097994  22.598955   \n",
                            "6  126272 2019-01-03 00:46:34 2019-01-03 07:51:17  113.831582  22.688753   \n",
                            "7  126272 2019-01-03 08:11:55 2019-01-03 14:11:15  113.842961  22.607576   \n",
                            "8  126272 2019-01-03 14:11:19 2019-01-03 16:40:00  113.829167  22.737915   \n",
                            "9  126272 2019-01-03 16:53:00 2019-01-03 21:15:07  114.023939  22.531267   \n",
                            "\n",
                            "   ptype  poi      date  cluster_id  \n",
                            "0      1    0  20190101           3  \n",
                            "1      0    3  20190101          20  \n",
                            "2      0   14  20190101           9  \n",
                            "3      0   11  20190101          -1  \n",
                            "4      0    2  20190102          26  \n",
                            "5      0    9  20190102          10  \n",
                            "6      1    0  20190103           3  \n",
                            "7      2    1  20190103          24  \n",
                            "8      0   10  20190103          -1  \n",
                            "9      0    2  20190103          22  "
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# ====== Step 1 & 2: Random Jitter & HDBSCAN Clustering ======\n",
                "import hdbscan\n",
                "from typing import Optional\n",
                "import plotly.express as px\n",
                "from pyproj import Transformer\n",
                "from shapely.geometry import Point\n",
                "\n",
                "# Random jitter function: uniform random offset within specified radius (area-uniform)\n",
                "def jitter_within_radius(xy: np.ndarray, max_radius_m: float, rng: np.random.Generator) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Apply uniform random jitter within a circle of specified radius.\n",
                "    Uses sqrt(r) for radius to ensure uniform area distribution.\n",
                "    \"\"\"\n",
                "    # Check: the dimension of the input array should be 2\n",
                "    if xy.shape[1] != 2:\n",
                "        raise ValueError(\"Input array should have exactly 2 columns (x, y)\")\n",
                "    n = xy.shape[0]\n",
                "    angles = rng.uniform(0.0, 2.0 * np.pi, size=n)\n",
                "    radii = max_radius_m * np.sqrt(rng.uniform(0.0, 1.0, size=n))\n",
                "    offsets = np.column_stack([radii * np.cos(angles), radii * np.sin(angles)])\n",
                "    return xy + offsets\n",
                "\n",
                "\n",
                "def add_jitter_and_cluster(\n",
                "    df: pd.DataFrame, \n",
                "    jitter_radius_m: float = 300, \n",
                "    min_cluster_size: int = 6, \n",
                "    min_samples: int = 6,\n",
                "    seed: int = 114514\n",
                ") -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Apply 300m random jitter to coordinates, then perform HDBSCAN clustering.\n",
                "    \n",
                "    This function:\n",
                "    1. Converts lon/lat to UTM coordinates (EPSG:32650)\n",
                "    2. Applies random jitter within specified radius\n",
                "    3. Converts back to WGS84 lon/lat (updating original coordinates)\n",
                "    4. Performs HDBSCAN clustering on the jittered coordinates\n",
                "    \n",
                "    Cluster labels:\n",
                "    - 0: Missing coordinates (outside study area)\n",
                "    - -1: HDBSCAN noise points\n",
                "    - 1+: HDBSCAN clusters (original HDBSCAN labels shifted by +1)\n",
                "    \n",
                "    Parameters:\n",
                "    - df: DataFrame with 'who', 'lon', 'lat' columns\n",
                "    - jitter_radius_m: Radius for random jitter in meters\n",
                "    - min_cluster_size: HDBSCAN parameter\n",
                "    - min_samples: HDBSCAN parameter\n",
                "    - seed: Random seed for reproducibility\n",
                "    \n",
                "    Returns:\n",
                "    - DataFrame with additional 'cluster_id' column (0 = missing, -1 = noise, 1+ = clusters)\n",
                "    \"\"\"\n",
                "    RNG = np.random.default_rng(seed)\n",
                "    df = df.copy()\n",
                "    \n",
                "    def cluster_one_person(person_df: pd.DataFrame) -> pd.DataFrame:\n",
                "        \"\"\"Cluster stays for a single person using HDBSCAN.\n",
                "        \n",
                "        Returns:\n",
                "        - cluster_id = 0: missing coordinates (outside study area)\n",
                "        - cluster_id = -1: HDBSCAN noise\n",
                "        - cluster_id = 1+: HDBSCAN clusters (original label + 1)\n",
                "        \"\"\"\n",
                "        person_df = person_df.copy()\n",
                "        \n",
                "        # Check for missing coordinates\n",
                "        has_coords = ~person_df[['lon', 'lat']].isna().any(axis=1)\n",
                "        valid_mask = has_coords.values\n",
                "        valid_count = valid_mask.sum()\n",
                "        total_count = len(person_df)\n",
                "        \n",
                "        # Initialize all cluster_ids to 0 (missing coordinates)\n",
                "        person_df['cluster_id'] = 0\n",
                "        \n",
                "        # If no valid coordinates at all, skip clustering\n",
                "        if valid_count == 0:\n",
                "            print(f\"  User {person_df['who'].iloc[0]}: All {total_count} records have missing coordinates, skipping clustering\")\n",
                "            return person_df\n",
                "        \n",
                "        # If some records have missing coordinates, report stats\n",
                "        if valid_count < total_count:\n",
                "            missing_count = total_count - valid_count\n",
                "            print(f\"  User {person_df['who'].iloc[0]}: {missing_count} missing, {valid_count} valid coordinates\")\n",
                "        \n",
                "        # Get valid records\n",
                "        valid_indices = person_df[has_coords].index\n",
                "        valid_lon = person_df.loc[valid_indices, 'lon'].values.copy()\n",
                "        valid_lat = person_df.loc[valid_indices, 'lat'].values.copy()\n",
                "        \n",
                "        # UTM conversion (EPSG:32650 = UTM Zone 50N, covers Shenzhen)\n",
                "        transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32650\", always_xy=True)\n",
                "        inverse_transformer = Transformer.from_crs(\"EPSG:32650\", \"EPSG:4326\", always_xy=True)\n",
                "        \n",
                "        # Convert lon/lat to UTM coordinates (meters)\n",
                "        utm_x, utm_y = transformer.transform(valid_lon, valid_lat)\n",
                "        \n",
                "        # Apply random jitter to UTM coordinates\n",
                "        xy_valid = np.column_stack([utm_x, utm_y])\n",
                "        xy_jittered = jitter_within_radius(xy_valid, max_radius_m=jitter_radius_m, rng=RNG)\n",
                "        \n",
                "        # Convert back to WGS84 lon/lat\n",
                "        jittered_lon, jittered_lat = inverse_transformer.transform(xy_jittered[:, 0], xy_jittered[:, 1])\n",
                "        \n",
                "        # Update original lon/lat with jittered coordinates\n",
                "        person_df.loc[valid_indices, 'lon'] = jittered_lon\n",
                "        person_df.loc[valid_indices, 'lat'] = jittered_lat\n",
                "        \n",
                "        # HDBSCAN clustering\n",
                "        clusterer = hdbscan.HDBSCAN(\n",
                "            min_cluster_size=min_cluster_size, \n",
                "            min_samples=min_samples,\n",
                "            metric='euclidean'\n",
                "        )\n",
                "        labels_valid = clusterer.fit_predict(xy_jittered)\n",
                "        \n",
                "        # Shift labels for valid records:\n",
                "        # HDBSCAN returns -1 for noise, 0, 1, 2... for clusters\n",
                "        # We want to keep -1 as noise, shift 0闂?, 1闂?, 2闂?...\n",
                "        # So we add 1 to non-noise labels only\n",
                "        labels_shifted = np.where(labels_valid == -1, -1, labels_valid + 1)\n",
                "        \n",
                "        # Assign shifted labels back to valid records\n",
                "        person_df.loc[valid_indices, 'cluster_id'] = labels_shifted\n",
                "        \n",
                "        return person_df\n",
                "    \n",
                "    # Group by 'who' and apply clustering\n",
                "    print(\"Applying random jitter (300m) and HDBSCAN clustering...\")\n",
                "    print(f\"Total users: {df['who'].nunique()}, Total records: {len(df)}\")\n",
                "    df_clustered = df.groupby('who', group_keys=False).apply(cluster_one_person)\n",
                "    \n",
                "    return df_clustered\n",
                "\n",
                "\n",
                "# Apply clustering to processed data\n",
                "print(\"Applying random jitter (300m) and HDBSCAN clustering...\")\n",
                "st_clustered = add_jitter_and_cluster(\n",
                "    st_processed, \n",
                "    jitter_radius_m=300, \n",
                "    min_cluster_size=6, \n",
                "    min_samples=10\n",
                ")\n",
                "\n",
                "# Display clustering results\n",
                "print(f\"\\nClustering complete!\")\n",
                "print(f\"Total records: {len(st_clustered)}\")\n",
                "print(f\"Missing coordinates (cluster_id=0): {(st_clustered['cluster_id'] == 0).sum()}\")\n",
                "print(f\"HDBSCAN noise (cluster_id=-1): {(st_clustered['cluster_id'] == -1).sum()}\")\n",
                "print(f\"Records in clusters (cluster_id>=1): {(st_clustered['cluster_id'] >= 1).sum()}\")\n",
                "print(f\"Number of unique clusters: {st_clustered[st_clustered['cluster_id'] >= 1]['cluster_id'].nunique()}\")\n",
                "\n",
                "st_clustered.head(10)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4e8421f7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ====== Step 3: Interactive Map Visualization ======\n",
                "def visualize_user_clusters(\n",
                "    df: pd.DataFrame, \n",
                "    user_id: int, \n",
                "    zoom: int = 12,\n",
                "    height: int = 600\n",
                ") -> px.scatter_mapbox:\n",
                "    \"\"\"\n",
                "    Create an interactive map showing cluster locations for a specific user.\n",
                "    \n",
                "    Parameters:\n",
                "    - df: DataFrame with 'who', 'lon', 'lat', 'cluster_id' columns\n",
                "    - user_id: User ID to visualize\n",
                "    - zoom: Initial zoom level\n",
                "    - height: Map height in pixels\n",
                "    \n",
                "    Returns:\n",
                "    - Plotly Express scatter_mapbox figure\n",
                "    \"\"\"\n",
                "    user_data = df[df['who'] == user_id].copy()\n",
                "    \n",
                "    if user_data.empty:\n",
                "        raise ValueError(f\"No data found for user {user_id}\")\n",
                "    \n",
                "    # Add cluster label for hover (missing vs noise vs cluster)\n",
                "    # cluster_id = 0: Missing coordinates (outside study area)\n",
                "    # cluster_id = -1: HDBSCAN noise\n",
                "    # cluster_id = 1+: HDBSCAN clusters\n",
                "    user_data['cluster_label'] = user_data['cluster_id'].apply(\n",
                "        lambda x: f\"Missing (Outside Study Area)\" if x == 0 \n",
                "                  else (f\"HDBSCAN Noise\" if x == -1 \n",
                "                  else f\"Cluster {x}\")\n",
                "    )\n",
                "    \n",
                "    # Define custom colors: missing=red, noise=gray, clusters=auto\n",
                "    color_discrete_map = {\n",
                "        'Missing (Outside Study Area)': 'red',\n",
                "        'HDBSCAN Noise': 'gray',\n",
                "    }\n",
                "    \n",
                "    # Get unique cluster labels for ordering\n",
                "    cluster_labels = [f'Cluster {i}' for i in sorted(user_data[user_data['cluster_id'] >= 1]['cluster_id'].unique())]\n",
                "    \n",
                "    # Create scatter mapbox with custom colors\n",
                "    fig = px.scatter_mapbox(\n",
                "        user_data,\n",
                "        lat='lat',\n",
                "        lon='lon',\n",
                "        color='cluster_label',\n",
                "        color_discrete_map=color_discrete_map,\n",
                "        category_orders={'cluster_label': ['Missing (Outside Study Area)', 'HDBSCAN Noise'] + cluster_labels},\n",
                "        hover_data={\n",
                "            'who': True,\n",
                "            't_start': True,\n",
                "            't_end': True,\n",
                "            'cluster_id': True,\n",
                "            'lat': ':.5f',\n",
                "            'lon': ':.5f'\n",
                "        },\n",
                "        title=f\"Stay locations for User {user_id}\",\n",
                "        zoom=zoom,\n",
                "        height=height,\n",
                "        size_max=15\n",
                "    )\n",
                "    \n",
                "    fig.update_layout(mapbox_style=\"carto-positron\")\n",
                "    fig.update_layout(margin={\"r\": 0, \"t\": 40, \"l\": 0, \"b\": 0})\n",
                "    \n",
                "    return fig\n",
                "\n",
                "\n",
                "# Example: Visualize clusters for a specific user\n",
                "# Change user_id to visualize different users\n",
                "inspect_user = st_clustered['who'].iloc[0]  # First user as example\n",
                "print(f\"Visualizing clusters for user {inspect_user}...\")\n",
                "fig = visualize_user_clusters(st_clustered, user_id=inspect_user, zoom=12, height=500)\n",
                "fig.show()\n",
                "\n",
                "# Visualize summary statistics per user\n",
                "# New cluster_id meanings: 0 = missing, -1 = noise, 1+ = clusters\n",
                "print(\"\\nCluster statistics per user:\")\n",
                "user_stats = st_clustered.groupby('who').agg({\n",
                "    'cluster_id': [\n",
                "        'count',                                          # total_stays\n",
                "        lambda x: (x == 0).sum(),                         # missing_stays\n",
                "        lambda x: (x != 0).sum(),                          # valid_stays (not missing)\n",
                "        lambda x: (x == -1).sum(),                         # noise_stays\n",
                "        lambda x: (x >= 1).sum(),                          # clustered_stays\n",
                "        lambda x: (x[x >= 1].nunique() if (x >= 1).any() else 0)  # num_clusters\n",
                "    ]\n",
                "}).reset_index()\n",
                "user_stats.columns = ['who', 'total_stays', 'missing_stays', 'valid_stays', 'noise_stays', 'clustered_stays', 'num_clusters']\n",
                "print(user_stats.head(10))\n",
                "\n",
                "# Summary\n",
                "print(f\"\\n=== Overall Statistics ===\")\n",
                "print(f\"Total records: {len(st_clustered)}\")\n",
                "print(f\"Missing coordinates (cluster_id=0): {(st_clustered['cluster_id'] == 0).sum()}\")\n",
                "print(f\"Valid coordinates: {(st_clustered['cluster_id'] != 0).sum()}\")\n",
                "print(f\"  - HDBSCAN noise (cluster_id=-1): {(st_clustered['cluster_id'] == -1).sum()}\")\n",
                "print(f\"  - In clusters (cluster_id>=1): {(st_clustered['cluster_id'] >= 1).sum()}\")\n",
                "print(f\"Number of unique clusters: {st_clustered[st_clustered['cluster_id'] >= 1]['cluster_id'].nunique()}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "e97b6d5d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Converting DataFrame to User Model...\n",
                        "============================================================\n",
                        "Converting DataFrame with 567555 records to User objects...\n",
                        "Created 441 User objects\n",
                        "\n",
                        "Sample users:\n",
                        "\n",
                        "User(id=126272, trajectories=350, total_visits=1320)\n",
                        "  - Unique clusters visited: 33\n",
                        "  - Date range: 20181231 to 20191231\n",
                        "\n",
                        "User(id=278978, trajectories=346, total_visits=1182)\n",
                        "  - Unique clusters visited: 25\n",
                        "  - Date range: 20181231 to 20191231\n",
                        "\n",
                        "User(id=395753, trajectories=360, total_visits=1537)\n",
                        "  - Unique clusters visited: 36\n",
                        "  - Date range: 20181231 to 20191231\n",
                        "\n",
                        "Verification: Converting User objects back to DataFrame...\n",
                        "User 126272 converted back to DataFrame: 1320 records\n",
                        "      who      date             t_start               t_end         lon  \\\n",
                        "0  126272  20181231 2019-01-01 00:28:58 2019-01-01 03:00:00  113.834643   \n",
                        "1  126272  20190101 2019-01-01 03:00:00 2019-01-01 10:11:49  113.834643   \n",
                        "2  126272  20190101 2019-01-01 10:19:15 2019-01-01 17:13:30  113.948529   \n",
                        "3  126272  20190101 2019-01-01 17:26:58 2019-01-01 18:47:28  113.889141   \n",
                        "4  126272  20190101 2019-01-01 18:59:06 2019-01-01 21:26:17  113.866880   \n",
                        "\n",
                        "         lat  cluster_id  ptype  poi  \n",
                        "0  22.690710           3      1    0  \n",
                        "1  22.690710           3      1    0  \n",
                        "2  22.530949          20      0    3  \n",
                        "3  22.773277           9      0   14  \n",
                        "4  22.573279          -1      0   11  \n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# ====== Step 4 & 5: OOP Data Models & Conversion ======\n",
                "from dataclasses import dataclass, field\n",
                "from typing import List, Dict, Optional\n",
                "from datetime import datetime, timedelta\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class Visit:\n",
                "    \"\"\"\n",
                "    Represents a single stay/visit at a location.\n",
                "    \n",
                "    Attributes:\n",
                "    - t_start: Start timestamp of the visit\n",
                "    - t_end: End timestamp of the visit\n",
                "    - lon: Longitude of the location\n",
                "    - lat: Latitude of the location\n",
                "    - cluster_id: Cluster label (0 = missing/outside study area, -1 = HDBSCAN noise, 1+ = HDBSCAN clusters)\n",
                "    - ptype: Place type (optional)\n",
                "    - poi: POI information (optional)\n",
                "    \"\"\"\n",
                "    t_start: datetime\n",
                "    t_end: datetime\n",
                "    lon: float\n",
                "    lat: float\n",
                "    cluster_id: int = 0  # 0 = missing, -1 = noise, 1+ = clusters\n",
                "    ptype: Optional[int] = None\n",
                "    poi: Optional[int] = None\n",
                "    \n",
                "    @property\n",
                "    def duration(self) -> timedelta:\n",
                "        \"\"\"Calculate duration of the visit.\"\"\"\n",
                "        return self.t_end - self.t_start\n",
                "    \n",
                "    @property\n",
                "    def date(self) -> int:\n",
                "        \"\"\"Extract date in YYYYMMDD format from t_start.\"\"\"\n",
                "        return int(self.t_start.strftime('%Y%m%d'))\n",
                "    \n",
                "    @classmethod\n",
                "    def from_dict(cls, data: dict) -> 'Visit':\n",
                "        \"\"\"Create Visit from dictionary.\"\"\"\n",
                "        return cls(\n",
                "            t_start=pd.to_datetime(data['t_start']),\n",
                "            t_end=pd.to_datetime(data['t_end']),\n",
                "            lon=float(data['lon']),\n",
                "            lat=float(data['lat']),\n",
                "            cluster_id=int(data.get('cluster_id', -1)),\n",
                "            ptype=int(data['ptype']) if pd.notna(data.get('ptype')) else None,\n",
                "            poi=int(data['poi']) if pd.notna(data.get('poi')) else None\n",
                "        )\n",
                "    \n",
                "    def to_dict(self) -> dict:\n",
                "        \"\"\"Convert Visit to dictionary.\"\"\"\n",
                "        return {\n",
                "            't_start': self.t_start,\n",
                "            't_end': self.t_end,\n",
                "            'lon': self.lon,\n",
                "            'lat': self.lat,\n",
                "            'cluster_id': self.cluster_id,\n",
                "            'ptype': self.ptype,\n",
                "            'poi': self.poi\n",
                "        }\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class Trajectory:\n",
                "    \"\"\"\n",
                "    Represents a daily trajectory (from 3:00 AM to next day 3:00 AM).\n",
                "    \n",
                "    Attributes:\n",
                "    - date: Date of the trajectory (YYYYMMDD format, representing the day starting at 3 AM)\n",
                "    - visits: List of Visit objects\n",
                "    \"\"\"\n",
                "    date: int\n",
                "    visits: List[Visit] = field(default_factory=list)\n",
                "    \n",
                "    @property\n",
                "    def num_visits(self) -> int:\n",
                "        \"\"\"Return number of visits.\"\"\"\n",
                "        return len(self.visits)\n",
                "    \n",
                "    @property\n",
                "    def start_time(self) -> Optional[datetime]:\n",
                "        \"\"\"Return start time of first visit.\"\"\"\n",
                "        return self.visits[0].t_start if self.visits else None\n",
                "    \n",
                "    @property\n",
                "    def end_time(self) -> Optional[datetime]:\n",
                "        \"\"\"Return end time of last visit.\"\"\"\n",
                "        return self.visits[-1].t_end if self.visits else None\n",
                "    \n",
                "    def add_visit(self, visit: Visit):\n",
                "        \"\"\"Add a visit to the trajectory.\"\"\"\n",
                "        self.visits.append(visit)\n",
                "        # Keep visits sorted by start time\n",
                "        self.visits.sort(key=lambda v: v.t_start)\n",
                "    \n",
                "    def to_dict(self) -> dict:\n",
                "        \"\"\"Convert trajectory to dictionary.\"\"\"\n",
                "        return {\n",
                "            'date': self.date,\n",
                "            'visits': [v.to_dict() for v in self.visits]\n",
                "        }\n",
                "\n",
                "\n",
                "class User:\n",
                "    \"\"\"\n",
                "    Represents a user with their trajectories and memory.\n",
                "    \n",
                "    Attributes:\n",
                "    - id: User identifier\n",
                "    - trajectories: Dictionary mapping date to Trajectory\n",
                "    - memory: User's memory (to be implemented)\n",
                "    \"\"\"\n",
                "    \n",
                "    # Reference time: 3:00 AM threshold for day boundaries\n",
                "    DAY_THRESHOLD_HOUR = 3\n",
                "    \n",
                "    def __init__(self, id: int):\n",
                "        \"\"\"\n",
                "        Initialize a User.\n",
                "        \n",
                "        Parameters:\n",
                "        - id: User identifier\n",
                "        \"\"\"\n",
                "        self.id = id\n",
                "        self.trajectories: Dict[int, Trajectory] = {}\n",
                "        self.memory: Optional[Dict] = None\n",
                "    \n",
                "    @property\n",
                "    def num_trajectories(self) -> int:\n",
                "        \"\"\"Return number of trajectories.\"\"\"\n",
                "        return len(self.trajectories)\n",
                "    \n",
                "    @property\n",
                "    def total_visits(self) -> int:\n",
                "        \"\"\"Return total number of visits across all trajectories.\"\"\"\n",
                "        return sum(t.num_visits for t in self.trajectories.values())\n",
                "    \n",
                "    @property\n",
                "    def unique_clusters(self) -> set:\n",
                "        \"\"\"Return set of unique cluster IDs visited by this user.\n",
                "        \n",
                "        Note: cluster_id >= 1 are actual clusters (0 = missing, -1 = noise)\n",
                "        \"\"\"\n",
                "        clusters = set()\n",
                "        for traj in self.trajectories.values():\n",
                "            for visit in traj.visits:\n",
                "                if visit.cluster_id >= 1:  # Only count actual clusters (not noise=-1 or missing=0)\n",
                "                    clusters.add(visit.cluster_id)\n",
                "        return clusters\n",
                "    \n",
                "    def add_trajectory(self, trajectory: Trajectory):\n",
                "        \"\"\"Add a trajectory to the user.\"\"\"\n",
                "        self.trajectories[trajectory.date] = trajectory\n",
                "    \n",
                "    def get_trajectory(self, date: int) -> Optional[Trajectory]:\n",
                "        \"\"\"Get trajectory for a specific date.\"\"\"\n",
                "        return self.trajectories.get(date)\n",
                "    \n",
                "    def get_all_visits(self) -> List[Visit]:\n",
                "        \"\"\"Get all visits across all trajectories.\"\"\"\n",
                "        visits = []\n",
                "        for traj in self.trajectories.values():\n",
                "            visits.extend(traj.visits)\n",
                "        return sorted(visits, key=lambda v: v.t_start)\n",
                "    \n",
                "    def to_dataframe(self) -> pd.DataFrame:\n",
                "        \"\"\"Convert user data to DataFrame.\"\"\"\n",
                "        records = []\n",
                "        for date, traj in self.trajectories.items():\n",
                "            for visit in traj.visits:\n",
                "                records.append({\n",
                "                    'who': self.id,\n",
                "                    'date': date,\n",
                "                    't_start': visit.t_start,\n",
                "                    't_end': visit.t_end,\n",
                "                    'lon': visit.lon,\n",
                "                    'lat': visit.lat,\n",
                "                    'cluster_id': visit.cluster_id,\n",
                "                    'ptype': visit.ptype,\n",
                "                    'poi': visit.poi\n",
                "                })\n",
                "        return pd.DataFrame(records)\n",
                "    \n",
                "    @classmethod\n",
                "    def from_dataframe(cls, df: pd.DataFrame) -> 'User':\n",
                "        \"\"\"\n",
                "        Create User objects from a DataFrame.\n",
                "        \n",
                "        Handles splitting visits that cross midnight (3 AM threshold).\n",
                "        \"\"\"\n",
                "        # Ensure datetime columns\n",
                "        df = df.copy()\n",
                "        df['t_start'] = pd.to_datetime(df['t_start'])\n",
                "        df['t_end'] = pd.to_datetime(df['t_end'])\n",
                "        \n",
                "        users_dict: Dict[int, User] = {}\n",
                "        \n",
                "        for _, row in df.iterrows():\n",
                "            who = int(row['who'])\n",
                "            \n",
                "            if who not in users_dict:\n",
                "                users_dict[who] = User(id=who)\n",
                "            \n",
                "            user = users_dict[who]\n",
                "            \n",
                "            # Process visits that may cross the 3 AM boundary\n",
                "            visits_to_add = cls._process_crossing_visits(row)\n",
                "            \n",
                "            for visit, visit_date in visits_to_add:\n",
                "                # Get or create trajectory for this date\n",
                "                if visit_date not in user.trajectories:\n",
                "                    user.trajectories[visit_date] = Trajectory(date=visit_date)\n",
                "                \n",
                "                user.trajectories[visit_date].add_visit(visit)\n",
                "        \n",
                "        return users_dict\n",
                "    \n",
                "    @staticmethod\n",
                "    def _process_crossing_visits(row: pd.Series) -> List[tuple]:\n",
                "        \"\"\"\n",
                "        Process a visit record that may cross midnight (3 AM threshold).\n",
                "        \n",
                "        The daily trajectory is defined as: from 3:00 AM today to 3:00 AM tomorrow.\n",
                "        A timestamp is assigned to its \"active day\" based on when the person was likely active:\n",
                "        - Before 3 AM: belongs to previous calendar day (stayed up late)\n",
                "        - After 3 AM: belongs to current calendar day\n",
                "        \n",
                "        This method splits a visit into segments that fit within individual active days.\n",
                "        \n",
                "        Handles complex cases:\n",
                "        1. Visit crosses a single 3 AM threshold\n",
                "        2. Visit crosses multiple 3 AM thresholds (e.g., spanning multiple days)\n",
                "        3. Visit starts after 3 AM and ends after 3 AM but on different days\n",
                "        \n",
                "        Returns:\n",
                "        - List of (Visit, date) tuples, where date is YYYYMMDD of the active day\n",
                "        \"\"\"\n",
                "        threshold_hour = User.DAY_THRESHOLD_HOUR\n",
                "        result = []\n",
                "        \n",
                "        t_start = pd.to_datetime(row['t_start'])\n",
                "        t_end = pd.to_datetime(row['t_end'])\n",
                "        \n",
                "        # Create initial visit with original attributes\n",
                "        visit = Visit(\n",
                "            t_start=t_start,\n",
                "            t_end=t_end,\n",
                "            lon=float(row['lon']),\n",
                "            lat=float(row['lat']),\n",
                "            cluster_id=int(row.get('cluster_id', -1)),\n",
                "            ptype=int(row['ptype']) if pd.notna(row.get('ptype')) else None,\n",
                "            poi=int(row['poi']) if pd.notna(row.get('poi')) else None\n",
                "        )\n",
                "        \n",
                "        # Calculate threshold dates for start and end\n",
                "        # The threshold date represents the \"active day\" for a given timestamp\n",
                "        # This is based on the intuition that:\n",
                "        # - Early morning (e.g., 2 AM) belongs to the previous calendar day (staying up late)\n",
                "        # - Late night (e.g., 4 AM) belongs to the current calendar day\n",
                "        #\n",
                "        # A trajectory day is: from 3:00 AM today to 3:00 AM tomorrow\n",
                "        # e.g., 2019-01-01 02:00 belongs to threshold date 2018-12-31 (stayed up late)\n",
                "        # e.g., 2019-01-01 04:00 belongs to threshold date 2019-01-01\n",
                "        def get_threshold_date(dt: pd.Timestamp) -> int:\n",
                "            \"\"\"Get the threshold date (YYYYMMDD) for a given datetime.\"\"\"\n",
                "            if dt.hour >= threshold_hour:\n",
                "                # After 3 AM: belongs to current calendar day\n",
                "                threshold_date = dt.date()\n",
                "            else:\n",
                "                # Before 3 AM: belongs to previous calendar day (stayed up late)\n",
                "                threshold_date = (dt - timedelta(days=1)).date()\n",
                "            return int(threshold_date.strftime('%Y%m%d'))\n",
                "        \n",
                "        start_threshold_date = get_threshold_date(t_start)\n",
                "        end_threshold_date = get_threshold_date(t_end)\n",
                "        \n",
                "        # If no threshold crossing, no splitting needed\n",
                "        if start_threshold_date == end_threshold_date:\n",
                "            result.append((visit, start_threshold_date))\n",
                "            return result\n",
                "        \n",
                "        # Visit spans multiple threshold dates - need to split\n",
                "        # Find the first threshold that is AFTER t_start\n",
                "        # If t_start is already past today's threshold, first threshold is tomorrow\n",
                "        first_threshold = t_start.replace(hour=threshold_hour, minute=0, second=0)\n",
                "        if t_start >= first_threshold:\n",
                "            first_threshold = first_threshold + timedelta(days=1)\n",
                "        \n",
                "        # Part 1: From t_start to first threshold (if t_start is before first threshold)\n",
                "        if t_start < first_threshold:\n",
                "            visit1 = Visit(\n",
                "                t_start=t_start,\n",
                "                t_end=first_threshold,\n",
                "                lon=visit.lon,\n",
                "                lat=visit.lat,\n",
                "                cluster_id=visit.cluster_id,\n",
                "                ptype=visit.ptype,\n",
                "                poi=visit.poi\n",
                "            )\n",
                "            # Segment ends before first threshold, so it belongs to previous trajectory\n",
                "            # But we use start_threshold_date which correctly handles this\n",
                "            result.append((visit1, start_threshold_date))\n",
                "        \n",
                "        # Middle parts: full threshold-to-threshold segments\n",
                "        # These represent full days where the person was stationary\n",
                "        current_threshold = first_threshold\n",
                "        while current_threshold < t_end:\n",
                "            next_threshold = current_threshold + timedelta(days=1)\n",
                "            if next_threshold > t_end:\n",
                "                break\n",
                "            \n",
                "            # Create a visit representing the full stationary day\n",
                "            middle_visit = Visit(\n",
                "                t_start=current_threshold,\n",
                "                t_end=next_threshold,\n",
                "                lon=visit.lon,\n",
                "                lat=visit.lat,\n",
                "                cluster_id=visit.cluster_id,\n",
                "                ptype=visit.ptype,\n",
                "                poi=visit.poi\n",
                "            )\n",
                "            # This segment crosses midnight, so belongs to the day after current_threshold starts\n",
                "            # Which is exactly what get_threshold_date(t_end) gives us\n",
                "            current_threshold_date = get_threshold_date(current_threshold + timedelta(hours=12))\n",
                "            result.append((middle_visit, current_threshold_date))\n",
                "            \n",
                "            current_threshold = next_threshold\n",
                "        \n",
                "        # Last part: from last threshold to t_end\n",
                "        last_threshold = current_threshold\n",
                "        if t_end > last_threshold:\n",
                "            last_visit = Visit(\n",
                "                t_start=last_threshold,\n",
                "                t_end=t_end,\n",
                "                lon=visit.lon,\n",
                "                lat=visit.lat,\n",
                "                cluster_id=visit.cluster_id,\n",
                "                ptype=visit.ptype,\n",
                "                poi=visit.poi\n",
                "            )\n",
                "            # Use the end time to determine the trajectory date\n",
                "            last_threshold_date = get_threshold_date(t_end)\n",
                "            result.append((last_visit, last_threshold_date))\n",
                "        \n",
                "        return result\n",
                "    \n",
                "    def __repr__(self) -> str:\n",
                "        return f\"User(id={self.id}, trajectories={self.num_trajectories}, total_visits={self.total_visits})\"\n",
                "\n",
                "\n",
                "def convert_dataframe_to_users(df: pd.DataFrame) -> Dict[int, User]:\n",
                "    \"\"\"\n",
                "    Convert a DataFrame to a dictionary of User objects.\n",
                "    \n",
                "    Parameters:\n",
                "    - df: DataFrame with columns ['who', 'date', 't_start', 't_end', 'lon', 'lat', 'cluster_id', 'ptype', 'poi']\n",
                "    \n",
                "    Returns:\n",
                "    - Dictionary mapping user_id to User objects\n",
                "    \"\"\"\n",
                "    print(f\"Converting DataFrame with {len(df)} records to User objects...\")\n",
                "    users = User.from_dataframe(df)\n",
                "    print(f\"Created {len(users)} User objects\")\n",
                "    return users\n",
                "\n",
                "\n",
                "# ====== Execute Step 5: Convert DataFrame to User Model ======\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Converting DataFrame to User Model...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "users_dict = convert_dataframe_to_users(st_clustered)\n",
                "\n",
                "# Display sample users\n",
                "print(\"\\nSample users:\")\n",
                "for i, (user_id, user) in enumerate(list(users_dict.items())[:3]):\n",
                "    print(f\"\\n{user}\")\n",
                "    print(f\"  - Unique clusters visited: {len(user.unique_clusters)}\")\n",
                "    print(f\"  - Date range: {min(user.trajectories.keys())} to {max(user.trajectories.keys())}\")\n",
                "\n",
                "# Convert back to DataFrame to verify\n",
                "print(\"\\nVerification: Converting User objects back to DataFrame...\")\n",
                "test_user_id = list(users_dict.keys())[0]\n",
                "test_user = users_dict[test_user_id]\n",
                "test_df = test_user.to_dataframe()\n",
                "print(f\"User {test_user_id} converted back to DataFrame: {len(test_df)} records\")\n",
                "print(test_df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "71b37fd7",
            "metadata": {},
            "source": [
                "# Model-free RL modeling"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b10809c5",
            "metadata": {},
            "source": [
                "## Simple TD learning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "a8edad0d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reference: 251111_rl_demo.ipynb - MF modeling approach\n",
                "# Enhanced with: time encoding, day sequence, forgetting rate, end-of-day action (-9), delayed state inclusion\n",
                "import time\n",
                "from scipy.special import logsumexp\n",
                "from scipy.optimize import minimize\n",
                "from collections import defaultdict\n",
                "from typing import Dict, List, Tuple, Optional, Any, Set\n",
                "from dataclasses import dataclass, field\n",
                "from datetime import datetime, timedelta\n",
                "import warnings\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3a9fe60a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# MF Model Configuration\n",
                "# =============================================================================\n",
                "\n",
                "@dataclass\n",
                "class MFConfig:\n",
                "    \"\"\"Configuration for Model-Free RL estimation.\"\"\"\n",
                "    # Learning rate for TD updates (alpha)\n",
                "    alpha_init: float = 0.1\n",
                "    # Softmax inverse temperature (beta)\n",
                "    beta_init: float = 1.0\n",
                "    # Exploration probability (epsilon)\n",
                "    epsilon_init: float = 0.1\n",
                "    # Forgetting rate (phi) - discount applied when day changes\n",
                "    phi_init: float = 0.1  # Will be estimated as sigmoid(logit_phi)\n",
                "    # Reward type: 'linear', 'power', 'log'\n",
                "    reward_type: str = 'log'\n",
                "    # Reward parameter (for power/log functions)\n",
                "    reward_param_init: float = 1.0\n",
                "    # Visit threshold before adding to known set\n",
                "    visit_threshold: int = 3\n",
                "    # Maximum iterations for optimization\n",
                "    maxiter: int = 1000\n",
                "    # Convergence tolerance\n",
                "    ftol: float = 1e-6\n",
                "    # Reference date for day sequence calculation\n",
                "    ref_date: Tuple[int, int, int] = (2018, 12, 31)\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Data Preparation Functions\n",
                "# =============================================================================\n",
                "\n",
                "def compute_day_sequence(date_array: np.ndarray[int], \n",
                "                        ref_date: int = None) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Compute day sequence as days since reference date.\n",
                "    \n",
                "    Parameters:\n",
                "    - date_array: Array of dates in YYYYMMDD format\n",
                "    - ref_year, ref_month, ref_day: Reference date\n",
                "    \n",
                "    Returns:\n",
                "    - Array of day sequences (integers, monotonically increasing)\n",
                "    \"\"\"\n",
                "    if ref_date is None:\n",
                "        ref_date = datetime(2018, 12, 31)\n",
                "    else:\n",
                "        # convert the YYYYMMDD integer string to datetime\n",
                "        ref_date = datetime.strptime(str(ref_date), '%Y%m%d')\n",
                "    day_seq = np.zeros(len(date_array), dtype=int)\n",
                "    for i, d in enumerate(date_array):\n",
                "        year = d // 10000\n",
                "        month = (d // 100) % 100\n",
                "        day = d % 100\n",
                "        current_date = datetime(year, month, day)\n",
                "        day_seq[i] = (current_date - ref_date).days\n",
                "    \n",
                "    return day_seq\n",
                "\n",
                "\n",
                "def compute_time_angle(time_val: datetime,\n",
                "                       angle_base_hour = 3) -> float:\n",
                "    \"\"\"\n",
                "    Compute time angle scaled to [0, 1] where:\n",
                "    - 0.0 = same day's 3 AM\n",
                "    - 1.0 = next day's 3 AM (24 hours later)\n",
                "    \n",
                "    Example: 4 AM -> 1/24, Noon (12 PM) -> 9/24 = 0.375\n",
                "    \n",
                "    Parameters:\n",
                "    - time_val: Datetime of stay end (departure time)\n",
                "    \n",
                "    Returns:\n",
                "    - Time angle in [0, 1]\n",
                "    \"\"\"\n",
                "    # 3 AM as the reference \"start\" of the day\n",
                "    \n",
                "    hour = time_val.hour\n",
                "    minute = time_val.minute\n",
                "    second = time_val.second\n",
                "\n",
                "    # Compute total seconds since 3 AM; wrap around 24 hours\n",
                "    seconds_in_day = 24 * 3600\n",
                "    time_seconds = hour * 3600 + minute * 60 + second\n",
                "    base_seconds = angle_base_hour * 3600\n",
                "\n",
                "    delta = (time_seconds - base_seconds) % seconds_in_day  # wrap negative values to next day\n",
                "    time_angle = delta / seconds_in_day  # scale to 0~1\n",
                "    return time_angle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "cc3c8620",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_reward_array(stay_minutes: np.ndarray, \n",
                "                           reward_type: str = 'log', \n",
                "                           reward_param: Optional[float] = None) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Compute reward array based on stay duration (time-based rewards).\n",
                "    \"\"\"\n",
                "    base = np.maximum(stay_minutes / 30.0, 0.0)\n",
                "    \n",
                "    if reward_type == 'linear' or reward_param is None:\n",
                "        rewards = base\n",
                "    elif reward_type == 'power':\n",
                "        rewards = np.power(base, reward_param)\n",
                "    elif reward_type == 'log':\n",
                "        # Normalized log reward: 0 to 1 range\n",
                "        rewards = np.log1p(reward_param * base) / np.log1p(reward_param)\n",
                "    else:\n",
                "        rewards = base\n",
                "    \n",
                "    return np.nan_to_num(rewards, nan=0.0, posinf=0.0, neginf=0.0)\n",
                "\n",
                "\n",
                "def compute_time_discount_factor(time_angle: float) -> float:\n",
                "    \"\"\"\n",
                "    Compute time discount factor based on time angle.\n",
                "    \n",
                "    The factor is: 1 / (1 - time_angle)\n",
                "    This scales Q values so that values at different times of day are comparable.\n",
                "    \n",
                "    Example:\n",
                "    - time_angle = 0 (3 AM): factor = 1.0\n",
                "    - time_angle = 0.75 (9 PM): factor = 4.0\n",
                "    - time_angle = 0.99 (2:58 AM next day): factor = 100.0\n",
                "    \n",
                "    Parameters:\n",
                "    - time_angle: Time of day in [0, 1] scale\n",
                "    \n",
                "    Returns:\n",
                "    - Discount factor for time scaling\n",
                "    \"\"\"\n",
                "    if 1.0 - time_angle <= 0:\n",
                "        factor = 100.0\n",
                "    else:\n",
                "        factor = 1.0 / (1.0 - time_angle)\n",
                "        if factor > 100.0:\n",
                "            factor = 100.0\n",
                "    return factor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0a6d7778",
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_mf_data(user_df: pd.DataFrame,\n",
                "                   config: MFConfig = None) -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Prepare MF modeling data from user trajectory.\n",
                "    \n",
                "    This function:\n",
                "    1. Computes day sequence (days since reference date)\n",
                "    2. Computes time angles for each stay\n",
                "    3. Labels actions: -9 (end-of-day), -1 (explore), 0+ (cluster)\n",
                "    \n",
                "    Parameters:\n",
                "    - user_df: DataFrame with columns ['t_start', 't_end', 'cluster_id', 'date']\n",
                "    - config: MF configuration\n",
                "    \n",
                "    Returns:\n",
                "    - Dictionary with prepared data for MF modeling\n",
                "    \"\"\"\n",
                "    if config is None:\n",
                "        config = MFConfig()\n",
                "    \n",
                "    # Sort by timestamp\n",
                "    df = user_df.sort_values('t_start').reset_index(drop=True)\n",
                "    \n",
                "    # Extract basic data\n",
                "    n_records = len(df)\n",
                "    \n",
                "    # Compute day sequence\n",
                "    if 'date' not in df.columns:\n",
                "        raise ValueError(\"Input DataFrame must contain 'date' column for MF model preparation.\")\n",
                "    date_array = df['date'].to_numpy()\n",
                "    day_seq = compute_day_sequence(date_array, \n",
                "                                   config.ref_date[0], \n",
                "                                   config.ref_date[1], \n",
                "                                   config.ref_date[2])\n",
                "\n",
                "    \n",
                "    # Compute time angles (0-1 scale from 3 AM to next 3 AM)\n",
                "    time_angles = np.zeros(n_records)\n",
                "    for i, t_end in enumerate(df['t_end']):\n",
                "        time_angles[i] = compute_time_angle(t_end)\n",
                "    \n",
                "    # Extract states (cluster_ids)\n",
                "    states = df['cluster_id'].astype(int).to_numpy()\n",
                "    \n",
                "    # Compute stay durations for rewards\n",
                "    # note, here the reward should be the next one, not current one.\n",
                "    stay_minutes = (df['t_end'] - df['t_start']).dt.total_seconds() / 60.0\n",
                "    stay_minutes = stay_minutes.to_numpy()\n",
                "    stay_minutes = np.roll(stay_minutes, -1)\n",
                "    stay_minutes[-1] = 0\n",
                "    \n",
                "    # Label actions\n",
                "    # -9: end-of-day action (when next day_seq < current day_seq or last record)\n",
                "    # -1: explore new location\n",
                "    # 0+: transition to specific cluster\n",
                "    actions = np.zeros(n_records, dtype=int)\n",
                "    actions[-1] = -9\n",
                "    for t in range(n_records - 1):\n",
                "        current_day = day_seq[t]\n",
                "        next_day = day_seq[t + 1]\n",
                "        \n",
                "        if next_day > current_day:\n",
                "            # Transition to next day -> end-of-day action\n",
                "            actions[t] = -9\n",
                "        else:\n",
                "            # Normal transition\n",
                "            next_state = states[t + 1]\n",
                "            if next_state == -1:\n",
                "                # Transition to noise/unknown location -> explore action\n",
                "                actions[t] = -1\n",
                "            elif next_state == 0:\n",
                "                # Transition to outside study area\n",
                "                actions[t] = 0\n",
                "            else:\n",
                "                # Transition to known cluster\n",
                "                actions[t] = next_state\n",
                "    \n",
                "    # Compute same-day next indicator for TD bootstrapping\n",
                "    same_day_next = np.zeros(n_records, dtype=bool)\n",
                "    if n_records > 2:\n",
                "        for t in range(n_records - 1):\n",
                "            if day_seq[t] == day_seq[t + 1]: \n",
                "                same_day_next[t] = True\n",
                "    \n",
                "    return {\n",
                "        'states': states,\n",
                "        'actions': actions,\n",
                "        'day_seq': day_seq,\n",
                "        'time_angles': time_angles,\n",
                "        'stay_minutes': stay_minutes,\n",
                "        'date_array': date_array,\n",
                "        'n_records': n_records, \n",
                "        'same_day_next': same_day_next,\n",
                "        'df': df\n",
                "    }\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "1179b7ac",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# =============================================================================\n",
                "# Parameter Handling\n",
                "# =============================================================================\n",
                "\n",
                "def unpack_params_mf(theta: np.ndarray, \n",
                "                     feature_dim: int = 0, \n",
                "                     has_reward_param: bool = True) -> Dict[str, float]:\n",
                "    \"\"\"\n",
                "    Unpack MF model parameters from optimization vector.\n",
                "    \n",
                "    Parameters:\n",
                "    - theta: Parameter vector\n",
                "    - feature_dim: Dimension of feature weights (for future extensions)\n",
                "    - has_reward_param: Whether reward_param is included\n",
                "    \n",
                "    Returns:\n",
                "    - Dictionary with alpha, beta, epsilon, phi, reward_param\n",
                "    \"\"\"\n",
                "    idx = 0\n",
                "    \n",
                "    # Feature weights (placeholder for future extensions)\n",
                "    w = theta[:feature_dim] if feature_dim > 0 else np.array([])\n",
                "    idx += feature_dim\n",
                "    \n",
                "    # TD learning rate: alpha = sigmoid(logit_alpha)\n",
                "    logit_alpha = theta[idx]; idx += 1\n",
                "    alpha = 1.0 / (1.0 + np.exp(-logit_alpha))\n",
                "    \n",
                "    # Softmax temperature: beta = exp(log_beta)\n",
                "    log_beta = theta[idx]; idx += 1\n",
                "    beta = np.exp(log_beta)\n",
                "    \n",
                "    # Exploration probability: epsilon = sigmoid(logit_epsilon)\n",
                "    logit_epsilon = theta[idx]; idx += 1\n",
                "    epsilon = 1.0 / (1.0 + np.exp(-logit_epsilon))\n",
                "    \n",
                "    # Forgetting rate: phi = sigmoid(logit_phi) in [0, 1)\n",
                "    logit_phi = theta[idx]; idx += 1\n",
                "    phi = 1.0 / (1.0 + np.exp(-logit_phi))\n",
                "    \n",
                "    result = {\n",
                "        'w': w,\n",
                "        'alpha': alpha,\n",
                "        'beta': beta,\n",
                "        'epsilon': epsilon,\n",
                "        'phi': phi\n",
                "    }\n",
                "    \n",
                "    if has_reward_param:\n",
                "        logit_reward = theta[idx]; idx += 1\n",
                "        reward_param = np.exp(logit_reward)\n",
                "        result['reward_param'] = reward_param\n",
                "    \n",
                "    return result\n",
                "\n",
                "\n",
                "def pack_params_mf(alpha: float, beta: float, epsilon: float, phi: float,\n",
                "                   reward_param: Optional[float] = None,\n",
                "                   feature_dim: int = 0) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Pack MF model parameters into optimization vector.\n",
                "    \n",
                "    Parameters:\n",
                "    - alpha: TD learning rate (0, 1)\n",
                "    - beta: Softmax inverse temperature (0, +inf)\n",
                "    - epsilon: Exploration probability (0, 1)\n",
                "    - phi: Forgetting rate (0, 1)\n",
                "    - reward_param: Reward function parameter\n",
                "    - feature_dim: Dimension of feature weights\n",
                "    \n",
                "    Returns:\n",
                "    - Parameter vector\n",
                "    \"\"\"\n",
                "    params = []\n",
                "    \n",
                "    if feature_dim > 0:\n",
                "        params.extend([0.0] * feature_dim)\n",
                "    \n",
                "    params.append(np.log(alpha / (1.0 - alpha)))  # logit_alpha\n",
                "    params.append(np.log(beta))                   # log_beta\n",
                "    params.append(np.log(epsilon / (1.0 - epsilon)))  # logit_epsilon\n",
                "    params.append(np.log(phi / (1.0 - phi)))     # logit_phi\n",
                "    \n",
                "    if reward_param is not None:\n",
                "        params.append(np.log(reward_param))       # log_reward_param\n",
                "    \n",
                "    return np.array(params, dtype=np.float64)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6439f3d0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# MF Simulation and Log-Likelihood\n",
                "# =============================================================================\n",
                "\n",
                "def simulate_and_loglik_mf(theta: np.ndarray,\n",
                "                          mf_data: Dict[str, Any],\n",
                "                          feature_dim: int = 0,\n",
                "                          reward_type: str = 'log',\n",
                "                          has_reward_param: bool = True,\n",
                "                          visit_threshold: int = 3) -> float:\n",
                "    \"\"\"\n",
                "    Compute negative log-likelihood for enhanced MF (TD) model.\n",
                "    \n",
                "    Features:\n",
                "    - TD learning with time scaling\n",
                "    - Forgetting rate between days\n",
                "    - Delayed state inclusion (after visit_threshold visits)\n",
                "    - Actions: -9 (end), -1 (explore), 0+ (clusters)\n",
                "    \n",
                "    Parameters:\n",
                "    - theta: Parameter vector\n",
                "    - mf_data: Prepared MF data dictionary\n",
                "    - feature_dim: Dimension of feature weights\n",
                "    - reward_type: Reward function type\n",
                "    - has_reward_param: Whether reward_param is included\n",
                "    - visit_threshold: Visits required before adding to known set\n",
                "    \n",
                "    Returns:\n",
                "    - Negative log-likelihood\n",
                "    \"\"\"\n",
                "    # Unpack parameters\n",
                "    params = unpack_params_mf(theta, feature_dim, has_reward_param)\n",
                "    alpha = params['alpha']\n",
                "    beta = params['beta']\n",
                "    epsilon = params['epsilon']\n",
                "    phi = params['phi']\n",
                "    reward_param = params.get('reward_param', 1.0)\n",
                "    \n",
                "    # Extract data\n",
                "    states = mf_data['states']\n",
                "    actions = mf_data['actions']\n",
                "    day_seq = mf_data['day_seq']\n",
                "    time_angles = mf_data['time_angles']\n",
                "    n_records = mf_data['n_records']\n",
                "    same_day_next = mf_data['same_day_next']\n",
                "    \n",
                "    # Compute reward array\n",
                "    reward_array = compute_reward_array(mf_data['stay_minutes'], reward_type, reward_param)\n",
                "    \n",
                "    # Q-tables: Q[s][a] -> Q value\n",
                "    Q_tables: Dict[int, Dict[int, float]] = defaultdict(lambda: defaultdict(float))\n",
                "    \n",
                "    # Track visit counts for delayed inclusion\n",
                "    visit_counts: Dict[int, int] = defaultdict(int)\n",
                "    \n",
                "    # Known states and actions (with delayed inclusion)\n",
                "    known_states: Set[int] = {-1, 0}  # Noise and outside area always known\n",
                "    known_actions: Set[int] = {-9, -1, 0}  # End, explore, outside always known\n",
                "    \n",
                "    loglik = 0.0\n",
                "    \n",
                "    # Track previous day for forgetting\n",
                "    prev_day = None\n",
                "    \n",
                "    for t in range(n_records):\n",
                "        s = int(states[t])\n",
                "        a = int(actions[t])\n",
                "        r_t = float(reward_array[t])\n",
                "        current_day = int(day_seq[t])\n",
                "        time_angle = float(time_angles[t])\n",
                "        \n",
                "        # Apply forgetting when day changes\n",
                "        if prev_day is not None and current_day > prev_day:\n",
                "            # New day: apply forgetting discount to all Q values\n",
                "            discount_factor = (1.0 - phi)\n",
                "            for state_dict in Q_tables.values():\n",
                "                for action_key in state_dict:\n",
                "                    multi_day_discount = discount_factor ** (current_day - prev_day)\n",
                "                    state_dict[action_key] *= multi_day_discount\n",
                "        prev_day = current_day\n",
                "                \n",
                "        # Update visit counts and known sets\n",
                "        if a > 0:  # Transition to a cluster\n",
                "            visit_counts[a] += 1\n",
                "            if visit_counts[a] >= visit_threshold:\n",
                "                known_states.add(a)\n",
                "                known_actions.add(a)\n",
                "\n",
                "        s_perc = s if s in known_states else -1\n",
                "        a_perc = a if a in known_actions else -1\n",
                "\n",
                "        # Apply time scaling to current Q values\n",
                "        # time_scale = compute_time_discount_factor(time_angle)\n",
                "        time_scale = 1\n",
                "        \n",
                "        # Build action list for current state\n",
                "        # Actions available: -9 (end), -1 (explore), 0 (outside), plus visited clusters\n",
                "        # Note: -1 (explore) is not selected through Q-function, but through epsilon,\n",
                "        # so exclude -1 from evaluated_actions for softmax policy\n",
                "        evaluated_actions = sorted([act for act in known_actions if act != -1])\n",
                "        \n",
                "        q_values = []\n",
                "        for act in evaluated_actions:\n",
                "            q_td = Q_tables[s_perc].get(act, 0.0)\n",
                "            # Scale Q value by time discount\n",
                "            q_values.append(q_td * time_scale)\n",
                "        \n",
                "        q_values = np.asarray(q_values, dtype=np.float64)\n",
                "        \n",
                "        # Softmax policy\n",
                "        logits = beta * q_values\n",
                "        probs_exploit = np.exp(logits - logsumexp(logits))\n",
                "        \n",
                "        # Map action to probability\n",
                "        if a_perc in evaluated_actions:\n",
                "            idx_a = evaluated_actions.index(a_perc)\n",
                "            if idx_a < len(probs_exploit):\n",
                "                action_prob = (1.0 - epsilon) * probs_exploit[idx_a]\n",
                "        elif a_perc == -1: \n",
                "        # meaning the action is exploration -1.\n",
                "        # Add exploration probability\n",
                "            action_prob = epsilon\n",
                "        else:\n",
                "            action_prob = 0\n",
                "            warnings.warn(\"The action falls out the consideration when evaluation.\")\n",
                "\n",
                "        loglik += np.log(action_prob + 1e-12)\n",
                "        \n",
                "        # TD update with time scaling\n",
                "        # Next Q value also needs time scaling\n",
                "        if t < n_records - 1 and same_day_next[t]:\n",
                "            a_next = int(actions[t + 1])\n",
                "            a_next_perc = a_next if a_next in known_actions else -1\n",
                "\n",
                "            next_time_angle = float(time_angles[t + 1])\n",
                "            # next_time_discount = compute_time_discount_factor(next_time_angle)\n",
                "            next_time_scale = 1\n",
                "            next_Q_td = Q_tables[a_perc].get(a_next_perc, 0.0) * next_time_scale\n",
                "        else:\n",
                "            next_Q_td = 0.0\n",
                "        \n",
                "        # TD error (using scaled Q values)\n",
                "        current_Q = Q_tables[s_perc].get(a_perc, 0.0) * time_scale\n",
                "        delta = r_t + next_Q_td - current_Q\n",
                "        \n",
                "        # Update Q value (unscale before update)\n",
                "        Q_tables[s_perc][a_perc] = Q_tables[s_perc].get(a_perc, 0.0) + alpha * delta\n",
                "    \n",
                "    return -loglik"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a038583d",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# =============================================================================\n",
                "# Model Fitting\n",
                "# =============================================================================\n",
                "\n",
                "def fit_mf_model(user_df: pd.DataFrame,\n",
                "                config: MFConfig = None,\n",
                "                verbose: bool = True) -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Fit enhanced MF model to a single user's trajectory data.\n",
                "    \n",
                "    Parameters:\n",
                "    - user_df: DataFrame with columns ['t_start', 't_end', 'cluster_id', 'date']\n",
                "    - config: MF configuration\n",
                "    - verbose: Whether to print progress\n",
                "    \n",
                "    Returns:\n",
                "    - Dictionary with fitted parameters, statistics, and metadata\n",
                "    \"\"\"\n",
                "    if config is None:\n",
                "        config = MFConfig()\n",
                "    \n",
                "    # Prepare MF data\n",
                "    mf_data = prepare_mf_data(user_df, config)\n",
                "    n_records = mf_data['n_records']\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"Preparing MF model for {len(user_df)} visits, {n_records} actions (including end-of-day)...\")\n",
                "    \n",
                "    # Parameter dimension\n",
                "    has_reward_param = config.reward_type in ('power', 'log')\n",
                "    extra_params = 4 + (1 if has_reward_param else 0)  # alpha, beta, epsilon, phi, [reward_param]\n",
                "    param_dim = extra_params\n",
                "    \n",
                "    # Initial parameters\n",
                "    initial_theta = pack_params_mf(\n",
                "        alpha=config.alpha_init,\n",
                "        beta=config.beta_init,\n",
                "        epsilon=config.epsilon_init,\n",
                "        phi=config.phi_init,\n",
                "        reward_param=config.reward_param_init if has_reward_param else None,\n",
                "        feature_dim=0\n",
                "    )\n",
                "    \n",
                "    # Optimize\n",
                "    with warnings.catch_warnings():\n",
                "        warnings.simplefilter(\"ignore\")\n",
                "        result = minimize(\n",
                "            simulate_and_loglik_mf,\n",
                "            initial_theta,\n",
                "            args=(mf_data, 0, config.reward_type, has_reward_param, config.visit_threshold),\n",
                "            method='L-BFGS-B',\n",
                "            options={'maxiter': config.maxiter, 'ftol': config.ftol}\n",
                "        )\n",
                "    \n",
                "    # Extract fitted parameters\n",
                "    fitted_params = unpack_params_mf(result.x, 0, has_reward_param)\n",
                "    final_loglik = -result.fun\n",
                "    \n",
                "    # Compute model selection criteria\n",
                "    k_params = param_dim\n",
                "    AIC = 2 * k_params - 2 * final_loglik\n",
                "    BIC = k_params * np.log(n_records) - 2 * final_loglik if n_records > 0 else np.inf\n",
                "    \n",
                "    summary = {\n",
                "        'n_records': int(n_records),\n",
                "        'log_likelihood': float(final_loglik),\n",
                "        'AIC': float(AIC),\n",
                "        'BIC': float(BIC),\n",
                "        'alpha_td': float(fitted_params['alpha']),\n",
                "        'beta': float(fitted_params['beta']),\n",
                "        'epsilon_explore': float(fitted_params['epsilon']),\n",
                "        'phi_forget': float(fitted_params['phi']),\n",
                "        'reward_type': config.reward_type,\n",
                "        'reward_param': float(fitted_params.get('reward_param', None)),\n",
                "        'visit_threshold': config.visit_threshold,\n",
                "        'converged': result.success,\n",
                "        'n_iterations': result.nit if hasattr(result, 'nit') else None,\n",
                "        'optimization_message': result.message if hasattr(result, 'message') else None\n",
                "    }\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"MF model fitting {'converged' if result.success else 'did not converge'}.\")\n",
                "        print(f\"  Log-likelihood: {final_loglik:.2f}, AIC: {AIC:.2f}, BIC: {BIC:.2f}\")\n",
                "        print(f\"  alpha (TD rate): {fitted_params['alpha']:.4f}\")\n",
                "        print(f\"  beta (softmax temp): {fitted_params['beta']:.4f}\")\n",
                "        print(f\"  epsilon (explore): {fitted_params['epsilon']:.4f}\")\n",
                "        print(f\"  phi (forgetting): {fitted_params['phi']:.4f}\")\n",
                "        if has_reward_param:\n",
                "            print(f\"  reward_param: {fitted_params.get('reward_param', None):.4f}\")\n",
                "    \n",
                "    return summary\n",
                "\n",
                "\n",
                "def fit_mf_for_all_users(users_dict: Dict[int, Any],\n",
                "                         config: MFConfig = None,\n",
                "                         sample_size: Optional[int] = None,\n",
                "                         verbose: bool = True) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Fit enhanced MF model for all users in the dictionary.\n",
                "    \n",
                "    Parameters:\n",
                "    - users_dict: Dictionary mapping user_id to User objects\n",
                "    - config: MF configuration\n",
                "    - sample_size: Number of users to fit (None for all)\n",
                "    - verbose: Whether to print progress\n",
                "    \n",
                "    Returns:\n",
                "    - DataFrame with fitted parameters for each user\n",
                "    \"\"\"\n",
                "    if config is None:\n",
                "        config = MFConfig()\n",
                "    \n",
                "    user_ids = list(users_dict.keys())\n",
                "    if sample_size is not None:\n",
                "        user_ids = user_ids[:sample_size]\n",
                "    \n",
                "    results = []\n",
                "    for i, user_id in enumerate(user_ids):\n",
                "        if verbose:\n",
                "            print(f\"[{i+1}/{len(user_ids)}] Fitting MF for user {user_id}...\", end=\"\")\n",
                "        \n",
                "        user = users_dict[user_id]\n",
                "        user_df = user.to_dataframe()\n",
                "        \n",
                "        try:\n",
                "            t_start = time.time()\n",
                "            result = fit_mf_model(user_df, config, verbose=False)\n",
                "            t_end = time.time()\n",
                "            elapsed = t_end - t_start\n",
                "            if verbose:\n",
                "                print(f\"\\tUser {user_id} MF model fit time: {elapsed:.2f} seconds\")\n",
                "            result['user_id'] = user_id\n",
                "            result['fit_time_seconds'] = elapsed\n",
                "            results.append(result)\n",
                "        except Exception as e:\n",
                "            if verbose:\n",
                "                print(f\"  Error fitting user {user_id}: {e}\")\n",
                "            results.append({\n",
                "                'user_id': user_id, 'n_visits': 0, 'n_records': 0,\n",
                "                'log_likelihood': np.nan, 'AIC': np.nan, 'BIC': np.nan,\n",
                "                'alpha_td': np.nan, 'beta': np.nan, 'epsilon_explore': np.nan,\n",
                "                'phi_forget': np.nan, 'reward_type': config.reward_type,\n",
                "                'reward_param': np.nan, 'visit_threshold': config.visit_threshold,\n",
                "                'converged': False, 'n_iterations': None, 'optimization_message': str(e),\n",
                "                'fit_time_seconds': np.nan\n",
                "            })\n",
                "    \n",
                "    return pd.DataFrame(results)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0346270c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Enhanced Model-Free (MF) RL Estimation\n",
                        "Features: Time encoding, Day sequence, Forgetting rate,\n",
                        "          End-of-day action (-9), Delayed state inclusion\n",
                        "============================================================\n",
                        "[1/50] Fitting MF for user 126272...\n",
                        "    User 126272 MF model fit time: 16.96 seconds\n",
                        "[2/50] Fitting MF for user 278978...\n",
                        "    User 278978 MF model fit time: 9.96 seconds\n",
                        "[3/50] Fitting MF for user 395753...\n",
                        "    User 395753 MF model fit time: 15.92 seconds\n",
                        "[4/50] Fitting MF for user 506035...\n",
                        "    User 506035 MF model fit time: 12.79 seconds\n",
                        "[5/50] Fitting MF for user 612431...\n",
                        "    User 612431 MF model fit time: 9.39 seconds\n",
                        "[6/50] Fitting MF for user 661336...\n",
                        "    User 661336 MF model fit time: 19.77 seconds\n",
                        "[7/50] Fitting MF for user 824617...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Cover\\AppData\\Local\\Temp\\ipykernel_60528\\1426595212.py:39: RuntimeWarning:\n",
                        "\n",
                        "overflow encountered in exp\n",
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    User 824617 MF model fit time: 11.81 seconds\n",
                        "[8/50] Fitting MF for user 928827...\n",
                        "    User 928827 MF model fit time: 21.78 seconds\n",
                        "[9/50] Fitting MF for user 1017498...\n",
                        "    User 1017498 MF model fit time: 10.35 seconds\n",
                        "[10/50] Fitting MF for user 1159109...\n",
                        "    User 1159109 MF model fit time: 15.34 seconds\n",
                        "[11/50] Fitting MF for user 1196732...\n",
                        "    User 1196732 MF model fit time: 13.46 seconds\n",
                        "[12/50] Fitting MF for user 1200620...\n",
                        "    User 1200620 MF model fit time: 13.82 seconds\n",
                        "[13/50] Fitting MF for user 1369893...\n",
                        "    User 1369893 MF model fit time: 12.41 seconds\n",
                        "[14/50] Fitting MF for user 1581867...\n",
                        "    User 1581867 MF model fit time: 11.12 seconds\n",
                        "[15/50] Fitting MF for user 1840362...\n",
                        "    User 1840362 MF model fit time: 13.13 seconds\n",
                        "[16/50] Fitting MF for user 2046261...\n",
                        "    User 2046261 MF model fit time: 10.79 seconds\n",
                        "[17/50] Fitting MF for user 2221830...\n",
                        "    User 2221830 MF model fit time: 14.87 seconds\n",
                        "[18/50] Fitting MF for user 2436270...\n",
                        "    User 2436270 MF model fit time: 14.15 seconds\n",
                        "[19/50] Fitting MF for user 2590264...\n",
                        "    User 2590264 MF model fit time: 15.61 seconds\n",
                        "[20/50] Fitting MF for user 2709435...\n",
                        "    User 2709435 MF model fit time: 11.55 seconds\n",
                        "[21/50] Fitting MF for user 2758208...\n",
                        "    User 2758208 MF model fit time: 13.22 seconds\n",
                        "[22/50] Fitting MF for user 2832819...\n",
                        "    User 2832819 MF model fit time: 15.26 seconds\n",
                        "[23/50] Fitting MF for user 2872767...\n",
                        "    User 2872767 MF model fit time: 12.94 seconds\n",
                        "[24/50] Fitting MF for user 2876341...\n",
                        "    User 2876341 MF model fit time: 29.21 seconds\n",
                        "[25/50] Fitting MF for user 2914560...\n",
                        "    User 2914560 MF model fit time: 22.54 seconds\n",
                        "[26/50] Fitting MF for user 3034223...\n",
                        "    User 3034223 MF model fit time: 27.89 seconds\n",
                        "[27/50] Fitting MF for user 3087457...\n",
                        "    User 3087457 MF model fit time: 14.89 seconds\n",
                        "[28/50] Fitting MF for user 3136106...\n",
                        "    User 3136106 MF model fit time: 43.35 seconds\n",
                        "[29/50] Fitting MF for user 3344810...\n",
                        "    User 3344810 MF model fit time: 34.10 seconds\n",
                        "[30/50] Fitting MF for user 3440298...\n",
                        "    User 3440298 MF model fit time: 46.11 seconds\n",
                        "[31/50] Fitting MF for user 3474122...\n",
                        "    User 3474122 MF model fit time: 13.49 seconds\n",
                        "[32/50] Fitting MF for user 3549549...\n",
                        "    User 3549549 MF model fit time: 12.94 seconds\n",
                        "[33/50] Fitting MF for user 3803542...\n",
                        "    User 3803542 MF model fit time: 18.76 seconds\n",
                        "[34/50] Fitting MF for user 3893481...\n",
                        "    User 3893481 MF model fit time: 18.89 seconds\n",
                        "[35/50] Fitting MF for user 3911975...\n",
                        "    User 3911975 MF model fit time: 18.87 seconds\n",
                        "[36/50] Fitting MF for user 3932007...\n",
                        "    User 3932007 MF model fit time: 8.48 seconds\n",
                        "[37/50] Fitting MF for user 4490197...\n",
                        "    User 4490197 MF model fit time: 11.23 seconds\n",
                        "[38/50] Fitting MF for user 4608520...\n",
                        "    User 4608520 MF model fit time: 17.57 seconds\n",
                        "[39/50] Fitting MF for user 4717957...\n",
                        "    User 4717957 MF model fit time: 11.06 seconds\n",
                        "[40/50] Fitting MF for user 4975553...\n",
                        "    User 4975553 MF model fit time: 11.53 seconds\n",
                        "[41/50] Fitting MF for user 5339697...\n",
                        "    User 5339697 MF model fit time: 6.62 seconds\n",
                        "[42/50] Fitting MF for user 5539576...\n",
                        "    User 5539576 MF model fit time: 9.69 seconds\n",
                        "[43/50] Fitting MF for user 5677425...\n",
                        "    User 5677425 MF model fit time: 17.56 seconds\n",
                        "[44/50] Fitting MF for user 5745873...\n",
                        "    User 5745873 MF model fit time: 17.57 seconds\n",
                        "[45/50] Fitting MF for user 6071595...\n",
                        "    User 6071595 MF model fit time: 9.94 seconds\n",
                        "[46/50] Fitting MF for user 6741566...\n",
                        "    User 6741566 MF model fit time: 21.75 seconds\n",
                        "[47/50] Fitting MF for user 6769345...\n",
                        "    User 6769345 MF model fit time: 14.09 seconds\n",
                        "[48/50] Fitting MF for user 6854307...\n",
                        "    User 6854307 MF model fit time: 13.58 seconds\n",
                        "[49/50] Fitting MF for user 7466093...\n",
                        "    User 7466093 MF model fit time: 24.74 seconds\n",
                        "[50/50] Fitting MF for user 7477809...\n",
                        "    User 7477809 MF model fit time: 30.01 seconds\n",
                        "\n",
                        "Enhanced MF Model Fitting Summary:\n",
                        " user_id  n_records  log_likelihood          AIC          BIC  alpha_td     beta  epsilon_explore  phi_forget\n",
                        "  126272       1320    -3201.916918  6413.833836  6439.760771  0.167137 4.836095         0.264387    0.076271\n",
                        "  278978       1182    -2679.279984  5368.559969  5393.934785  0.108396 3.677174         0.223318    0.057132\n",
                        "  395753       1537    -3868.584408  7747.168817  7773.856756  0.307550 0.920731         0.302523    0.056956\n",
                        "  506035       1592    -3610.951109  7231.902217  7258.765949  0.377843 0.483012         0.343508    0.041699\n",
                        "  612431       1626    -4161.223363  8332.446726  8359.416118  0.215246 1.508641         0.261119    0.058590\n",
                        "  661336       1860    -4626.698463  9263.396926  9291.038585  0.197042 2.354835         0.391979    0.076330\n",
                        "  824617        446     -710.343976  1430.687952  1451.189547  0.048847 4.915388         0.156952    0.000000\n",
                        "  928827       1838    -4578.292040  9166.584079  9194.166246  0.246393 2.805061         0.394994    0.101761\n",
                        " 1017498        975    -2047.575851  4105.151702  4129.563889  0.183178 2.715280         0.203107    0.046926\n",
                        " 1159109       1805    -4874.627258  9759.254517  9786.746096  0.217998 2.713806         0.263795    0.079479\n",
                        " 1196732       1286    -3043.337028  6096.674056  6122.470516  0.187417 3.379814         0.284591    0.063621\n",
                        " 1200620       1369    -3506.905043  7023.810086  7049.919265  0.245436 2.452976         0.256584    0.051761\n",
                        " 1369893       1271    -3106.576229  6223.152458  6248.890254  0.199696 2.671991         0.238102    0.063841\n",
                        " 1581867       1781    -4576.858629  9163.717258  9191.141910  0.279196 1.380344         0.348006    0.056016\n",
                        " 1840362       1617    -4019.960318  8049.920635  8076.862274  0.221525 1.879160         0.282645    0.067069\n",
                        " 2046261       1123    -2521.696346  5053.392692  5078.511487  0.172110 3.762975         0.267053    0.067741\n",
                        " 2221830       1428    -3656.128505  7322.257010  7348.577161  0.174976 4.976276         0.289748    0.086690\n",
                        " 2436270       1555    -3869.362693  7748.725385  7775.471539  0.151546 3.140734         0.364634    0.063993\n",
                        " 2590264       1769    -4446.900230  8903.800459  8931.191308  0.222532 1.519021         0.355582    0.057010\n",
                        " 2709435       1308    -3021.875170  6053.750340  6079.631612  0.217981 0.755838         0.265969    0.056541\n",
                        " 2758208       1105    -2322.024410  4654.048820  4679.086823  0.134766 4.765230         0.248638    0.070601\n",
                        " 2832819       1217    -2745.099180  5500.198361  5525.719081  0.120578 6.315969         0.248101    0.093807\n",
                        " 2872767       1370    -3274.812599  6559.625198  6585.738028  0.187706 1.183515         0.297634    0.044422\n",
                        " 2876341       2433    -7083.642758 14177.285516 14206.269918  0.170063 2.456039         0.339223    0.043583\n",
                        " 2914560       1684    -4443.024150  8896.048300  8923.192936  0.142637 4.612959         0.320480    0.080990\n",
                        " 3034223       2159    -5760.468465 11530.936931 11559.323933  0.172122 2.554508         0.388447    0.069179\n",
                        " 3087457       1315    -3235.668806  6481.337613  6507.245572  0.205829 1.985664         0.255333    0.062119\n",
                        " 3136106       2627    -7526.625077 15063.250153 15092.618142  0.116308 5.644683         0.378862    0.082732\n",
                        " 3344810       1623    -4065.575918  8141.151836  8168.111994  0.101825 8.136168         0.345600    0.084556\n",
                        " 3440298       1544    -3961.520755  7933.041511  7959.752169  0.097171 5.713508         0.345460    0.031055\n",
                        " 3474122        698    -1209.759093  2429.518187  2452.259282  0.297300 0.308236         0.216329    0.022825\n",
                        " 3549549       1751    -4389.975247  8789.950495  8817.290206  0.255666 1.409127         0.274680    0.068032\n",
                        " 3803542       1288    -2872.513042  5755.026084  5780.830314  0.148126 5.150506         0.274733    0.078686\n",
                        " 3893481       1411    -3250.556489  6511.112978  6537.373248  0.136980 5.117400         0.343018    0.072134\n",
                        " 3911975       1525    -3622.143651  7254.287301  7280.936050  0.101039 5.930420         0.354027    0.072153\n",
                        " 3932007       1403    -3307.245520  6624.491040  6650.722880  0.244124 1.423437         0.337810    0.063978\n",
                        " 4490197       1173    -2648.480003  5306.960006  5332.296605  0.182659 2.640656         0.290694    0.063464\n",
                        " 4608520       1704    -4549.741748  9109.483496  9136.687164  0.235264 2.332300         0.272913    0.060595\n",
                        " 4717957        914    -1898.510844  3807.021689  3831.110842  0.059741 4.221226         0.261491    0.005149\n",
                        " 4975553       1510    -3667.186990  7344.373981  7370.973306  0.261470 0.748494         0.259564    0.048391\n",
                        " 5339697       1421    -3489.026940  6988.053880  7014.349461  0.242152 1.437255         0.265224    0.068423\n",
                        " 5539576        962    -2045.086439  4100.172879  4124.517951  0.217586 2.293072         0.278562    0.049107\n",
                        " 5677425       1312    -3131.880714  6273.761429  6299.657968  0.143757 5.440303         0.215620    0.082231\n",
                        " 5745873       1267    -3051.940390  6113.880781  6139.602816  0.237159 4.236271         0.266141    0.068961\n",
                        " 6071595       1926    -4911.129127  9832.258254  9860.074257  0.291184 1.288747         0.318810    0.071312\n",
                        " 6741566       1935    -5320.007694 10650.015387 10677.854700  0.426596 0.498780         0.269220    0.047545\n",
                        " 6769345       1661    -4283.538046  8577.076092  8604.151967  0.349889 0.954511         0.301027    0.071544\n",
                        " 6854307       1508    -3587.772367  7185.544734  7212.137432  0.164573 2.171643         0.384746    0.061581\n",
                        " 7466093       1427    -3648.426819  7306.853637  7333.170285  0.148870 5.249976         0.282958    0.051954\n",
                        " 7477809       2338    -6728.807666 13467.615332 13496.400588  0.267629 2.080263         0.325457    0.094866\n",
                        "\n",
                        "Enhanced MF estimation results saved to 'mf_estimation_results_enhanced.csv'\n",
                        "\n",
                        "Aggregate Statistics Across Users:\n",
                        "  Average alpha (TD rate): 0.2001 闂?0.0767\n",
                        "  Average beta (softmax temp): 3.0230 闂?1.8455\n",
                        "  Average epsilon (explore): 0.2944 闂?0.0537\n",
                        "  Average phi (forgetting): 0.0623 闂?0.0199\n",
                        "  Average log-likelihood: -3723.23 闂?1313.69\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# =============================================================================\n",
                "# Execute Enhanced MF Modeling\n",
                "# =============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Enhanced Model-Free (MF) RL Estimation\")\n",
                "print(\"Features: Time encoding, Day sequence, Forgetting rate,\")\n",
                "print(\"          End-of-day action (-9), Delayed state inclusion\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Configure enhanced MF model\n",
                "mf_config = MFConfig(\n",
                "    alpha_init=0.1,\n",
                "    beta_init=1.0,\n",
                "    epsilon_init=0.1,\n",
                "    phi_init=0.1,  # Initial forgetting rate\n",
                "    reward_type='log',\n",
                "    reward_param_init=1.0,\n",
                "    visit_threshold=3,  # Add to known set after 3 visits\n",
                "    maxiter=1000,\n",
                "    ftol=1e-6\n",
                ")\n",
                "\n",
                "# Fit MF model for sample users\n",
                "mf_results = fit_mf_for_all_users(\n",
                "    users_dict, \n",
                "    config=mf_config, \n",
                "    sample_size=50,  # Change to None for all users\n",
                "    verbose=True\n",
                ")\n",
                "\n",
                "print(\"\\nEnhanced MF Model Fitting Summary:\")\n",
                "print(mf_results[['user_id', 'n_records', 'log_likelihood', 'AIC', 'BIC', \n",
                "                  'alpha_td', 'beta', 'epsilon_explore', 'phi_forget']].to_string(index=False))\n",
                "\n",
                "# Save results\n",
                "mf_results.to_csv('mf_estimation_results_enhanced.csv', index=False)\n",
                "print(f\"\\nEnhanced MF estimation results saved to 'mf_estimation_results_enhanced.csv'\")\n",
                "\n",
                "# Aggregate statistics\n",
                "print(\"\\nAggregate Statistics Across Users:\")\n",
                "print(f\"  Average alpha (TD rate): {mf_results['alpha_td'].mean():.4f} ± {mf_results['alpha_td'].std():.4f}\")\n",
                "print(f\"  Average beta (softmax temp): {mf_results['beta'].mean():.4f} ± {mf_results['beta'].std():.4f}\")\n",
                "print(f\"  Average epsilon (explore): {mf_results['epsilon_explore'].mean():.4f} ± {mf_results['epsilon_explore'].std():.4f}\")\n",
                "print(f\"  Average phi (forgetting): {mf_results['phi_forget'].mean():.4f} ± {mf_results['phi_forget'].std():.4f}\")\n",
                "print(f\"  Average log-likelihood: {mf_results['log_likelihood'].mean():.2f} ± {mf_results['log_likelihood'].std():.2f}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8105dc26",
            "metadata": {},
            "source": [
                "## MF enhanced with episodic memory"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6161f75e",
            "metadata": {},
            "source": [
                "When considering generalization to novel time state, we refer to the approach in area of episodic RL to estimate the new Q-value or SR value. It functions in a non-parametric way: \n",
                "- Retrival: kernal weights + recency bias\n",
                "- Storage: direct input.\n",
                "Bayesian method (no explicity episodic memory) is not a possible choice, because the value estimation problem is not a stochastic one."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "d12b21a5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# MF Enhanced with Episodic Memory (Time-Augmented)\n",
                "# =============================================================================\n",
                "\n",
                "from typing import Dict, List, Tuple, Optional, Any, Set\n",
                "from dataclasses import dataclass\n",
                "from datetime import datetime\n",
                "from collections import defaultdict\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class MFEpiConfig:\n",
                "    \"\"\"Configuration for MF + episodic memory model.\"\"\"\n",
                "    alpha_init: float = 0.1\n",
                "    beta_init: float = 1.0\n",
                "    epsilon_init: float = 0.1\n",
                "    phi_init: float = 0.1\n",
                "    sigma_t_init: float = 1.0 / 12.0\n",
                "\n",
                "    reward_type: str = 'log'\n",
                "    reward_param_init: float = 1.0\n",
                "\n",
                "    visit_threshold: int = 3\n",
                "    memory_threshold: float = 0.01\n",
                "\n",
                "    maxiter: int = 1000\n",
                "    ftol: float = 1e-6\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class EpisodicRecord:\n",
                "    \"\"\"One memory trace for a (state, action, time) tuple.\"\"\"\n",
                "    q_value: float\n",
                "    time_angle: float\n",
                "    day_seq: int\n",
                "    record_date: int\n",
                "    strength: float = 1.0\n",
                "\n",
                "\n",
                "class EpisodicMemory:\n",
                "    \"\"\"Non-parametric episodic memory for Q-value retrieval.\"\"\"\n",
                "\n",
                "    def __init__(self, phi: float, config: Optional[MFEpiConfig] = None):\n",
                "        self.config = config if config is not None else MFEpiConfig()\n",
                "        self.phi = phi\n",
                "        self.Q_table: Dict[int, Dict[int, List[EpisodicRecord]]] = defaultdict(lambda: defaultdict(list))\n",
                "        self.Q_decay: Dict[int, Dict[int, float]] = defaultdict(lambda: defaultdict(float))\n",
                "        self.node_strength: Dict[int, float] = {}\n",
                "        self.node_visits: Dict[int, int] = {}\n",
                "        self.last_day: Optional[int] = None\n",
                "        self._active_nodes: Optional[Set[int]] = None\n",
                "\n",
                "    def add_record(self, node_id: int, action_id: int, time_angle: float, q_value: float,\n",
                "                   day_seq: int, record_date: int):\n",
                "        rec = EpisodicRecord(\n",
                "            q_value=float(q_value),\n",
                "            time_angle=float(time_angle),\n",
                "            day_seq=int(day_seq),\n",
                "            record_date=int(record_date),\n",
                "            strength=1.0,\n",
                "        )\n",
                "        self.Q_table[node_id][action_id].append(rec)\n",
                "        self.Q_decay[node_id][action_id] = 1.0\n",
                "        self.node_strength[node_id] = self.node_strength.get(node_id, 0.0) + 1.0\n",
                "        self.node_visits[node_id] = self.node_visits.get(node_id, 0) + 1\n",
                "        self._active_nodes = None\n",
                "\n",
                "    def decay(self, current_day: int):\n",
                "        if self.last_day is None:\n",
                "            self.last_day = current_day\n",
                "            return\n",
                "\n",
                "        day_diff = int(current_day - self.last_day)\n",
                "        if day_diff > 0:\n",
                "            factor = (1.0 - self.phi) ** day_diff\n",
                "            for s in self.Q_table:\n",
                "                for a in self.Q_table[s]:\n",
                "                    for rec in self.Q_table[s][a]:\n",
                "                        rec.strength *= factor\n",
                "            for node_id in self.node_strength:\n",
                "                self.node_strength[node_id] *= factor\n",
                "                for action_id in self.Q_decay[node_id]:\n",
                "                    self.Q_decay[node_id][action_id] *= factor\n",
                "            self._active_nodes = None\n",
                "\n",
                "        self.last_day = current_day\n",
                "\n",
                "    @staticmethod\n",
                "    def compute_time_similarity(t1: float, t2: float, sigma_t: float) -> float:\n",
                "        \"\"\"Circular Gaussian kernel on normalized time angle [0, 1).\"\"\"\n",
                "        diff = abs(float(t1) - float(t2))\n",
                "        sigma = max(float(sigma_t), 1e-6)\n",
                "        return float(np.exp(-0.5 * (diff / sigma) ** 2))\n",
                "\n",
                "    def get_records_for_sa_pair(self, node_id: int, action_id: int) -> List[EpisodicRecord]:\n",
                "        return self.Q_table[int(node_id)][int(action_id)]\n",
                "\n",
                "    def retrieve_q(self, target_node: int, target_action: int, target_time: float,\n",
                "                   sigma_t: Optional[float] = None) -> float:\n",
                "        \"\"\"Return Q-hat(target_node, target_action, target_time); returns 0.0 if no evidence.\"\"\"\n",
                "        sigma = self.config.sigma_t_init if sigma_t is None else float(sigma_t)\n",
                "        recs = self.get_records_for_sa_pair(target_node, target_action)\n",
                "        if not recs:\n",
                "            return 0.0\n",
                "\n",
                "        q_values = np.array([rec.q_value for rec in recs], dtype=np.float64)\n",
                "        strengths = np.array([rec.strength for rec in recs], dtype=np.float64)\n",
                "        similarities = np.array([self.compute_time_similarity(target_time, rec.time_angle, sigma) for rec in recs])\n",
                "\n",
                "        weights = similarities * strengths\n",
                "        q_estimate = np.average(q_values, weights=weights)\n",
                "        q_estimate *= self.Q_decay[target_node][target_action]\n",
                "\n",
                "        return q_estimate\n",
                "\n",
                "    def get_active_nodes(self) -> Set[int]:\n",
                "        if self._active_nodes is not None:\n",
                "            return self._active_nodes\n",
                "\n",
                "        self._active_nodes = {\n",
                "            node_id\n",
                "            for node_id, strength in self.node_strength.items()\n",
                "            if strength >= self.config.memory_threshold\n",
                "            and self.node_visits.get(node_id, 0) >= self.config.visit_threshold\n",
                "        }\n",
                "        return self._active_nodes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "7c919fbe",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# Data Preparation and Parameter Utilities\n",
                "# =============================================================================\n",
                "\n",
                "def prepare_data(user_df: pd.DataFrame, config: Optional[MFEpiConfig] = None) -> Dict[str, Any]:\n",
                "    \"\"\"Prepare trajectory arrays for MF + episodic model.\"\"\"\n",
                "    config = config if config is not None else MFEpiConfig()\n",
                "\n",
                "    df = user_df.sort_values(by=['t_start']).reset_index(drop=True)\n",
                "    n_records = len(df)\n",
                "\n",
                "    states = df['cluster_id'].astype(int).to_numpy()\n",
                "    time_angles = df['t_end'].apply(compute_time_angle).to_numpy(dtype=float)\n",
                "    date_array = df['date'].to_numpy()\n",
                "\n",
                "    date_baseline = int(date_array.min())\n",
                "    day_seq = compute_day_sequence(date_array, date_baseline)\n",
                "\n",
                "    stay_minutes = (df['t_end'] - df['t_start']).dt.total_seconds() / 60.0\n",
                "    stay_minutes = stay_minutes.to_numpy(dtype=float)\n",
                "    stay_minutes = np.roll(stay_minutes, -1)\n",
                "    stay_minutes[-1] = 0.0\n",
                "\n",
                "    actions = np.zeros(n_records, dtype=int)\n",
                "    actions[-1] = -9\n",
                "    for t in range(n_records - 1):\n",
                "        if day_seq[t + 1] > day_seq[t]:\n",
                "            actions[t] = -9\n",
                "        else:\n",
                "            actions[t] = int(states[t + 1])\n",
                "\n",
                "\n",
                "    same_day_next = np.zeros(n_records, dtype=bool)\n",
                "    for t in range(n_records - 1):\n",
                "        same_day_next[t] = day_seq[t] == day_seq[t + 1]\n",
                "\n",
                "    return {\n",
                "        'states': states,\n",
                "        'actions': actions,\n",
                "        'day_seq': day_seq,\n",
                "        'time_angles': time_angles,\n",
                "        'date_array': date_array,\n",
                "        'stay_minutes': stay_minutes,\n",
                "        'same_day_next': same_day_next,\n",
                "        'n_records': n_records,\n",
                "    }\n",
                "\n",
                "\n",
                "def unpack_params_time_epi(theta: np.ndarray) -> Dict[str, float]:\n",
                "    \"\"\"theta = [log_alpha, log_beta, logit_epsilon, logit_phi, log_sigma_t].\"\"\"\n",
                "    idx = 0\n",
                "    alpha = 1.0 / (1.0 + np.exp(-theta[idx])); idx += 1\n",
                "    beta = np.exp(theta[idx]); idx += 1\n",
                "    epsilon = 1.0 / (1.0 + np.exp(-theta[idx])); idx += 1\n",
                "    phi = 1.0 / (1.0 + np.exp(-theta[idx])); idx += 1\n",
                "\n",
                "    alpha = float(np.clip(alpha, 1e-6, 1.0))\n",
                "    epsilon = float(np.clip(epsilon, 1e-6, 1.0 - 1e-6))\n",
                "\n",
                "    return {\n",
                "        'alpha': alpha,\n",
                "        'beta': float(beta),\n",
                "        'epsilon': epsilon,\n",
                "        'phi': float(phi),\n",
                "    }\n",
                "\n",
                "\n",
                "def pack_params_time_epi(alpha: float, beta: float,\n",
                "                         epsilon: float, phi: float) -> np.ndarray:\n",
                "    return np.array([\n",
                "        np.log(alpha / (1.0 - alpha)),\n",
                "        np.log(beta),\n",
                "        np.log(epsilon / (1.0 - epsilon)),\n",
                "        np.log(phi / (1.0 - phi)),\n",
                "    ], dtype=float)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "id": "f0d4b3fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# Training Objective and Model Fitting\n",
                "# =============================================================================\n",
                "\n",
                "def simulate_and_loglik_mfe(theta: np.ndarray,\n",
                "                            mfe_data: Dict[str, Any],\n",
                "                            config: Optional[MFEpiConfig] = None) -> float:\n",
                "    \"\"\"Negative log-likelihood for MF + episodic memory with TD updates.\"\"\"\n",
                "    config = config if config is not None else MFEpiConfig()\n",
                "\n",
                "    params = unpack_params_time_epi(theta)\n",
                "    alpha = params['alpha']\n",
                "    beta = params['beta']\n",
                "    epsilon = params['epsilon']\n",
                "    phi = params['phi']\n",
                "\n",
                "\n",
                "    states = mfe_data['states']\n",
                "    actions = mfe_data['actions']\n",
                "    time_angles = mfe_data['time_angles']\n",
                "    day_seq = mfe_data['day_seq']\n",
                "    date_array = mfe_data['date_array']\n",
                "    same_day_next = mfe_data['same_day_next']\n",
                "    n_records = mfe_data['n_records']\n",
                "\n",
                "    reward_array = compute_reward_array(\n",
                "        mfe_data['stay_minutes'],\n",
                "        config.reward_type,\n",
                "        config.reward_param_init,\n",
                "    )\n",
                "\n",
                "    visit_counts: Dict[int, int] = defaultdict(int)\n",
                "    known_states: Set[int] = {-1, 0}\n",
                "    known_actions: Set[int] = {-9, -1, 0}\n",
                "\n",
                "    memory = EpisodicMemory(phi, config)\n",
                "    loglik = 0.0\n",
                "\n",
                "    for t in range(n_records):\n",
                "        s = int(states[t])\n",
                "        a = int(actions[t])\n",
                "        r_t = float(reward_array[t])\n",
                "        current_day = int(day_seq[t])\n",
                "        current_date = int(date_array[t])\n",
                "        time_angle = float(time_angles[t])\n",
                "\n",
                "        memory.decay(current_day)\n",
                "\n",
                "        if a > 0:\n",
                "            visit_counts[a] += 1\n",
                "            if visit_counts[a] >= config.visit_threshold:\n",
                "                known_states.add(a)\n",
                "                known_actions.add(a)\n",
                "\n",
                "        s_perc = s if s in known_states else -1\n",
                "        a_perc = a if a in known_actions else -1\n",
                "\n",
                "        evaluated_actions = sorted([act for act in known_actions if act != -1])\n",
                "        if len(evaluated_actions) == 0:\n",
                "            action_prob = np.clip(epsilon, 1e-12, 1.0)\n",
                "        else:\n",
                "            q_values = np.array([\n",
                "                memory.retrieve_q(s_perc, act, time_angle, config.sigma_t_init)\n",
                "                for act in evaluated_actions\n",
                "            ], dtype=float)\n",
                "\n",
                "            beta_q = beta * q_values\n",
                "            beta_q -= np.max(beta_q)\n",
                "            softmax = np.exp(beta_q)\n",
                "            softmax /= (np.sum(softmax) + 1e-12)\n",
                "\n",
                "            probs = (1.0 - epsilon) * softmax\n",
                "            if a_perc in evaluated_actions:\n",
                "                idx_a = evaluated_actions.index(a_perc)\n",
                "                action_prob = float(np.clip(probs[idx_a], 1e-12, 1.0))\n",
                "            else:\n",
                "                action_prob = float(np.clip(epsilon, 1e-12, 1.0))\n",
                "\n",
                "        loglik += np.log(action_prob)\n",
                "\n",
                "        if t < n_records - 1 and same_day_next[t]:\n",
                "            a_next = int(actions[t + 1])\n",
                "            a_next_perc = a_next if a_next in known_actions else -1\n",
                "            next_time_angle = float(time_angles[t + 1])\n",
                "            next_q = memory.retrieve_q(a_perc, a_next_perc, next_time_angle, config.sigma_t_init)\n",
                "        else:\n",
                "            next_q = 0.0\n",
                "\n",
                "        if a_perc != -1:\n",
                "            current_q = memory.retrieve_q(s_perc, a_perc, time_angle, config.sigma_t_init)\n",
                "            delta = r_t + next_q - current_q\n",
                "            new_q = current_q + alpha * delta\n",
                "            memory.add_record(s_perc, a_perc, time_angle, new_q, current_day, current_date)\n",
                "\n",
                "    return float(-loglik)\n",
                "\n",
                "\n",
                "def fit_mfe_model(user_df: pd.DataFrame,\n",
                "                            config: Optional[MFEpiConfig] = None,\n",
                "                            verbose: bool = True) -> Dict[str, Any]:\n",
                "    \"\"\"Fit MF + episodic memory model for one user trajectory.\"\"\"\n",
                "\n",
                "    episodic_data = prepare_data(user_df, config)\n",
                "    n_records = episodic_data['n_records']\n",
                "    if n_records < 2:\n",
                "        return {\n",
                "            'n_records': int(n_records),\n",
                "            'log_likelihood': np.nan,\n",
                "            'AIC': np.nan,\n",
                "            'BIC': np.nan,\n",
                "            'alpha_td': np.nan,\n",
                "            'beta': np.nan,\n",
                "            'epsilon_explore': np.nan,\n",
                "            'phi_forget': np.nan,\n",
                "            'converged': False,\n",
                "            'n_iterations': 0,\n",
                "            'optimization_message': 'Insufficient records',\n",
                "        }\n",
                "\n",
                "    theta_init = pack_params_time_epi(\n",
                "        alpha=np.clip(config.alpha_init, 1e-6, 1.0),\n",
                "        beta=max(config.beta_init, 1e-6),\n",
                "        epsilon=np.clip(config.epsilon_init, 1e-4, 1 - 1e-4),\n",
                "        phi=np.clip(config.phi_init, 1e-4, 1 - 1e-4),\n",
                "    )\n",
                "\n",
                "\n",
                "    with warnings.catch_warnings():\n",
                "        warnings.simplefilter('ignore')\n",
                "        result = minimize(\n",
                "            simulate_and_loglik_mfe,\n",
                "            theta_init,\n",
                "            args=(episodic_data, config),\n",
                "            method='L-BFGS-B',\n",
                "            options={'maxiter': int(config.maxiter), 'ftol': float(config.ftol), 'disp': False},\n",
                "        )\n",
                "\n",
                "    fitted = unpack_params_time_epi(result.x)\n",
                "    log_likelihood = -float(result.fun)\n",
                "\n",
                "    k_params = 4\n",
                "    aic = 2 * k_params - 2 * log_likelihood\n",
                "    bic = k_params * np.log(max(n_records, 1)) - 2 * log_likelihood\n",
                "\n",
                "    summary = {\n",
                "        'n_records': int(n_records),\n",
                "        'log_likelihood': float(log_likelihood),\n",
                "        'AIC': float(aic),\n",
                "        'BIC': float(bic),\n",
                "        'alpha_td': float(fitted['alpha']),\n",
                "        'beta': float(fitted['beta']),\n",
                "        'epsilon_explore': float(fitted['epsilon']),\n",
                "        'phi_forget': float(fitted['phi']),\n",
                "        'converged': bool(result.success),\n",
                "        'n_iterations': int(result.nit),\n",
                "        'optimization_message': str(result.message),\n",
                "    }\n",
                "\n",
                "    if verbose:\n",
                "        print(f\"MFE model fitting {'converged' if result.success else 'did not converge'}.\")\n",
                "        print(f\"  Log-likelihood: {summary['log_likelihood']:.2f}, AIC: {summary['AIC']:.2f}, BIC: {summary['BIC']:.2f}\")\n",
                "        print(f\"  alpha (TD rate): {summary['alpha_td']:.4f}\")\n",
                "        print(f\"  beta (softmax temp): {summary['beta']:.4f}\")\n",
                "        print(f\"  epsilon (explore): {summary['epsilon_explore']:.4f}\")\n",
                "        print(f\"  phi (forgetting): {summary['phi_forget']:.4f}\")\n",
                "\n",
                "    return summary\n",
                "\n",
                "\n",
                "def fit_mfe_for_all_users(users_dict: Dict[int, Any],\n",
                "                          config: Optional[MFEpiConfig] = None,\n",
                "                          sample_size: Optional[int] = None,\n",
                "                          verbose: bool = True) -> pd.DataFrame:\n",
                "    \"\"\"Fit MF+episodic model for all users.\"\"\"\n",
                "    config = config if config is not None else MFEpiConfig()\n",
                "\n",
                "    user_ids = list(users_dict.keys())\n",
                "    if sample_size is not None:\n",
                "        user_ids = user_ids[:sample_size]\n",
                "\n",
                "    results = []\n",
                "    for i, user_id in enumerate(user_ids):\n",
                "        if verbose:\n",
                "            print(f\"[{i+1}/{len(user_ids)}] Fitting MFE for user {user_id}...\", end='')\n",
                "\n",
                "        user = users_dict[user_id]\n",
                "        user_df = user.to_dataframe()\n",
                "\n",
                "        try:\n",
                "            t0 = time.time()\n",
                "            result = fit_mfe_model(user_df, config, verbose=False)\n",
                "            elapsed = time.time() - t0\n",
                "            if verbose:\n",
                "                print(f\"\\tUser {user_id} MFE model fit time: {elapsed:.2f} seconds\")\n",
                "            result['user_id'] = user_id\n",
                "            result['fit_time_seconds'] = elapsed\n",
                "            results.append(result)\n",
                "        except Exception as e:\n",
                "            if verbose:\n",
                "                print(f\"  Error fitting user {user_id}: {e}\")\n",
                "            results.append({\n",
                "                'user_id': user_id,\n",
                "                'n_records': 0,\n",
                "                'log_likelihood': np.nan,\n",
                "                'AIC': np.nan,\n",
                "                'BIC': np.nan,\n",
                "                'alpha_td': np.nan,\n",
                "                'beta': np.nan,\n",
                "                'epsilon_explore': np.nan,\n",
                "                'phi_forget': np.nan,\n",
                "                'converged': False,\n",
                "                'n_iterations': None,\n",
                "                'optimization_message': str(e),\n",
                "                'fit_time_seconds': np.nan,\n",
                "            })\n",
                "\n",
                "    return pd.DataFrame(results)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "id": "8edff62a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "MF Enhanced with Episodic Memory Estimation\n",
                        "Features: TD updates + non-parametric episodic Q retrieval\n",
                        "============================================================\n",
                        "[1/10] Fitting MFE for user 126272...\tUser 126272 MFE model fit time: 44.81 seconds\n",
                        "[2/10] Fitting MFE for user 278978...\tUser 278978 MFE model fit time: 29.88 seconds\n",
                        "[3/10] Fitting MFE for user 395753...\tUser 395753 MFE model fit time: 55.93 seconds\n",
                        "[4/10] Fitting MFE for user 506035...\tUser 506035 MFE model fit time: 69.07 seconds\n",
                        "[5/10] Fitting MFE for user 612431...\tUser 612431 MFE model fit time: 54.67 seconds\n",
                        "[6/10] Fitting MFE for user 661336...\tUser 661336 MFE model fit time: 123.93 seconds\n",
                        "[7/10] Fitting MFE for user 824617...\tUser 824617 MFE model fit time: 9.13 seconds\n",
                        "[8/10] Fitting MFE for user 928827...\tUser 928827 MFE model fit time: 98.38 seconds\n",
                        "[9/10] Fitting MFE for user 1017498...\tUser 1017498 MFE model fit time: 19.54 seconds\n",
                        "[10/10] Fitting MFE for user 1159109...\tUser 1159109 MFE model fit time: 63.69 seconds\n",
                        "\n",
                        "MFE Model Fitting Summary:\n",
                        " user_id  n_records  log_likelihood         AIC         BIC  alpha_td     beta  epsilon_explore  phi_forget\n",
                        "  126272       1320    -3150.082773 6308.165546 6328.907094  0.349662 1.386816         0.264363    0.040710\n",
                        "  278978       1182    -2597.870855 5203.741710 5224.041562  0.292027 1.493645         0.223345    0.029264\n",
                        "  395753       1537    -3784.139280 7576.278560 7597.628911  0.272918 1.782851         0.302409    0.034897\n",
                        "  506035       1592    -3535.070086 7078.140171 7099.631157  0.407042 1.199021         0.344356    0.029973\n",
                        "  612431       1626    -4107.000443 8222.000887 8243.576400  0.321307 1.677921         0.261401    0.050288\n",
                        "  661336       1860    -4513.383927 9034.767854 9056.881181  0.269188 1.826092         0.391930    0.024136\n",
                        "  824617        446     -699.735133 1407.470266 1423.871542  0.133395 1.928849         0.156942    0.006960\n",
                        "  928827       1838    -4511.503342 9031.006685 9053.072418  0.317120 1.739189         0.394995    0.055329\n",
                        " 1017498        975    -1991.060758 3990.121516 4009.651266  0.392458 1.113587         0.203070    0.031480\n",
                        " 1159109       1805    -4778.212710 9564.425420 9586.418683  0.338819 1.643818         0.263719    0.048362\n",
                        "\n",
                        "MFE estimation results saved to 'mfe_estimation_results.csv'\n",
                        "\n",
                        "Aggregate Statistics Across Users:\n",
                        "  Average alpha (TD rate): 0.3094 ± 0.0770\n",
                        "  Average beta (softmax temp): 1.5792 ± 0.2728\n",
                        "  Average epsilon (explore): 0.2807 ± 0.0785\n",
                        "  Average phi (forgetting): 0.0351 ± 0.0143\n",
                        "  Average log-likelihood: -3366.81 ± 1291.49\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# Execute MFE (MF + Episodic Memory) Modeling\n",
                "# =============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"MF Enhanced with Episodic Memory Estimation\")\n",
                "print(\"Features: TD updates + non-parametric episodic Q retrieval\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "mfe_config = MFEpiConfig(\n",
                "    alpha_init=0.1,\n",
                "    beta_init=1.0,\n",
                "    epsilon_init=0.1,\n",
                "    phi_init=0.1,\n",
                "    sigma_t_init=1.0 / 12.0,\n",
                "    reward_type='log',\n",
                "    reward_param_init=1.0,\n",
                "    visit_threshold=3,\n",
                "    memory_threshold=0.01,\n",
                "    maxiter=1000,\n",
                "    ftol=1e-6,\n",
                ")\n",
                "\n",
                "mfe_results = fit_mfe_for_all_users(\n",
                "    users_dict,\n",
                "    config=mfe_config,\n",
                "    sample_size=10,\n",
                "    verbose=True,\n",
                ")\n",
                "\n",
                "print(\"\\nMFE Model Fitting Summary:\")\n",
                "print(mfe_results[['user_id', 'n_records', 'log_likelihood', 'AIC', 'BIC',\n",
                "                   'alpha_td', 'beta', 'epsilon_explore', 'phi_forget']].to_string(index=False))\n",
                "\n",
                "mfe_results.to_csv('mfe_estimation_results.csv', index=False)\n",
                "print(\"\\nMFE estimation results saved to 'mfe_estimation_results.csv'\")\n",
                "\n",
                "print(\"\\nAggregate Statistics Across Users:\")\n",
                "print(f\"  Average alpha (TD rate): {mfe_results['alpha_td'].mean():.4f} ± {mfe_results['alpha_td'].std():.4f}\")\n",
                "print(f\"  Average beta (softmax temp): {mfe_results['beta'].mean():.4f} ± {mfe_results['beta'].std():.4f}\")\n",
                "print(f\"  Average epsilon (explore): {mfe_results['epsilon_explore'].mean():.4f} ± {mfe_results['epsilon_explore'].std():.4f}\")\n",
                "print(f\"  Average phi (forgetting): {mfe_results['phi_forget'].mean():.4f} ± {mfe_results['phi_forget'].std():.4f}\")\n",
                "print(f\"  Average log-likelihood: {mfe_results['log_likelihood'].mean():.2f} ± {mfe_results['log_likelihood'].std():.2f}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bd24f87e",
            "metadata": {},
            "source": [
                "# Model-based RL modeling"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "827246a6",
            "metadata": {},
            "source": [
                "## Episodic RL (Not very successful)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "10e97122",
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "Episodic RL Modeling with Contextual Memory\n",
                "============================================\n",
                "\n",
                "This module implements an episodic reinforcement learning model that incorporates\n",
                "contextual/situational memory for urban cognition modeling.\n",
                "\n",
                "Key Features:\n",
                "1. Graph Memory: Tracks visited locations and their memory strength\n",
                "2. Episodic Q-Table: Records daily cumulative rewards and temporal context\n",
                "3. Time-Weighted Retrieval: Computes Q-values using temporal similarity\n",
                "4. Memory Decay: Geometric decay of memory strength across days\n",
                "\"\"\"\n",
                "\n",
                "from typing import Dict, List, Tuple, Optional, Any, Set\n",
                "from dataclasses import dataclass, field\n",
                "from collections import defaultdict\n",
                "from scipy.special import logsumexp\n",
                "import warnings\n",
                "from datetime import timedelta\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Episodic Memory Configuration\n",
                "# =============================================================================\n",
                "\n",
                "@dataclass\n",
                "class EpisodicConfig:\n",
                "    \"\"\"Configuration for Episodic RL model.\"\"\"\n",
                "    # Memory formation threshold\n",
                "    visit_threshold: int = 3  # Visits required to add cluster to graph\n",
                "    \n",
                "    # Memory decay parameter (phi_episodic)\n",
                "    phi_episodic: float = 0.1  # Daily decay rate for memory strength\n",
                "    \n",
                "    # Memory strength threshold for inclusion in decisions\n",
                "    memory_threshold: float = 0.1  # Exclude nodes with strength < 0.1\n",
                "    \n",
                "    # Time similarity Gaussian std (in time_angle units, 0-1 = 24h)\n",
                "    # std = 2 hours = 2/24 = 1/12 闂?0.0833\n",
                "    time_sim_std: float = 1.0 / 12.0\n",
                "    \n",
                "    # Exploration rate\n",
                "    epsilon_init: float = 0.1\n",
                "    \n",
                "    # Softmax temperature\n",
                "    beta_init: float = 1.0\n",
                "    \n",
                "    # Reward type for episodic gains\n",
                "    reward_type: str = 'log'\n",
                "    reward_param: float = 1.0\n",
                "    \n",
                "    # Optimization settings\n",
                "    maxiter: int = 1000\n",
                "    ftol: float = 1e-6\n",
                "\n",
                "    # Reference date for day sequence calculation\n",
                "    ref_date: Tuple[int, int, int] = (2018, 12, 31)\n",
                "\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Data Structures for Episodic Memory\n",
                "# =============================================================================\n",
                "\n",
                "@dataclass\n",
                "class EpisodicRecord:\n",
                "    \"\"\"\n",
                "    A single episodic memory record for one action (cluster) on one day.\n",
                "\n",
                "    Stores:\n",
                "    - cluster_id: The cluster visited (action taken)\n",
                "    - gain: Cumulative reward for that day (sum of rewards from this action onward)\n",
                "    - time_angle: Time angle when the action was taken\n",
                "    - record_date: Date of the record (YYYYMMDD)\n",
                "    - trace: Decay factor for this record (starts at 1.0, decays daily)\n",
                "\n",
                "    This record is created at the end of each day and added to episodic_table.\n",
                "    It is used in Q-value computation via weighted average.\n",
                "    \"\"\"\n",
                "    cluster_id: int           # The cluster visited (action)\n",
                "    gain: float               # Cumulative reward for the day (no discount)\n",
                "    time_angle: float         # Time angle when the action was taken\n",
                "    record_date: int          # Date when the record was made\n",
                "    trace: float = 1.0        # Internal difference in days (for decay)\n",
                "    \n",
                "    def to_dict(self) -> dict:\n",
                "        return {\n",
                "            'cluster_id': self.cluster_id,\n",
                "            'gain': self.gain,\n",
                "            'time_angle': self.time_angle,\n",
                "            'record_date': self.record_date\n",
                "        }\n",
                "    \n",
                "    @classmethod\n",
                "    def from_dict(cls, data: dict) -> 'EpisodicRecord':\n",
                "        return cls(\n",
                "            cluster_id=int(data['cluster_id']),\n",
                "            gain=float(data['gain']),\n",
                "            time_angle=float(data['time_angle']),\n",
                "            record_date=int(data['record_date'])\n",
                "        )\n",
                "\n",
                "\n",
                "class Memory:\n",
                "    \"\"\"\n",
                "    Graph-based memory structure for episodic RL.\n",
                "\n",
                "    Maintains:\n",
                "    - graph_nodes: Set of cluster IDs that have been added to graph (after visit_threshold visits)\n",
                "    - visit_counts: Number of times each cluster has been visited\n",
                "    - memory_strength: Current memory strength for each cluster in graph\n",
                "    - episodic_table: List of EpisodicRecord (all historical records, used for Q-computation)\n",
                "\n",
                "    Key Design:\n",
                "    - A cluster enters graph when visited visit_threshold times\n",
                "    - Memory strength is initialized to 1.0 upon entry\n",
                "    - Memory strength increases by 1.0 after each day's visit (applied at day-end)\n",
                "    - Memory strength decays geometrically at the start of each new day\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, config: EpisodicConfig):\n",
                "        self.config = config\n",
                "        self.graph_nodes: Set[int] = set()  # Clusters in the graph\n",
                "        self.visit_counts: Dict[int, int] = defaultdict(int)\n",
                "        self.memory_strength: Dict[int, float] = defaultdict(float)\n",
                "        self.episodic_table: List[EpisodicRecord] = []  # date -> table\n",
                "        \n",
                "        # Special nodes always present (but with 0 memory strength initially)\n",
                "        # -1: noise/unknown, 0: outside study area\n",
                "        self.always_present: Set[int] = {-1, 0}\n",
                "    \n",
                "    def add_visit(self, cluster_id: int):\n",
                "        \"\"\"\n",
                "        Record a visit to a cluster and update memory strength.\n",
                "\n",
                "        If visit count reaches threshold, add cluster to graph with initial strength 1.0.\n",
                "        If already in graph, increase memory strength by 1.0.\n",
                "\n",
                "        Note: This is called at the END of each day. The purpose of this function is to select the suitable cluster.\n",
                "        \"\"\"\n",
                "        if cluster_id <= 0:  # Skip special nodes (-1: noise, 0: outside)\n",
                "            return\n",
                "\n",
                "        self.visit_counts[cluster_id] += 1\n",
                "\n",
                "        # Add to graph if threshold reached\n",
                "        if self.visit_counts[cluster_id] >= self.config.visit_threshold:\n",
                "            if cluster_id not in self.graph_nodes:\n",
                "                # First time entering graph\n",
                "                self.graph_nodes.add(cluster_id)\n",
                "                self.memory_strength[cluster_id] = 1.0\n",
                "            else:\n",
                "                # Already in graph: increase strength by 1.0\n",
                "                self.memory_strength[cluster_id] += 1.0\n",
                "    \n",
                "\n",
                "    def decay(self, date_diff: int = 1):\n",
                "        \"\"\"\n",
                "        Apply geometric decay to memory strength at the start of a new day.\n",
                "\n",
                "        Called at the START of each new day (before any decision-making).\n",
                "\n",
                "        Memory strength decays as: strength *= (1 - phi_episodic) ^ date_diff\n",
                "        EpisodicRecord.trace also decays similarly (used for Q-value computation).\n",
                "\n",
                "        Parameters:\n",
                "        - date_diff: Number of days since last decay (usually 1, can be >1 for gaps)\n",
                "        \"\"\"\n",
                "        decay_factor = (1.0 - self.config.phi_episodic) ** date_diff\n",
                "        for node_id in self.graph_nodes:\n",
                "            self.memory_strength[node_id] *= decay_factor\n",
                "        for episode_record in self.episodic_table:\n",
                "            episode_record.trace *= decay_factor\n",
                "\n",
                "\n",
                "    def get_active_nodes(self) -> Set[int]:\n",
                "        \"\"\"\n",
                "        Get clusters with memory strength above threshold for Q-function inclusion.\n",
                "\n",
                "        Only clusters with sufficient memory strength participate in episodic Q-computation.\n",
                "        Weak memories (below threshold) are excluded, effectively \"forgotten\".\n",
                "\n",
                "        Returns:\n",
                "        - Set of cluster IDs with memory_strength >= memory_threshold\n",
                "        \"\"\"\n",
                "        if self.config.memory_threshold <= 0:\n",
                "            return self.graph_nodes.copy()\n",
                "\n",
                "        return {\n",
                "            node_id for node_id in self.graph_nodes\n",
                "            if self.memory_strength.get(node_id, 0.0) >= self.config.memory_threshold\n",
                "        }\n",
                "\n",
                "    \n",
                "    def add_episodic_record(self, record: EpisodicRecord):\n",
                "        \"\"\"\n",
                "        Add an episodic record to the episodic table.\n",
                "\n",
                "        Called at day-end to store the day's experiences for future Q-computation.\n",
                "        \"\"\"\n",
                "        self.episodic_table.append(record)\n",
                "\n",
                "\n",
                "    def get_records_for_action(self, cluster_id: int) -> List[EpisodicRecord]:\n",
                "        \"\"\"\n",
                "        Get all historical episodic records for a specific action (cluster).\n",
                "\n",
                "        These records are used in compute_episodic_Q to calculate Q-values\n",
                "        based on time-weighted average of historical gains.\n",
                "\n",
                "        Parameters:\n",
                "        - cluster_id: The cluster/action to retrieve records for\n",
                "\n",
                "        Returns:\n",
                "        - List of EpisodicRecord objects for this cluster\n",
                "        \"\"\"\n",
                "        assert cluster_id != -1, \"cluster_id could not be -1.\"\n",
                "        return [r for r in self.episodic_table if r.cluster_id == cluster_id]\n",
                "    \n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Reward Computation Functions\n",
                "# =============================================================================\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Data Preparation for Episodic Model\n",
                "# =============================================================================\n",
                "\n",
                "def prepare_episodic_data(user_df: pd.DataFrame,\n",
                "                         config: EpisodicConfig = None) -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Prepare episodic modeling data from user trajectory.\n",
                "    \n",
                "    Parameters:\n",
                "    - user_df: DataFrame with columns ['t_start', 't_end', 'cluster_id', 'date']\n",
                "    - config: Episodic configuration\n",
                "    \n",
                "    Returns:\n",
                "    - Dictionary with prepared data for episodic modeling\n",
                "    \"\"\"\n",
                "    if config is None:\n",
                "        config = EpisodicConfig()\n",
                "    \n",
                "    # Ensure datetime columns\n",
                "    df = user_df.copy()\n",
                "    # Sort by time\n",
                "    df = df.sort_values(by=['t_start']).reset_index(drop=True)\n",
                "    \n",
                "    # Basic data\n",
                "    n_records = len(df)\n",
                "    states = df['cluster_id'].to_numpy()\n",
                "    \n",
                "    # Compute time angles\n",
                "    time_angles = np.array([\n",
                "        compute_time_angle(dt) for dt in df['t_end']\n",
                "    ])\n",
                "    \n",
                "    # Extract dates as array\n",
                "    date_array = df['date'].to_numpy()\n",
                "    unique_dates = sorted(df['date'].unique())\n",
                "    # Compute day sequence (consecutive integers starting from 0)\n",
                "    day_seq = compute_day_sequence(date_array, \n",
                "                                   config.ref_date[0], \n",
                "                                   config.ref_date[1], \n",
                "                                   config.ref_date[2])\n",
                "    \n",
                "    # Compute stay durations and rewards\n",
                "    stay_minutes = (df['t_end'] - df['t_start']).dt.total_seconds() / 60.0\n",
                "    stay_minutes = stay_minutes.to_numpy()\n",
                "    stay_minutes = np.roll(stay_minutes, -1)\n",
                "    stay_minutes[-1] = 0\n",
                "\n",
                "    # Reward for each record\n",
                "    reward_array = compute_reward_array(\n",
                "        stay_minutes, \n",
                "        config.reward_type, \n",
                "        config.reward_param\n",
                "    )\n",
                "    \n",
                "    # Compute daily gains as backward cumulative sum of rewards\n",
                "    # Gain at time t: sum of rewards from t until the end of that day\n",
                "    # This represents the cumulative reward obtained AFTER taking the action at time t\n",
                "    gains = np.zeros(n_records)\n",
                "    for date in unique_dates:\n",
                "        day_mask = df['date'] == date\n",
                "        idxs, = np.where(day_mask.values)\n",
                "        day_rewards = reward_array[idxs]\n",
                "        # Compute backward cumulative sum (Gain at each time step: sum of future rewards including now)\n",
                "        backward_cumsum = np.cumsum(day_rewards[::-1])[::-1]\n",
                "        gains[idxs] = backward_cumsum\n",
                "    \n",
                "    # Compute actions\n",
                "    # -9: end-of-day, -1: explore (noise), 0: outside, 1+: clusters\n",
                "    actions = np.zeros(n_records, dtype=int)\n",
                "    actions[-1] = -9  # Last action is end-of-day\n",
                "    \n",
                "    for t in range(n_records - 1):\n",
                "        current_date = df.iloc[t]['date']\n",
                "        next_date = df.iloc[t + 1]['date']\n",
                "        \n",
                "        if next_date > current_date:\n",
                "            # Next day - end of trajectory\n",
                "            actions[t] = -9\n",
                "        else:\n",
                "            next_state = states[t + 1]\n",
                "            if next_state == -1:\n",
                "                actions[t] = -1  # Explore to noise\n",
                "            elif next_state == 0:\n",
                "                actions[t] = 0  # Outside study area\n",
                "            else:\n",
                "                actions[t] = next_state  # Transition to cluster\n",
                "    \n",
                "    # Compute same-day next indicator\n",
                "    same_day_next = np.zeros(n_records, dtype=bool)\n",
                "    for t in range(n_records - 1):\n",
                "        if day_seq[t] == day_seq[t + 1]:\n",
                "            same_day_next[t] = True\n",
                "        \n",
                "    return {\n",
                "        'states': states,\n",
                "        'actions': actions,\n",
                "        'time_angles': time_angles,\n",
                "        'date_array': date_array,\n",
                "        'day_seq': day_seq,\n",
                "        'stay_minutes': stay_minutes,\n",
                "        'reward_array': reward_array,\n",
                "        'gains': gains,\n",
                "        'n_records': n_records,\n",
                "        'same_day_next': same_day_next,\n",
                "        'df': df\n",
                "    }\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Parameter Handling\n",
                "# =============================================================================\n",
                "\n",
                "def unpack_params_epi(theta: np.ndarray) -> Dict[str, float]:\n",
                "    \"\"\"\n",
                "    Unpack episodic model parameters from optimization vector.\n",
                "    \n",
                "    Parameters:\n",
                "    - theta: Parameter vector [phi_episodic, beta, epsilon]\n",
                "    \n",
                "    Returns:\n",
                "    - Dictionary with phi_episodic, beta, epsilon\n",
                "    \"\"\"\n",
                "    idx = 0\n",
                "    \n",
                "    # Episodic decay rate: phi = sigmoid(logit_phi) in [0, 1)\n",
                "    logit_phi = theta[idx]; idx += 1\n",
                "    phi_episodic = 1.0 / (1.0 + np.exp(-logit_phi))\n",
                "    \n",
                "    # Softmax temperature: beta = exp(log_beta)\n",
                "    log_beta = theta[idx]; idx += 1\n",
                "    beta = np.exp(log_beta)\n",
                "    \n",
                "    # Exploration probability: epsilon = sigmoid(logit_epsilon)\n",
                "    logit_epsilon = theta[idx]; idx += 1\n",
                "    epsilon = 1.0 / (1.0 + np.exp(-logit_epsilon))\n",
                "    \n",
                "    return {\n",
                "        'phi_episodic': phi_episodic,\n",
                "        'beta': beta,\n",
                "        'epsilon': epsilon\n",
                "    }\n",
                "\n",
                "\n",
                "def pack_params_epi(phi_episodic: float, beta: float, epsilon: float) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Pack episodic model parameters into optimization vector.\n",
                "    \n",
                "    Parameters:\n",
                "    - phi_episodic: Episodic decay rate (0, 1)\n",
                "    - beta: Softmax inverse temperature (0, +inf)\n",
                "    - epsilon: Exploration probability (0, 1)\n",
                "    \n",
                "    Returns:\n",
                "    - Parameter vector\n",
                "    \"\"\"\n",
                "    return np.array([\n",
                "        np.log(phi_episodic / (1.0 - phi_episodic)),  # logit_phi\n",
                "        np.log(beta),                                  # log_beta\n",
                "        np.log(epsilon / (1.0 - epsilon))              # logit_epsilon\n",
                "    ], dtype=np.float64)\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Episodic Q-Value Computation\n",
                "# =============================================================================\n",
                "\n",
                "def compute_episodic_Q(\n",
                "    cluster_id: int,\n",
                "    current_time_angle: float,\n",
                "    memory: Memory,\n",
                "    episode_decay: bool = True\n",
                ") -> float:\n",
                "    \"\"\"\n",
                "    Compute episodic Q-value for a given action (cluster) at current time.\n",
                "\n",
                "    Q(action) = weighted_average(gain, weights = time_similarity * decay_trace)\n",
                "\n",
                "    This retrieves all historical records for this cluster and computes\n",
                "    a time-weighted average of gains, where:\n",
                "    - Time similarity: Gaussian similarity between current time and record time\n",
                "    - Decay trace: Decay factor based on days since record (from EpisodicRecord.trace)\n",
                "\n",
                "    Parameters:\n",
                "    - cluster_id: The action (cluster) to compute Q-value for\n",
                "    - current_time_angle: Current time of decision (in [0, 1) scale)\n",
                "    - memory: Memory object containing episodic_table with historical records\n",
                "    - episode_decay: Whether to apply decay weights (True) or use raw traces (False)\n",
                "\n",
                "    Returns:\n",
                "    - Episodic Q-value (float). Returns 0.0 if no historical records exist.\n",
                "    \"\"\"\n",
                "\n",
                "    # Get all historical records for this cluster\n",
                "    records = memory.get_records_for_action(cluster_id)\n",
                "    if not records:\n",
                "        return 0.0\n",
                "\n",
                "    gain_array = np.array([r.gain for r in records])\n",
                "    time_sim_array = np.array([compute_time_similarity(r.time_angle, current_time_angle) for r in records])\n",
                "\n",
                "    if episode_decay:\n",
                "        decay_array = np.array([r.trace for r in records])\n",
                "        weights = decay_array * time_sim_array\n",
                "    else:\n",
                "        weights = time_sim_array\n",
                "\n",
                "    # Weighted average of gains based on time similarity and decay\n",
                "    estimate_Q = np.average(gain_array, weights=weights)\n",
                "\n",
                "    return estimate_Q\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Episodic Simulation and Log-Likelihood\n",
                "# =============================================================================\n",
                "\n",
                "def simulate_and_loglik_epi(\n",
                "    theta: np.ndarray,\n",
                "    episodic_data: Dict[str, Any],\n",
                "    config: EpisodicConfig = None\n",
                ") -> float:\n",
                "    \"\"\"\n",
                "    Compute negative log-likelihood for episodic RL model.\n",
                "\n",
                "    This function simulates decision-making using episodic memory:\n",
                "\n",
                "    CAUSAL FLOW (Critical Design):\n",
                "    --------------------------------\n",
                "    1. At time t, decision is based on HISTORICAL memory only\n",
                "    2. Current action is recorded but NOT immediately added to memory\n",
                "    3. At day-end: all recorded actions are added to memory (update graph + episodic records)\n",
                "    4. At next-day start: memory decays based on phi_episodic\n",
                "\n",
                "    This ensures no information leakage: decisions use only what was known BEFORE.\n",
                "\n",
                "    Processing Order at Each Step t:\n",
                "    --------------------------------\n",
                "    1. Record current action (add to adopted_actions, cached_episodes)\n",
                "    2. Apply decay if new day started (based on prev_date)\n",
                "    3. Get active nodes (memory_strength >= threshold)\n",
                "    4. Compute Q-values from historical episodic records\n",
                "    5. Compute action probability via softmax(beta * Q)\n",
                "    6. Accumulate log-likelihood\n",
                "    7. At day-end: update memory (add_visit, add_episodic_record)\n",
                "\n",
                "    Parameters:\n",
                "    - theta: Parameter vector [phi_episodic, beta, epsilon]\n",
                "    - episodic_data: Prepared episodic data (states, actions, time_angles, gains, etc.)\n",
                "    - config: Episodic configuration\n",
                "\n",
                "    Returns:\n",
                "    - Negative log-likelihood (to be minimized)\n",
                "    \"\"\"\n",
                "    if config is None:\n",
                "        config = EpisodicConfig()\n",
                "\n",
                "    # Unpack parameters\n",
                "    params = unpack_params_epi(theta)\n",
                "    config.phi_episodic = params['phi_episodic']\n",
                "    beta = params['beta']\n",
                "    epsilon = params['epsilon']\n",
                "\n",
                "    # Initialize fresh memory for this user\n",
                "    base_memory = Memory(config)\n",
                "\n",
                "    # Extract prepared data\n",
                "    day_seq = episodic_data['day_seq']\n",
                "    time_angles = episodic_data['time_angles']\n",
                "    date_array = episodic_data['date_array']\n",
                "    action_seq = episodic_data['actions']\n",
                "    n_records = episodic_data['n_records']\n",
                "    same_day_next = episodic_data['same_day_next']\n",
                "    gains = episodic_data['gains']\n",
                "\n",
                "    # Tracking variables\n",
                "    prev_date = None           # Previous date for detecting day transitions\n",
                "    adopted_actions = set()    # Actions taken on current day (for memory update)\n",
                "    cached_episodes = list()   # Episode records for current day\n",
                "    loglik = 0.0\n",
                "\n",
                "    # Main simulation loop\n",
                "    for t in range(n_records):\n",
                "        a = int(action_seq[t])           # Current action\n",
                "        time_angle = float(time_angles[t])  # Current time angle\n",
                "        current_date = int(date_array[t])   # Current date\n",
                "        current_day_seq = int(day_seq[t])   # Current day sequence number\n",
                "\n",
                "        # --- Step 1: Record current action (but don't update memory yet) ---\n",
                "        adopted_actions.add(a)  # Track for day-end update\n",
                "\n",
                "        # Create episodic record (will be added to episodic_table at day-end)\n",
                "        cached_episodes.append(EpisodicRecord(\n",
                "            cluster_id=a,\n",
                "            time_angle=time_angle,\n",
                "            record_date=current_date,\n",
                "            gain=gains[t],\n",
                "        ))\n",
                "\n",
                "        # --- Step 2: Apply decay at start of new day ---\n",
                "        if prev_date is not None and current_date > prev_date:\n",
                "            prev_date_seq = day_seq[t-1]\n",
                "            date_diff = current_day_seq - prev_date_seq\n",
                "            assert date_diff > 0, f\"Date difference should be positive\"\n",
                "            base_memory.decay(date_diff)\n",
                "        prev_date = current_date\n",
                "\n",
                "        # --- Step 3 & 4: Compute Q-values from historical memory ---\n",
                "        active_nodes = base_memory.get_active_nodes()\n",
                "\n",
                "        # Available actions for Q-function:\n",
                "        # -9 (end trajectory): always available\n",
                "        # 0 (outside city): always available\n",
                "        # active clusters: only those with memory_strength >= threshold\n",
                "        available_actions = sorted([-9, 0] + list(active_nodes))\n",
                "\n",
                "        # Compute episodic Q-values for each available action\n",
                "        q_values = []\n",
                "        for action in available_actions:\n",
                "            Q = compute_episodic_Q(\n",
                "                action,\n",
                "                time_angle,\n",
                "                base_memory,\n",
                "                episode_decay=False  # Trace already applied in decay()\n",
                "            )\n",
                "            q_values.append(Q)\n",
                "\n",
                "        q_values = np.asarray(q_values, dtype=np.float64)\n",
                "\n",
                "        # --- Step 5: Compute action probability ---\n",
                "        # Softmax policy over Q-values\n",
                "        logits = beta * q_values\n",
                "        probs_exploit = np.exp(logits - logsumexp(logits))\n",
                "\n",
                "        # Map observed action to probability\n",
                "        action_prob = 0.0\n",
                "\n",
                "        if a == -1:\n",
                "            # Noise/explore action: always pure exploration\n",
                "            action_prob = epsilon\n",
                "        elif a in available_actions:\n",
                "            # Exploitation: (1-epsilon) * softmax probability\n",
                "            idx = available_actions.index(a)\n",
                "            if idx < len(probs_exploit):\n",
                "                action_prob = (1.0 - epsilon) * probs_exploit[idx]\n",
                "        else:\n",
                "            # Action in memory but forgotten (strength < threshold)\n",
                "            # Or action never recorded: treated as exploration\n",
                "            action_prob = epsilon\n",
                "\n",
                "        # --- Step 6: Accumulate log-likelihood ---\n",
                "        loglik += np.log(action_prob + 1e-12)\n",
                "\n",
                "        # --- Step 7: At day-end, update memory ---\n",
                "        if not same_day_next[t]:\n",
                "            # Add all visited actions to graph (update memory_strength)\n",
                "            for act in adopted_actions:\n",
                "                base_memory.add_visit(act)\n",
                "            adopted_actions.clear()\n",
                "\n",
                "            # Add all episode records to episodic_table (for future Q-computation)\n",
                "            for record in cached_episodes:\n",
                "                base_memory.add_episodic_record(record)\n",
                "            cached_episodes.clear()\n",
                "    \n",
                "    return -loglik\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Model Fitting\n",
                "# =============================================================================\n",
                "\n",
                "def fit_epi_model(\n",
                "    user_df: pd.DataFrame,\n",
                "    config: EpisodicConfig = None,\n",
                "    verbose: bool = True\n",
                ") -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Fit episodic RL model to a single user's trajectory data.\n",
                "    \n",
                "    Parameters:\n",
                "    - user_df: DataFrame with columns ['t_start', 't_end', 'cluster_id', 'date']\n",
                "    - config: Episodic configuration\n",
                "    - verbose: Whether to print progress\n",
                "    \n",
                "    Returns:\n",
                "    - Dictionary with fitted parameters and statistics\n",
                "    \"\"\"\n",
                "    \n",
                "    if config is None:\n",
                "        config = EpisodicConfig()\n",
                "    \n",
                "    # Prepare episodic data\n",
                "    episodic_data = prepare_episodic_data(user_df, config)\n",
                "    n_records = episodic_data['n_records']\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"Preparing Episodic model for {len(user_df)} visits, {n_records} actions...\")\n",
                "    \n",
                "    # Initial parameters\n",
                "    initial_theta = pack_params_epi(\n",
                "        phi_episodic=config.phi_episodic,\n",
                "        beta=config.beta_init,\n",
                "        epsilon=config.epsilon_init\n",
                "    )\n",
                "    \n",
                "    # Optimize\n",
                "    with warnings.catch_warnings():\n",
                "        warnings.simplefilter(\"ignore\")\n",
                "        result = minimize(\n",
                "            simulate_and_loglik_epi,\n",
                "            initial_theta,\n",
                "            args=(episodic_data, config),\n",
                "            method='L-BFGS-B',\n",
                "            options={'maxiter': config.maxiter, 'ftol': config.ftol}\n",
                "        )\n",
                "    \n",
                "    # Extract fitted parameters\n",
                "    fitted_params = unpack_params_epi(result.x)\n",
                "    final_loglik = -result.fun\n",
                "    \n",
                "    # Compute model selection criteria\n",
                "    k_params = 3  # phi_episodic, beta, epsilon\n",
                "    n_obs = n_records\n",
                "    AIC = 2 * k_params - 2 * final_loglik\n",
                "    BIC = k_params * np.log(n_obs) - 2 * final_loglik if n_obs > 0 else np.inf\n",
                "    \n",
                "    summary = {\n",
                "        'n_records': int(n_records),\n",
                "        'log_likelihood': float(final_loglik),\n",
                "        'AIC': float(AIC),\n",
                "        'BIC': float(BIC),\n",
                "        'phi_episodic': float(fitted_params['phi_episodic']),\n",
                "        'beta': float(fitted_params['beta']),\n",
                "        'epsilon': float(fitted_params['epsilon']),\n",
                "        'converged': result.success,\n",
                "        'n_iterations': result.nit if hasattr(result, 'nit') else None,\n",
                "        'optimization_message': result.message if hasattr(result, 'message') else None\n",
                "    }\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"Episodic model fitting {'converged' if result.success else 'did not converge'}.\")\n",
                "        print(f\"  Log-likelihood: {final_loglik:.2f}, AIC: {AIC:.2f}, BIC: {BIC:.2f}\")\n",
                "        print(f\"  phi_episodic (decay): {fitted_params['phi_episodic']:.4f}\")\n",
                "        print(f\"  beta (softmax temp): {fitted_params['beta']:.4f}\")\n",
                "        print(f\"  epsilon (explore): {fitted_params['epsilon']:.4f}\")\n",
                "    \n",
                "    return summary\n",
                "\n",
                "\n",
                "def fit_epi_for_all_users(\n",
                "    users_dict: Dict[int, Any],\n",
                "    config: EpisodicConfig = None,\n",
                "    sample_size: Optional[int] = None,\n",
                "    verbose: bool = True\n",
                ") -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Fit episodic RL model for all users in the dictionary.\n",
                "    \n",
                "    Parameters:\n",
                "    - users_dict: Dictionary mapping user_id to User objects\n",
                "    - config: Episodic configuration\n",
                "    - sample_size: Number of users to fit (None for all)\n",
                "    - verbose: Whether to print progress\n",
                "    \n",
                "    Returns:\n",
                "    - DataFrame with fitted parameters for each user\n",
                "    \"\"\"\n",
                "    if config is None:\n",
                "        config = EpisodicConfig()\n",
                "    \n",
                "    user_ids = list(users_dict.keys())\n",
                "    if sample_size is not None:\n",
                "        user_ids = user_ids[:sample_size]\n",
                "    \n",
                "    results = []\n",
                "    for i, user_id in enumerate(user_ids):\n",
                "        if verbose:\n",
                "            print(f\"[{i+1}/{len(user_ids)}] Fitting Episodic for user {user_id}...\", end=\"\")\n",
                "        \n",
                "        user = users_dict[user_id]\n",
                "        user_df = user.to_dataframe()\n",
                "        \n",
                "        try:\n",
                "            t_start = time.time()\n",
                "            result = fit_epi_model(user_df, config, verbose=False)\n",
                "            t_end = time.time()\n",
                "            elapsed = t_end - t_start\n",
                "            if verbose:\n",
                "                print(f\"\\tUser {user_id} Episodic model fit time: {elapsed:.2f} seconds\")\n",
                "            result['user_id'] = user_id\n",
                "            result['fit_time_seconds'] = elapsed\n",
                "            results.append(result)\n",
                "        except Exception as e:\n",
                "            if verbose:\n",
                "                print(f\"  Error fitting user {user_id}: {e}\")\n",
                "            results.append({\n",
                "                'user_id': user_id,\n",
                "                'n_records': 0,\n",
                "                'log_likelihood': np.nan,\n",
                "                'AIC': np.nan,\n",
                "                'BIC': np.nan,\n",
                "                'phi_episodic': np.nan,\n",
                "                'beta': np.nan,\n",
                "                'epsilon': np.nan,\n",
                "                'converged': False,\n",
                "                'n_iterations': None,\n",
                "                'optimization_message': str(e)\n",
                "            })\n",
                "    \n",
                "    return pd.DataFrame(results)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "id": "7be79102",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Testing different phi values:\n",
                        "phi=0.01: loglik=-8035.39, active_nodes=33\n",
                        "phi=0.10: loglik=-7260.12, active_nodes=14\n",
                        "phi=0.40: loglik=-5954.11, active_nodes=6\n",
                        "phi=0.90: loglik=-4338.02, active_nodes=2\n",
                        "\n",
                        "Log-likelihood range: 3697.3654\n",
                        "闂?phi does affect the model.\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# Diagnostic: Check if phi_episodic is actually used in optimization\n",
                "# =============================================================================\n",
                "\n",
                "def simulate_with_different_phi(theta, episodic_data, phi_values=[0.01, 0.1, 0.4, 0.9], verbose=True):\n",
                "    \"\"\"\n",
                "    Test the model with different phi values to see if they produce different log-likelihoods.\n",
                "    \"\"\"\n",
                "    results = []\n",
                "\n",
                "    for phi in phi_values:\n",
                "        config = EpisodicConfig(\n",
                "            visit_threshold=3,\n",
                "            phi_episodic=phi,\n",
                "            memory_threshold=0.1,\n",
                "            time_sim_std=1.0/12.0,\n",
                "            epsilon_init=0.1,\n",
                "            beta_init=1.0,\n",
                "        )\n",
                "\n",
                "        params = unpack_params_epi(theta)\n",
                "        beta = params['beta']\n",
                "        epsilon = params['epsilon']\n",
                "        base_memory = Memory(config)\n",
                "\n",
                "        day_seq = episodic_data['day_seq']\n",
                "        time_angles = episodic_data['time_angles']\n",
                "        date_array = episodic_data['date_array']\n",
                "        action_seq = episodic_data['actions']\n",
                "        n_records = episodic_data['n_records']\n",
                "        same_day_next = episodic_data['same_day_next']\n",
                "        gains = episodic_data['gains']\n",
                "\n",
                "        prev_date = None\n",
                "        adopted_actions = set()\n",
                "        cached_episodes = list()\n",
                "        loglik = 0.0\n",
                "\n",
                "        for t in range(n_records):\n",
                "            a = int(action_seq[t])\n",
                "            time_angle = float(time_angles[t])\n",
                "            current_date = int(date_array[t])\n",
                "            current_day_seq = int(day_seq[t])\n",
                "\n",
                "            adopted_actions.add(a)\n",
                "            cached_episodes.append(EpisodicRecord(\n",
                "                cluster_id=a, time_angle=time_angle,\n",
                "                record_date=current_date, gain=gains[t],\n",
                "            ))\n",
                "\n",
                "            if prev_date is not None and current_date > prev_date:\n",
                "                prev_date_seq = day_seq[t-1]\n",
                "                date_diff = current_day_seq - prev_date_seq\n",
                "                base_memory.decay(date_diff)\n",
                "            prev_date = current_date\n",
                "\n",
                "            active_nodes = base_memory.get_active_nodes()\n",
                "            available_actions = sorted([-9, 0] + list(active_nodes))\n",
                "\n",
                "            q_values = []\n",
                "            for action in available_actions:\n",
                "                Q = compute_episodic_Q(action, time_angle, base_memory, episode_decay=False)\n",
                "                q_values.append(Q)\n",
                "            q_values = np.asarray(q_values, dtype=np.float64)\n",
                "\n",
                "            logits = beta * q_values\n",
                "            probs_exploit = np.exp(logits - logsumexp(logits))\n",
                "\n",
                "            if a == -1:\n",
                "                action_prob = epsilon\n",
                "            elif a in available_actions:\n",
                "                idx = available_actions.index(a)\n",
                "                action_prob = (1.0 - epsilon) * probs_exploit[idx]\n",
                "            else:\n",
                "                action_prob = epsilon\n",
                "\n",
                "            loglik += np.log(action_prob + 1e-12)\n",
                "\n",
                "            if not same_day_next[t]:\n",
                "                for act in adopted_actions:\n",
                "                    base_memory.add_visit(act)\n",
                "                adopted_actions.clear()\n",
                "                for record in cached_episodes:\n",
                "                    base_memory.add_episodic_record(record)\n",
                "                cached_episodes.clear()\n",
                "\n",
                "        results.append({\n",
                "            'phi': phi,\n",
                "            'loglik': loglik,\n",
                "            'n_active_nodes': len(base_memory.get_active_nodes())\n",
                "        })\n",
                "\n",
                "        if verbose:\n",
                "            print(f\"phi={phi:.2f}: loglik={loglik:.2f}, active_nodes={len(base_memory.get_active_nodes())}\")\n",
                "\n",
                "    return results\n",
                "\n",
                "\n",
                "# Run diagnostic\n",
                "sample_user_id = list(users_dict.keys())[0]\n",
                "sample_df = users_dict[sample_user_id].to_dataframe()\n",
                "episodic_data = prepare_episodic_data(sample_df)\n",
                "theta = pack_params_epi(phi_episodic=0.4, beta=1.0, epsilon=0.1)\n",
                "\n",
                "print(\"Testing different phi values:\")\n",
                "results = simulate_with_different_phi(theta, episodic_data)\n",
                "\n",
                "range_val = max(r['loglik'] for r in results) - min(r['loglik'] for r in results)\n",
                "print(f\"\\nLog-likelihood range: {range_val:.4f}\")\n",
                "\n",
                "if range_val < 1.0:\n",
                "    print(\"闂備礁鐤囧▔鏇熷垔鐎靛摜绠? WARNING: phi has almost NO effect on log-likelihood!\")\n",
                "else:\n",
                "    print(\"闂?phi does affect the model.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "id": "5bd4fc6c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Effect of phi on exploration:\n",
                        "phi=0.01: loglik=-8035.4, active=33, explore_rate=29.17%\n",
                        "phi=0.10: loglik=-7260.1, active=14, explore_rate=32.88%\n",
                        "phi=0.20: loglik=-6702.7, active=9, explore_rate=35.23%\n",
                        "phi=0.40: loglik=-5954.1, active=6, explore_rate=40.91%\n",
                        "phi=0.60: loglik=-5289.0, active=4, explore_rate=47.95%\n",
                        "phi=0.80: loglik=-4551.8, active=2, explore_rate=54.55%\n",
                        "phi=0.90: loglik=-4338.0, active=2, explore_rate=56.74%\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# Diagnostic: Detailed phi optimization analysis\n",
                "# =============================================================================\n",
                "\n",
                "def detailed_phi_analysis(theta, episodic_data, phi_values, verbose=True):\n",
                "    results = []\n",
                "\n",
                "    for phi in phi_values:\n",
                "        config = EpisodicConfig(\n",
                "            visit_threshold=3, phi_episodic=phi, memory_threshold=0.1,\n",
                "            time_sim_std=1.0/12.0, epsilon_init=0.1, beta_init=1.0,\n",
                "        )\n",
                "\n",
                "        params = unpack_params_epi(theta)\n",
                "        beta = params['beta']\n",
                "        epsilon = params['epsilon']\n",
                "        base_memory = Memory(config)\n",
                "\n",
                "        day_seq = episodic_data['day_seq']\n",
                "        time_angles = episodic_data['time_angles']\n",
                "        date_array = episodic_data['date_array']\n",
                "        action_seq = episodic_data['actions']\n",
                "        n_records = episodic_data['n_records']\n",
                "        same_day_next = episodic_data['same_day_next']\n",
                "        gains = episodic_data['gains']\n",
                "\n",
                "        prev_date = None\n",
                "        adopted_actions = set()\n",
                "        cached_episodes = list()\n",
                "        loglik = 0.0\n",
                "        n_explored = 0\n",
                "\n",
                "        for t in range(n_records):\n",
                "            a = int(action_seq[t])\n",
                "            time_angle = float(time_angles[t])\n",
                "            current_date = int(date_array[t])\n",
                "            current_day_seq = int(day_seq[t])\n",
                "\n",
                "            adopted_actions.add(a)\n",
                "            cached_episodes.append(EpisodicRecord(\n",
                "                cluster_id=a, time_angle=time_angle,\n",
                "                record_date=current_date, gain=gains[t],\n",
                "            ))\n",
                "\n",
                "            if prev_date is not None and current_date > prev_date:\n",
                "                base_memory.decay(current_day_seq - day_seq[t-1])\n",
                "            prev_date = current_date\n",
                "\n",
                "            active_nodes = base_memory.get_active_nodes()\n",
                "            available_actions = sorted([-9, 0] + list(active_nodes))\n",
                "\n",
                "            if a not in available_actions:\n",
                "                n_explored += 1\n",
                "\n",
                "            q_values = [compute_episodic_Q(a, time_angle, base_memory, episode_decay=False) \n",
                "                       for a in available_actions]\n",
                "            q_values = np.asarray(q_values, dtype=np.float64)\n",
                "\n",
                "            logits = beta * q_values\n",
                "            probs_exploit = np.exp(logits - logsumexp(logits))\n",
                "\n",
                "            if a == -1:\n",
                "                action_prob = epsilon\n",
                "            elif a in available_actions:\n",
                "                idx = available_actions.index(a)\n",
                "                action_prob = (1.0 - epsilon) * probs_exploit[idx]\n",
                "            else:\n",
                "                action_prob = epsilon\n",
                "\n",
                "            loglik += np.log(action_prob + 1e-12)\n",
                "\n",
                "            if not same_day_next[t]:\n",
                "                for act in adopted_actions:\n",
                "                    base_memory.add_visit(act)\n",
                "                adopted_actions.clear()\n",
                "                for record in cached_episodes:\n",
                "                    base_memory.add_episodic_record(record)\n",
                "                cached_episodes.clear()\n",
                "\n",
                "        results.append({\n",
                "            'phi': phi, 'loglik': loglik,\n",
                "            'n_active_final': len(base_memory.get_active_nodes()),\n",
                "            'explore_rate': n_explored / n_records\n",
                "        })\n",
                "\n",
                "        if verbose:\n",
                "            print(f\"phi={phi:.2f}: loglik={loglik:.1f}, active={len(base_memory.get_active_nodes())}, \"\n",
                "                  f\"explore_rate={n_explored/n_records:.2%}\")\n",
                "\n",
                "    return results\n",
                "\n",
                "\n",
                "# Run\n",
                "sample_user_id = list(users_dict.keys())[0]\n",
                "sample_df = users_dict[sample_user_id].to_dataframe()\n",
                "episodic_data = prepare_episodic_data(sample_df)\n",
                "theta = pack_params_epi(phi_episodic=0.4, beta=1.0, epsilon=0.1)\n",
                "\n",
                "print(\"Effect of phi on exploration:\")\n",
                "results = detailed_phi_analysis(theta, episodic_data, [0.01, 0.1, 0.2, 0.4, 0.6, 0.8, 0.9])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d63e67fc",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "17cc71dc",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "id": "539be3ed",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Episodic RL Model Fitting\n",
                        "Features: Episodic memory, Time-weighted Q retrieval, Memory decay\n",
                        "============================================================\n",
                        "[1/10] Fitting Episodic for user 126272...\tUser 126272 Episodic model fit time: 170.71 seconds\n",
                        "[2/10] Fitting Episodic for user 278978...\tUser 278978 Episodic model fit time: 128.25 seconds\n",
                        "[3/10] Fitting Episodic for user 395753...\tUser 395753 Episodic model fit time: 184.04 seconds\n",
                        "[4/10] Fitting Episodic for user 506035...\tUser 506035 Episodic model fit time: 206.65 seconds\n",
                        "[5/10] Fitting Episodic for user 612431...\tUser 612431 Episodic model fit time: 275.18 seconds\n",
                        "[6/10] Fitting Episodic for user 661336...\tUser 661336 Episodic model fit time: 351.99 seconds\n",
                        "[7/10] Fitting Episodic for user 824617...\tUser 824617 Episodic model fit time: 8.21 seconds\n",
                        "[8/10] Fitting Episodic for user 928827...\tUser 928827 Episodic model fit time: 281.36 seconds\n",
                        "[9/10] Fitting Episodic for user 1017498...\tUser 1017498 Episodic model fit time: 55.59 seconds\n",
                        "[10/10] Fitting Episodic for user 1159109...\tUser 1159109 Episodic model fit time: 238.66 seconds\n",
                        "\n",
                        "Episodic Model Fitting Summary:\n",
                        " user_id  n_records  log_likelihood         AIC         BIC  phi_episodic     beta  epsilon\n",
                        "  126272       1320    -3290.074297 6586.148594 6601.704755           0.1 0.000002 0.313444\n",
                        "  278978       1182    -2898.682577 5803.365155 5818.590044           0.1 0.000001 0.270796\n",
                        "  395753       1537    -3875.945745 7757.891491 7773.904254           0.1 0.000026 0.349381\n",
                        "  506035       1592    -3772.555226 7551.110452 7567.228692           0.1 0.000009 0.384961\n",
                        "  612431       1626    -4386.427157 8778.854315 8795.035950           0.1 0.000001 0.305764\n",
                        "  661336       1860    -4629.309796 9264.619591 9281.204586           0.1 0.000002 0.442744\n",
                        "  824617        446     -796.156215 1598.312429 1610.613386           0.1 0.026313 0.177153\n",
                        "  928827       1838    -4570.018745 9146.037490 9162.586790           0.1 0.000001 0.450441\n",
                        " 1017498        975    -2298.139307 4602.278614 4616.925926           0.1 0.000002 0.239988\n",
                        " 1159109       1805    -4853.938693 9713.877385 9730.372333           0.1 0.000003 0.318563\n",
                        "\n",
                        "Episodic estimation results saved to 'epi_estimation_results.csv'\n",
                        "\n",
                        "Aggregate Statistics Across Users:\n",
                        "  Average phi_episodic (decay): 0.1000 闂?0.0000\n",
                        "  Average beta (softmax temp): 0.0026 闂?0.0083\n",
                        "  Average epsilon (explore): 0.3253 闂?0.0859\n",
                        "  Average log-likelihood: -3537.12 闂?1265.00\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# Episodic RL Model Fitting (Sample Test)\n",
                "# =============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Episodic RL Model Fitting\")\n",
                "print(\"Features: Episodic memory, Time-weighted Q retrieval, Memory decay\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# 1. Create configuration\n",
                "epi_config = EpisodicConfig(\n",
                "    visit_threshold=3,\n",
                "    phi_episodic=0.1,\n",
                "    memory_threshold=0.01,\n",
                "    time_sim_std=1.0/12.0,\n",
                "    epsilon_init=0.1,\n",
                "    beta_init=1.0,\n",
                "    maxiter=1000,\n",
                "    ftol=1e-6\n",
                ")\n",
                "\n",
                "# 2. Fit Episodic model for sample users\n",
                "epi_results = fit_epi_for_all_users(\n",
                "    users_dict,\n",
                "    config=epi_config,\n",
                "    sample_size=10,  # Change to None for all users\n",
                "    verbose=True\n",
                ")\n",
                "\n",
                "print(\"\\nEpisodic Model Fitting Summary:\")\n",
                "print(epi_results[['user_id', 'n_records', 'log_likelihood', 'AIC', 'BIC',\n",
                "                   'phi_episodic', 'beta', 'epsilon']].to_string(index=False))\n",
                "\n",
                "# Save results\n",
                "epi_results.to_csv('epi_estimation_results.csv', index=False)\n",
                "print(f\"\\nEpisodic estimation results saved to 'epi_estimation_results.csv'\")\n",
                "\n",
                "# Aggregate statistics\n",
                "print(\"\\nAggregate Statistics Across Users:\")\n",
                "print(f\"  Average phi_episodic (decay): {epi_results['phi_episodic'].mean():.4f} 闂?{epi_results['phi_episodic'].std():.4f}\")\n",
                "print(f\"  Average beta (softmax temp): {epi_results['beta'].mean():.4f} 闂?{epi_results['beta'].std():.4f}\")\n",
                "print(f\"  Average epsilon (explore): {epi_results['epsilon'].mean():.4f} 闂?{epi_results['epsilon'].std():.4f}\")\n",
                "print(f\"  Average log-likelihood: {epi_results['log_likelihood'].mean():.2f} 闂?{epi_results['log_likelihood'].std():.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "id": "a0a2c0fe",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Episodic RL Model Fitting\n",
                        "Features: Episodic memory, Time-weighted Q retrieval, Memory decay\n",
                        "============================================================\n",
                        "[1/10] Fitting Episodic for user 126272...\tUser 126272 Episodic model fit time: 149.96 seconds\n",
                        "[2/10] Fitting Episodic for user 278978...\tUser 278978 Episodic model fit time: 114.38 seconds\n",
                        "[3/10] Fitting Episodic for user 395753...\tUser 395753 Episodic model fit time: 196.55 seconds\n",
                        "[4/10] Fitting Episodic for user 506035...\tUser 506035 Episodic model fit time: 242.20 seconds\n",
                        "[5/10] Fitting Episodic for user 612431...\tUser 612431 Episodic model fit time: 355.89 seconds\n",
                        "[6/10] Fitting Episodic for user 661336...\tUser 661336 Episodic model fit time: 487.11 seconds\n",
                        "[7/10] Fitting Episodic for user 824617...\tUser 824617 Episodic model fit time: 20.37 seconds\n",
                        "[8/10] Fitting Episodic for user 928827...\tUser 928827 Episodic model fit time: 490.92 seconds\n",
                        "[9/10] Fitting Episodic for user 1017498...\tUser 1017498 Episodic model fit time: 106.61 seconds\n",
                        "[10/10] Fitting Episodic for user 1159109...\tUser 1159109 Episodic model fit time: 472.73 seconds\n",
                        "\n",
                        "Episodic Model Fitting Summary:\n",
                        " user_id  n_records  log_likelihood          AIC          BIC  phi_episodic     beta  epsilon\n",
                        "  126272       1320    -3638.481101  7282.962203  7298.518364      0.069497 0.000001 0.291637\n",
                        "  278978       1182    -3247.985589  6501.971178  6517.196067      0.060083 0.000001 0.249570\n",
                        "  395753       1537    -4324.712829  8655.425658  8671.438421      0.065970 0.000014 0.331257\n",
                        "  506035       1592    -4204.317627  8414.635255  8430.753494      0.387585 0.000001 0.364485\n",
                        "  612431       1626    -4730.747851  9467.495702  9483.677337      0.257593 0.000003 0.284181\n",
                        "  661336       1860    -5005.555584 10017.111168 10033.696163      0.123828 0.000002 0.417753\n",
                        "  824617        446     -928.341473  1862.682946  1874.983903      0.226279 0.000003 0.177158\n",
                        "  928827       1838    -4942.298228  9890.596456  9907.145756      0.344616 0.000003 0.423317\n",
                        " 1017498        975    -2502.938672  5011.877344  5026.524656      0.526066 0.000001 0.225515\n",
                        " 1159109       1805    -5358.706636 10723.413273 10739.908220      0.866462 0.000002 0.299212\n",
                        "\n",
                        "Episodic estimation results saved to 'epi_estimation_results.csv'\n",
                        "\n",
                        "Aggregate Statistics Across Users:\n",
                        "  Average phi_episodic (decay): 0.2928 闂?0.2549\n",
                        "  Average beta (softmax temp): 0.0000 闂?0.0000\n",
                        "  Average epsilon (explore): 0.3064 闂?0.0798\n",
                        "  Average log-likelihood: -3888.41 闂?1361.14\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# Episodic RL Model Fitting (Sample Test)\n",
                "# =============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Episodic RL Model Fitting\")\n",
                "print(\"Features: Episodic memory, Time-weighted Q retrieval, Memory decay\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# 1. Create configuration\n",
                "epi_config = EpisodicConfig(\n",
                "    visit_threshold=3,\n",
                "    phi_episodic=0.1,\n",
                "    memory_threshold=0,\n",
                "    time_sim_std=1.0/12.0,\n",
                "    epsilon_init=0.1,\n",
                "    beta_init=1.0,\n",
                "    maxiter=1000,\n",
                "    ftol=1e-6\n",
                ")\n",
                "\n",
                "# 2. Fit Episodic model for sample users\n",
                "epi_results = fit_epi_for_all_users(\n",
                "    users_dict,\n",
                "    config=epi_config,\n",
                "    sample_size=10,  # Change to None for all users\n",
                "    verbose=True\n",
                ")\n",
                "\n",
                "print(\"\\nEpisodic Model Fitting Summary:\")\n",
                "print(epi_results[['user_id', 'n_records', 'log_likelihood', 'AIC', 'BIC',\n",
                "                   'phi_episodic', 'beta', 'epsilon']].to_string(index=False))\n",
                "\n",
                "# Save results\n",
                "epi_results.to_csv('epi_estimation_results.csv', index=False)\n",
                "print(f\"\\nEpisodic estimation results saved to 'epi_estimation_results.csv'\")\n",
                "\n",
                "# Aggregate statistics\n",
                "print(\"\\nAggregate Statistics Across Users:\")\n",
                "print(f\"  Average phi_episodic (decay): {epi_results['phi_episodic'].mean():.4f} 闂?{epi_results['phi_episodic'].std():.4f}\")\n",
                "print(f\"  Average beta (softmax temp): {epi_results['beta'].mean():.4f} 闂?{epi_results['beta'].std():.4f}\")\n",
                "print(f\"  Average epsilon (explore): {epi_results['epsilon'].mean():.4f} 闂?{epi_results['epsilon'].std():.4f}\")\n",
                "print(f\"  Average log-likelihood: {epi_results['log_likelihood'].mean():.2f} 闂?{epi_results['log_likelihood'].std():.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "id": "83bf2f1e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Episodic RL Model Fitting\n",
                        "Features: Episodic memory, Time-weighted Q retrieval, Memory decay\n",
                        "============================================================\n",
                        "[1/10] Fitting Episodic for user 126272...\tUser 126272 Episodic model fit time: 194.47 seconds\n",
                        "[2/10] Fitting Episodic for user 278978...\tUser 278978 Episodic model fit time: 165.09 seconds\n",
                        "[3/10] Fitting Episodic for user 395753...\tUser 395753 Episodic model fit time: 314.74 seconds\n",
                        "[4/10] Fitting Episodic for user 506035...\tUser 506035 Episodic model fit time: 1423.11 seconds\n",
                        "[5/10] Fitting Episodic for user 612431...\tUser 612431 Episodic model fit time: 15273.50 seconds\n",
                        "[6/10] Fitting Episodic for user 661336...\tUser 661336 Episodic model fit time: 513.82 seconds\n",
                        "[7/10] Fitting Episodic for user 824617...\tUser 824617 Episodic model fit time: 12.90 seconds\n",
                        "[8/10] Fitting Episodic for user 928827...\tUser 928827 Episodic model fit time: 594.02 seconds\n",
                        "[9/10] Fitting Episodic for user 1017498...\tUser 1017498 Episodic model fit time: 107.40 seconds\n",
                        "[10/10] Fitting Episodic for user 1159109...\tUser 1159109 Episodic model fit time: 501.42 seconds\n",
                        "\n",
                        "Episodic Model Fitting Summary:\n",
                        " user_id  n_records  log_likelihood          AIC          BIC  phi_episodic         beta  epsilon\n",
                        "  126272       1320    -3638.507421  7283.014841  7298.571002           0.1 1.753404e-05 0.291707\n",
                        "  278978       1182    -3247.985705  6501.971410  6517.196300           0.1 1.585301e-06 0.249572\n",
                        "  395753       1537    -4324.671569  8655.343138  8671.355901           0.1 6.568097e-07 0.331856\n",
                        "  506035       1592    -4204.326518  8414.653035  8430.771275           0.1 3.808199e-06 0.363321\n",
                        "  612431       1626    -4730.774699  9467.549398  9483.731033           0.1 1.804743e-05 0.284470\n",
                        "  661336       1860    -5005.555217 10017.110435 10033.695430           0.1 1.577436e-06 0.417656\n",
                        "  824617        446     -928.341418  1862.682835  1874.983792           0.1 2.081995e-06 0.176982\n",
                        "  928827       1838    -4942.296594  9890.593187  9907.142487           0.1 2.166675e-06 0.423284\n",
                        " 1017498        975    -2502.938214  5011.876428  5026.523741           0.1 1.149894e-06 0.225637\n",
                        " 1159109       1805    -5358.738544 10723.477088 10739.972035           0.1 1.974952e-05 0.299796\n",
                        "\n",
                        "Episodic estimation results saved to 'epi_estimation_results.csv'\n",
                        "\n",
                        "Aggregate Statistics Across Users:\n",
                        "  Average phi_episodic (decay): 0.1000 闂?0.0000\n",
                        "  Average beta (softmax temp): 0.0000 闂?0.0000\n",
                        "  Average epsilon (explore): 0.3064 闂?0.0797\n",
                        "  Average log-likelihood: -3888.41 闂?1361.14\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# Episodic RL Model Fitting (Sample Test)\n",
                "# =============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Episodic RL Model Fitting\")\n",
                "print(\"Features: Episodic memory, Time-weighted Q retrieval, Memory decay\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# 1. Create configuration\n",
                "epi_config = EpisodicConfig(\n",
                "    visit_threshold=3,\n",
                "    phi_episodic=0.1,\n",
                "    memory_threshold=0,\n",
                "    time_sim_std=1.0/12.0,\n",
                "    epsilon_init=0.1,\n",
                "    beta_init=1.0,\n",
                "    maxiter=1000,\n",
                "    ftol=1e-6\n",
                ")\n",
                "\n",
                "# 2. Fit Episodic model for sample users\n",
                "epi_results = fit_epi_for_all_users(\n",
                "    users_dict,\n",
                "    config=epi_config,\n",
                "    sample_size=10,  # Change to None for all users\n",
                "    verbose=True\n",
                ")\n",
                "\n",
                "print(\"\\nEpisodic Model Fitting Summary:\")\n",
                "print(epi_results[['user_id', 'n_records', 'log_likelihood', 'AIC', 'BIC',\n",
                "                   'phi_episodic', 'beta', 'epsilon']].to_string(index=False))\n",
                "\n",
                "# Save results\n",
                "epi_results.to_csv('epi_estimation_results.csv', index=False)\n",
                "print(f\"\\nEpisodic estimation results saved to 'epi_estimation_results.csv'\")\n",
                "\n",
                "# Aggregate statistics\n",
                "print(\"\\nAggregate Statistics Across Users:\")\n",
                "print(f\"  Average phi_episodic (decay): {epi_results['phi_episodic'].mean():.4f} 闂?{epi_results['phi_episodic'].std():.4f}\")\n",
                "print(f\"  Average beta (softmax temp): {epi_results['beta'].mean():.4f} 闂?{epi_results['beta'].std():.4f}\")\n",
                "print(f\"  Average epsilon (explore): {epi_results['epsilon'].mean():.4f} 闂?{epi_results['epsilon'].std():.4f}\")\n",
                "print(f\"  Average log-likelihood: {epi_results['log_likelihood'].mean():.2f} 闂?{epi_results['log_likelihood'].std():.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ef207572",
            "metadata": {},
            "source": [
                "## Successor Representation (SR)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6cca2067",
            "metadata": {},
            "source": [
                "This method should ideally be integrated with the classic Dyna architecture. Here we prospect its structure.\n",
                "\n",
                "Initialize:\n",
                "* A network graph containing all node states, for random walk.\n",
                "* A null world model, with the ability to online learning.\n",
                "* A huge null sparse matrix to record the Successsor Representation (SR).\n",
                "\n",
                "Procedures:\n",
                "The agent decides to exploit or explore based on the world model and SR today.\n",
                "At the end of the day:\n",
                "1 Collect all \n",
                "2 \n",
                "3 "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed73c106",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# SR Dyna: Model-Based RL using Successor Representation (Revised)\n",
                "# =============================================================================\n",
                "\"\"\"\n",
                "SR Dyna combines Successor Representation (SR) with Dyna architecture.\n",
                "\n",
                "Key Concept (No gamma discount - trajectories end within day):\n",
                "- SR[s][a][s'] = Expected visit count to future state s' \n",
                "                  from state s taking action a, WITHIN THE SAME DAY\n",
                "- Q(s, a) = Σ_{s'} SR[s][a][s'] * R(s')  (weighted sum of rewards)\n",
                "\n",
                "Key Differences from MF Episodic Memory:\n",
                "- MF stores: Q-values (cumulative rewards from time t to end of day)\n",
                "- SR stores: Visit counts to each future state s' from (s,a)\n",
                "- Both use episodic memory + time similarity kernel for generalization\n",
                "- Both can reuse EpisodicRecord-like structure\n",
                "\n",
                "Algorithm:\n",
                "1. Collect real trajectory data\n",
                "2. Update SR from real transitions: SR[s,a][s'] += 1 for observed transition\n",
                "3. Dyna planning: simulate N steps using learned model, update SR\n",
                "4. Compute Q-values from SR: Q(s,a) = SR[s,a] · R (weighted by rewards)\n",
                "5. Use Q-values for action selection (softmax policy)\n",
                "\n",
                "Note: No gamma discount needed because all trajectories end within the day.\n",
                "\"\"\"\n",
                "\n",
                "from typing import Dict, List, Tuple, Optional, Any, Set\n",
                "from dataclasses import dataclass, field\n",
                "from collections import defaultdict\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import warnings\n",
                "from scipy.optimize import minimize\n",
                "import time\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# SR Dyna Configuration\n",
                "# =============================================================================\n",
                "\n",
                "@dataclass\n",
                "class SRDynaConfig:\n",
                "    \"\"\"Configuration for SR Dyna model.\"\"\"\n",
                "    # Learning rates\n",
                "    alpha_init: float = 0.1       # SR learning rate (for real transitions)\n",
                "    alpha_plan: float = 0.1       # Planning SR learning rate\n",
                "    \n",
                "    # Policy parameters\n",
                "    beta_init: float = 1.0        # Softmax temperature\n",
                "    epsilon_init: float = 0.1     # Exploration rate\n",
                "    \n",
                "    # Forgetting (memory decay between days, NO gamma - no discount within day)\n",
                "    phi_init: float = 0.1         # Memory decay rate between days\n",
                "    \n",
                "    # Dyna planning\n",
                "    n_planning_steps: int = 10     # Number of simulated steps per real step\n",
                "    \n",
                "    # Reward function\n",
                "    reward_type: str = 'log'\n",
                "    reward_param_init: float = 1.0\n",
                "    \n",
                "    # Memory thresholds\n",
                "    visit_threshold: int = 3       # Visits required to add node to active set\n",
                "    memory_threshold: float = 0.01  # Memory strength threshold\n",
                "    \n",
                "    # Time similarity kernel\n",
                "    sigma_t_init: float = 1.0 / 12.0  # 2 hours = 2/24 in time_angle units\n",
                "    \n",
                "    # Optimization settings\n",
                "    maxiter: int = 1000\n",
                "    ftol: float = 1e-6\n",
                "    \n",
                "    # Reference date\n",
                "    ref_date: Tuple[int, int, int] = (2018, 12, 31)\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# SR Record: Similar to EpisodicRecord but stores visit counts instead of Q-values\n",
                "# =============================================================================\n",
                "\n",
                "@dataclass\n",
                "class SRRecord:\n",
                "    \"\"\"\n",
                "    A single SR memory record for one (state, action) pair at a specific time.\n",
                "    \n",
                "    Unlike EpisodicRecord which stores Q-values (gains), this stores:\n",
                "    - future_state: The destination state visited\n",
                "    - visit_count: Number of visits to this future state from (s,a)\n",
                "    - time_angle: Time when the action was taken\n",
                "    - day_seq: Day sequence number\n",
                "    - record_date: Date of the record\n",
                "    - strength: Memory strength (for decay)\n",
                "    \"\"\"\n",
                "    future_state: int        # Destination state s'\n",
                "    visit_count: float      # Expected visit count (can be fractional)\n",
                "    time_angle: float       # Time angle when action was taken\n",
                "    day_seq: int           # Day sequence number\n",
                "    record_date: int       # Date (YYYYMMDD)\n",
                "    strength: float = 1.0  # Memory strength (for decay)\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# SR Memory: Stores and manages Successor Representation\n",
                "# =============================================================================\n",
                "\n",
                "class SRMemory:\n",
                "    \"\"\"\n",
                "    Memory structure for Successor Representation.\n",
                "    \n",
                "    Key difference from EpisodicMemory:\n",
                "    - EpisodicMemory: stores Q-values (cumulative rewards)\n",
                "    - SRMemory: stores visit counts to future states\n",
                "    \n",
                "    Structure:\n",
                "    - SR[state][action] = List[SRRecord] (all visits to various future states)\n",
                "    - R[state] = Expected reward at state (same as EpisodicMemory)\n",
                "    - Memory decay applied when day changes (phi)\n",
                "    - Time similarity kernel (sigma_t) for generalization to unknown times\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, phi: float, config: Optional[SRDynaConfig] = None):\n",
                "        self.config = config if config is not None else SRDynaConfig()\n",
                "        self.phi = phi\n",
                "        \n",
                "        # SR matrices: SR[state][action] = List[SRRecord]\n",
                "        # Each record stores: (future_state, visit_count, time_angle, ...)\n",
                "        self.SR: Dict[int, Dict[int, List[SRRecord]]] = defaultdict(\n",
                "            lambda: defaultdict(list)\n",
                "        )\n",
                "        \n",
                "        # Decay factors: similar to EpisodicMemory.Q_decay\n",
                "        self.SR_decay: Dict[int, Dict[int, float]] = defaultdict(\n",
                "            lambda: defaultdict(float)\n",
                "        )\n",
                "        \n",
                "        # Expected rewards: R[state] = expected reward at state\n",
                "        self.R: Dict[int, float] = defaultdict(float)\n",
                "        \n",
                "        # Visit counts and memory strengths (same as EpisodicMemory)\n",
                "        self.visit_counts: Dict[int, int] = defaultdict(int)\n",
                "        self.memory_strength: Dict[int, float] = defaultdict(float)\n",
                "        \n",
                "        # Active states (above threshold)\n",
                "        self._active_states: Optional[Set[int]] = None\n",
                "        self.last_day: Optional[int] = None\n",
                "\n",
                "    def add_transition(self, state: int, action: int, future_state: int,\n",
                "                       reward: float, is_planning: bool = False):\n",
                "        \"\"\"\n",
                "        Add a transition to SR memory (increment visit count).\n",
                "        \n",
                "        Similar to EpisodicMemory.add_record, but instead of storing Q-value,\n",
                "        we store/increment the visit count to the future state.\n",
                "        \n",
                "        Parameters:\n",
                "        - state: Current state s\n",
                "        - action: Action taken a\n",
                "        - future_state: Resulting state s'\n",
                "        - reward: Immediate reward R(s')\n",
                "        - is_planning: Whether this is a planning (simulated) update\n",
                "        \"\"\"\n",
                "        # Find existing record for this (future_state, time_bucket) if any\n",
                "        # For simplicity, we just append new records (non-parametric approach)\n",
                "        \n",
                "        if future_state > 0:  # Only valid states (not noise/outside)\n",
                "            # Create new SR record\n",
                "            # Note: time_angle and day_seq would be passed from caller\n",
                "            # For now, we'll handle time in retrieve_sr\n",
                "            pass  # Will be handled in add_record method\n",
                "        \n",
                "        # Update expected reward (exponential moving average)\n",
                "        if state > 0:\n",
                "            alpha = self.config.alpha_init if not is_planning else self.config.alpha_plan\n",
                "            current_R = self.R.get(state, 0.0)\n",
                "            self.R[state] = current_R + alpha * (reward - current_R)\n",
                "        \n",
                "        # Update visit counts and memory strength\n",
                "        if state > 0:\n",
                "            self.visit_counts[state] += 1\n",
                "            if self.visit_counts[state] >= self.config.visit_threshold:\n",
                "                self.memory_strength[state] = self.memory_strength.get(state, 0.0) + 1.0\n",
                "        \n",
                "        self._active_states = None\n",
                "    \n",
                "    def add_record(self, state: int, action: int, future_state: int,\n",
                "                   time_angle: float, day_seq: int, record_date: int,\n",
                "                   visit_count: float = 1.0):\n",
                "        \"\"\"\n",
                "        Add an SR record to memory (similar to EpisodicMemory.add_record).\n",
                "        \n",
                "        Parameters:\n",
                "        - state: Current state s\n",
                "        - action: Action taken a  \n",
                "        - future_state: Destination state s'\n",
                "        - time_angle: Time when action was taken\n",
                "        - day_seq: Day sequence number\n",
                "        - record_date: Date\n",
                "        - visit_count: Visit count to increment\n",
                "        \"\"\"\n",
                "        if future_state <= 0:\n",
                "            return\n",
                "            \n",
                "        rec = SRRecord(\n",
                "            future_state=future_state,\n",
                "            visit_count=visit_count,\n",
                "            time_angle=time_angle,\n",
                "            day_seq=day_seq,\n",
                "            record_date=record_date,\n",
                "            strength=1.0,\n",
                "        )\n",
                "        self.SR[state][action].append(rec)\n",
                "        self.SR_decay[state][action] = 1.0\n",
                "        self._active_states = None\n",
                "\n",
                "    @staticmethod\n",
                "    def compute_time_similarity(t1: float, t2: float, sigma_t: float) -> float:\n",
                "        \"\"\"Circular Gaussian kernel on normalized time angle [0, 1).\"\"\"\n",
                "        diff = abs(float(t1) - float(t2))\n",
                "        sigma = max(float(sigma_t), 1e-6)\n",
                "        return float(np.exp(-0.5 * (diff / sigma) ** 2))\n",
                "\n",
                "    def retrieve_sr(self, state: int, action: int, target_time: float,\n",
                "                   sigma_t: Optional[float] = None) -> Dict[int, float]:\n",
                "        \"\"\"\n",
                "        Retrieve SR values for (state, action) at target time.\n",
                "        \n",
                "        Similar to EpisodicMemory.retrieve_q, but returns a dictionary\n",
                "        of future_state -> expected_visit_count\n",
                "        \n",
                "        Returns:\n",
                "        - Dict mapping future_state -> time-weighted visit count\n",
                "        \"\"\"\n",
                "        sigma = self.config.sigma_t_init if sigma_t is None else float(sigma_t)\n",
                "        records = self.SR[int(state)][int(action)]\n",
                "        \n",
                "        if not records:\n",
                "            return {}\n",
                "        \n",
                "        # Aggregate visit counts by future state, weighted by time similarity\n",
                "        sr_values: Dict[int, float] = defaultdict(float)\n",
                "        \n",
                "        for rec in records:\n",
                "            time_sim = self.compute_time_similarity(target_time, rec.time_angle, sigma)\n",
                "            weight = time_sim * rec.strength\n",
                "            sr_values[rec.future_state] += rec.visit_count * weight\n",
                "        \n",
                "        # Apply decay\n",
                "        decay = self.SR_decay[int(state)][int(action)]\n",
                "        for s in sr_values:\n",
                "            sr_values[s] *= decay\n",
                "        \n",
                "        return dict(sr_values)\n",
                "\n",
                "    def compute_Q(self, state: int, action: int, target_time: float,\n",
                "                 sigma_t: Optional[float] = None) -> float:\n",
                "        \"\"\"\n",
                "        Compute Q(s, a) using Successor Representation.\n",
                "        \n",
                "        Q(s, a) = Σ_{s'} SR[s,a][s'] * R(s')\n",
                "        \n",
                "        Parameters:\n",
                "        - state: Current state s\n",
                "        - action: Action a\n",
                "        - target_time: Time for SR retrieval\n",
                "        - sigma_t: Time similarity kernel width\n",
                "        \n",
                "        Returns:\n",
                "        - Q-value estimate\n",
                "        \"\"\"\n",
                "        sigma = self.config.sigma_t_init if sigma_t is None else float(sigma_t)\n",
                "        \n",
                "        # Get SR values (visit counts to each future state)\n",
                "        sr_values = self.retrieve_sr(state, action, target_time, sigma)\n",
                "        \n",
                "        # Compute Q as weighted sum of rewards\n",
                "        q_value = 0.0\n",
                "        for future_state, sr_count in sr_values.items():\n",
                "            reward_at_future = self.R.get(future_state, 0.0)\n",
                "            q_value += sr_count * reward_at_future\n",
                "        \n",
                "        return q_value\n",
                "\n",
                "    def decay(self, current_day: int):\n",
                "        \"\"\"Apply memory decay when day changes (same as EpisodicMemory).\"\"\"\n",
                "        if self.last_day is None:\n",
                "            self.last_day = current_day\n",
                "            return\n",
                "        \n",
                "        day_diff = int(current_day - self.last_day)\n",
                "        if day_diff > 0:\n",
                "            factor = (1.0 - self.phi) ** day_diff\n",
                "            \n",
                "            # Decay all SR records\n",
                "            for state in self.SR:\n",
                "                for action in self.SR[state]:\n",
                "                    for rec in self.SR[state][action]:\n",
                "                        rec.strength *= factor\n",
                "            \n",
                "            # Decay SR decay factors\n",
                "            for state in self.SR_decay:\n",
                "                for action in self.SR_decay[state]:\n",
                "                    self.SR_decay[state][action] *= factor\n",
                "            \n",
                "            # Decay rewards\n",
                "            for state in self.R:\n",
                "                self.R[state] *= factor\n",
                "            \n",
                "            # Decay memory strengths\n",
                "            for state in self.memory_strength:\n",
                "                self.memory_strength[state] *= factor\n",
                "            \n",
                "            self._active_states = None\n",
                "        \n",
                "        self.last_day = current_day\n",
                "\n",
                "    def get_active_states(self) -> Set[int]:\n",
                "        \"\"\"Get states with memory strength above threshold.\"\"\"\n",
                "        if self._active_states is not None:\n",
                "            return self._active_states\n",
                "        \n",
                "        self._active_states = {\n",
                "            state_id\n",
                "            for state_id, strength in self.memory_strength.items()\n",
                "            if strength >= self.config.memory_threshold\n",
                "            and self.visit_counts.get(state_id, 0) >= self.config.visit_threshold\n",
                "        }\n",
                "        return self._active_states\n",
                "\n",
                "    def simulate_step(self, current_state: int, target_time: float) -> Tuple[int, int, float]:\n",
                "        \"\"\"\n",
                "        Simulate one step using learned SR model (for Dyna planning).\n",
                "        \n",
                "        Uses the SR matrix to sample a future state based on visit frequencies.\n",
                "        \n",
                "        Returns:\n",
                "        - (next_state, action, reward)\n",
                "        \"\"\"\n",
                "        active_states = self.get_active_states()\n",
                "        \n",
                "        if current_state not in self.SR or not self.SR[current_state]:\n",
                "            return (-1, -1, 0.0)  # No knowledge: exploration\n",
                "        \n",
                "        # Get all actions with records\n",
                "        actions_with_records = list(self.SR[current_state].keys())\n",
                "        \n",
                "        if not actions_with_records:\n",
                "            return (-1, -1, 0.0)\n",
                "        \n",
                "        # For each action, compute total visit count to active states\n",
                "        action_weights = {}\n",
                "        total_weight = 0.0\n",
                "        \n",
                "        for action in actions_with_records:\n",
                "            sr_values = self.retrieve_sr(current_state, action, target_time)\n",
                "            # Weight = sum of visit counts to active states\n",
                "            weight = sum(\n",
                "                sr_values.get(s, 0.0)\n",
                "                for s in sr_values.keys()\n",
                "                if s in active_states or s <= 0\n",
                "            )\n",
                "            if weight > 0:\n",
                "                action_weights[action] = weight\n",
                "                total_weight += weight\n",
                "        \n",
                "        if total_weight <= 0:\n",
                "            return (-1, -1, 0.0)\n",
                "        \n",
                "        # Sample action based on weights\n",
                "        actions = list(action_weights.keys())\n",
                "        probs = [action_weights[a] / total_weight for a in actions]\n",
                "        action = actions[np.random.choice(len(actions), p=probs)]\n",
                "        \n",
                "        # Sample next state based on SR values\n",
                "        sr_values = self.retrieve_sr(current_state, action, target_time)\n",
                "        \n",
                "        if not sr_values:\n",
                "            return (action, action, 0.0)\n",
                "        \n",
                "        # Normalize to probability distribution\n",
                "        states = list(sr_values.keys())\n",
                "        counts = [sr_values[s] for s in states]\n",
                "        total = sum(counts)\n",
                "        \n",
                "        if total <= 0:\n",
                "            return (action, action, 0.0)\n",
                "        \n",
                "        state_probs = [c / total for c in counts]\n",
                "        next_state = states[np.random.choice(len(states), p=state_probs)]\n",
                "        \n",
                "        reward = self.R.get(next_state, 0.0)\n",
                "        \n",
                "        return (next_state, action, reward)\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Data Preparation for SR Dyna\n",
                "# =============================================================================\n",
                "\n",
                "def prepare_sr_dyna_data(user_df: pd.DataFrame,\n",
                "                        config: Optional[SRDynaConfig] = None) -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Prepare trajectory data for SR Dyna modeling.\n",
                "    \"\"\"\n",
                "    if config is None:\n",
                "        config = SRDynaConfig()\n",
                "    \n",
                "    df = user_df.sort_values('t_start').reset_index(drop=True)\n",
                "    n_records = len(df)\n",
                "    \n",
                "    # Extract arrays\n",
                "    states = df['cluster_id'].astype(int).to_numpy()\n",
                "    date_array = df['date'].to_numpy()\n",
                "    \n",
                "    # Compute time angles\n",
                "    time_angles = np.array([compute_time_angle(dt) for dt in df['t_end']])\n",
                "    \n",
                "    # Compute day sequence\n",
                "    day_seq = compute_day_sequence(date_array, config.ref_date[0], \n",
                "                                   config.ref_date[1], config.ref_date[2])\n",
                "    \n",
                "    # Compute stay durations and rewards\n",
                "    stay_minutes = (df['t_end'] - df['t_start']).dt.total_seconds() / 60.0\n",
                "    stay_minutes = stay_minutes.to_numpy()\n",
                "    stay_minutes = np.roll(stay_minutes, -1)\n",
                "    stay_minutes[-1] = 0.0\n",
                "    \n",
                "    reward_array = compute_reward_array(stay_minutes, config.reward_type, \n",
                "                                        config.reward_param_init)\n",
                "    \n",
                "    # Compute actions\n",
                "    actions = np.zeros(n_records, dtype=int)\n",
                "    actions[-1] = -9\n",
                "    for t in range(n_records - 1):\n",
                "        if day_seq[t + 1] > day_seq[t]:\n",
                "            actions[t] = -9  # End of day\n",
                "        else:\n",
                "            actions[t] = int(states[t + 1])\n",
                "    \n",
                "    # Same-day indicator\n",
                "    same_day_next = np.zeros(n_records, dtype=bool)\n",
                "    for t in range(n_records - 1):\n",
                "        same_day_next[t] = (day_seq[t] == day_seq[t + 1])\n",
                "    \n",
                "    return {\n",
                "        'states': states,\n",
                "        'actions': actions,\n",
                "        'day_seq': day_seq,\n",
                "        'time_angles': time_angles,\n",
                "        'date_array': date_array,\n",
                "        'reward_array': reward_array,\n",
                "        'same_day_next': same_day_next,\n",
                "        'n_records': n_records,\n",
                "    }\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Parameter Handling\n",
                "# =============================================================================\n",
                "\n",
                "def unpack_params_sr_dyna(theta: np.ndarray) -> Dict[str, float]:\n",
                "    \"\"\"\n",
                "    Unpack SR Dyna parameters from optimization vector.\n",
                "    \n",
                "    Following Simple MF parameterization:\n",
                "    - alpha = sigmoid(logit_alpha) ∈ (0, 1)  # 使用 logit 变换\n",
                "    - beta = exp(log_beta) ∈ (0, +inf)       # 使用 log 变换\n",
                "    - epsilon = sigmoid(logit_epsilon) ∈ (0, 1)  # 使用 logit 变换\n",
                "    - phi = sigmoid(logit_phi) ∈ (0, 1)          # 使用 logit 变换\n",
                "    \"\"\"\n",
                "    idx = 0\n",
                "    \n",
                "    # Learning rate: alpha = sigmoid(logit_alpha) ∈ (0, 1)\n",
                "    logit_alpha = theta[idx]; idx += 1\n",
                "    alpha = 1.0 / (1.0 + np.exp(-logit_alpha))\n",
                "    \n",
                "    # Softmax temperature: beta = exp(log_beta) ∈ (0, +inf)\n",
                "    log_beta = theta[idx]; idx += 1\n",
                "    beta = np.exp(log_beta)\n",
                "    \n",
                "    # Exploration: epsilon = sigmoid(logit_epsilon) ∈ (0, 1)\n",
                "    logit_epsilon = theta[idx]; idx += 1\n",
                "    epsilon = 1.0 / (1.0 + np.exp(-logit_epsilon))\n",
                "    \n",
                "    # Forgetting: phi = sigmoid(logit_phi) ∈ (0, 1)\n",
                "    logit_phi = theta[idx]; idx += 1\n",
                "    phi = 1.0 / (1.0 + np.exp(-logit_phi))\n",
                "    \n",
                "    # Clip values\n",
                "    alpha = float(np.clip(alpha, 1e-6, 1.0 - 1e-6))\n",
                "    epsilon = float(np.clip(epsilon, 1e-6, 1.0 - 1e-6))\n",
                "    phi = float(np.clip(phi, 1e-6, 1.0 - 1e-6))\n",
                "    \n",
                "    return {\n",
                "        'alpha': alpha,\n",
                "        'beta': float(beta),\n",
                "        'epsilon': epsilon,\n",
                "        'phi': phi,\n",
                "    }\n",
                "\n",
                "\n",
                "def pack_params_sr_dyna(alpha: float, beta: float, epsilon: float, phi: float) -> np.ndarray:\n",
                "    \"\"\"Pack SR Dyna parameters into optimization vector (following Simple MF).\"\"\"\n",
                "    return np.array([\n",
                "        np.log(alpha / (1.0 - alpha)),   # logit_alpha\n",
                "        np.log(beta),                      # log_beta\n",
                "        np.log(epsilon / (1.0 - epsilon)),  # logit_epsilon\n",
                "        np.log(phi / (1.0 - phi)),         # logit_phi\n",
                "    ], dtype=np.float64)\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# SR Dyna Simulation and Log-Likelihood\n",
                "# =============================================================================\n",
                "\n",
                "def simulate_and_loglik_sr_dyna(theta: np.ndarray,\n",
                "                                sr_data: Dict[str, Any],\n",
                "                                config: Optional[SRDynaConfig] = None) -> float:\n",
                "    \"\"\"\n",
                "    Compute negative log-likelihood for SR Dyna model.\n",
                "    \n",
                "    Algorithm:\n",
                "    1. At each step t:\n",
                "       a. Get current state and time\n",
                "       b. Apply memory decay if new day\n",
                "       c. Compute Q-values using SR: Q(s,a) = SR[s,a] · R\n",
                "       d. Sample action using softmax policy (with epsilon exploration)\n",
                "       e. Add SR record from real transition\n",
                "       f. Perform Dyna planning: simulate N steps, update SR\n",
                "       g. Accumulate log-likelihood\n",
                "    \"\"\"\n",
                "    if config is None:\n",
                "        config = SRDynaConfig()\n",
                "    \n",
                "    # Unpack parameters\n",
                "    params = unpack_params_sr_dyna(theta)\n",
                "    alpha = params['alpha']\n",
                "    beta = params['beta']\n",
                "    epsilon = params['epsilon']\n",
                "    phi = params['phi']\n",
                "    \n",
                "    # Extract data\n",
                "    states = sr_data['states']\n",
                "    actions = sr_data['actions']\n",
                "    day_seq = sr_data['day_seq']\n",
                "    time_angles = sr_data['time_angles']\n",
                "    date_array = sr_data['date_array']\n",
                "    reward_array = sr_data['reward_array']\n",
                "    same_day_next = sr_data['same_day_next']\n",
                "    n_records = sr_data['n_records']\n",
                "    \n",
                "    # Initialize SR memory\n",
                "    memory = SRMemory(phi, config)\n",
                "    \n",
                "    # Track known states and actions\n",
                "    known_states: Set[int] = {-1, 0}\n",
                "    known_actions: Set[int] = {-9, -1, 0}\n",
                "    \n",
                "    loglik = 0.0\n",
                "    \n",
                "    for t in range(n_records):\n",
                "        s = int(states[t])\n",
                "        a = int(actions[t])\n",
                "        r_t = float(reward_array[t])\n",
                "        current_day = int(day_seq[t])\n",
                "        current_date = int(date_array[t])\n",
                "        time_angle = float(time_angles[t])\n",
                "        \n",
                "        # Apply decay if new day\n",
                "        memory.decay(current_day)\n",
                "        \n",
                "        # Update known sets\n",
                "        if a > 0:\n",
                "            known_states.add(a)\n",
                "            known_actions.add(a)\n",
                "        \n",
                "        s_perc = s if s in known_states else -1\n",
                "        a_perc = a if a in known_actions else -1\n",
                "        \n",
                "        # Get available actions\n",
                "        available_actions = sorted([act for act in known_actions if act != -1])\n",
                "        \n",
                "        # Compute Q-values using SR\n",
                "        q_values = []\n",
                "        for act in available_actions:\n",
                "            q = memory.compute_Q(s_perc, act, time_angle, config.sigma_t_init)\n",
                "            q_values.append(q)\n",
                "        \n",
                "        q_values = np.asarray(q_values, dtype=np.float64)\n",
                "        \n",
                "        # Softmax policy\n",
                "        if len(q_values) > 0:\n",
                "            logits = beta * q_values\n",
                "            logits -= np.max(logits)\n",
                "            softmax = np.exp(logits)\n",
                "            softmax /= (np.sum(softmax) + 1e-12)\n",
                "        else:\n",
                "            softmax = np.array([1.0])\n",
                "        \n",
                "        # Map action to probability\n",
                "        if a_perc in available_actions:\n",
                "            idx_a = available_actions.index(a_perc)\n",
                "            if idx_a < len(softmax):\n",
                "                action_prob = (1.0 - epsilon) * softmax[idx_a]\n",
                "            else:\n",
                "                action_prob = epsilon / len(available_actions) if available_actions else epsilon\n",
                "        elif a_perc == -1:\n",
                "            action_prob = epsilon\n",
                "        else:\n",
                "            action_prob = epsilon / len(available_actions) if available_actions else epsilon\n",
                "        \n",
                "        loglik += np.log(action_prob + 1e-12)\n",
                "        \n",
                "        # Add SR record from real transition\n",
                "        if t < n_records - 1:\n",
                "            a_next = int(actions[t + 1])\n",
                "            a_next_perc = a_next if a_next in known_actions else -1\n",
                "            \n",
                "            # Get next state\n",
                "            if same_day_next[t]:\n",
                "                next_state = int(states[t + 1])\n",
                "            else:\n",
                "                next_state = -9  # End of day\n",
                "            \n",
                "            # Add SR record (increment visit count)\n",
                "            if a_perc != -1 and next_state > 0:\n",
                "                memory.add_record(\n",
                "                    state=s_perc,\n",
                "                    action=a_perc,\n",
                "                    future_state=next_state,\n",
                "                    time_angle=time_angle,\n",
                "                    day_seq=current_day,\n",
                "                    record_date=current_date,\n",
                "                    visit_count=1.0\n",
                "                )\n",
                "                # Update reward estimate\n",
                "                memory.add_transition(s_perc, a_perc, next_state, r_t, is_planning=False)\n",
                "            \n",
                "            # Dyna planning: simulate additional steps\n",
                "            for _ in range(config.n_planning_steps):\n",
                "                # Random start state from visited states\n",
                "                if len(memory.visit_counts) > 0:\n",
                "                    visited_states = list(memory.visit_counts.keys())\n",
                "                    counts = list(memory.visit_counts.values())\n",
                "                    total = sum(counts)\n",
                "                    probs = [c/total for c in counts]\n",
                "                    plan_state = np.random.choice(visited_states, p=probs)\n",
                "                else:\n",
                "                    plan_state = s_perc\n",
                "                \n",
                "                # Simulate step (need to provide time - use current time for simplicity)\n",
                "                plan_next_state, plan_action, plan_reward = memory.simulate_step(plan_state, time_angle)\n",
                "                \n",
                "                if plan_next_state > 0 and plan_action > 0:\n",
                "                    memory.add_record(\n",
                "                        state=plan_state,\n",
                "                        action=plan_action,\n",
                "                        future_state=plan_next_state,\n",
                "                        time_angle=time_angle,\n",
                "                        day_seq=current_day,\n",
                "                        record_date=current_date,\n",
                "                        visit_count=0.5 * alpha  # Smaller weight for planning\n",
                "                    )\n",
                "                    memory.add_transition(plan_state, plan_action, plan_next_state, \n",
                "                                         plan_reward, is_planning=True)\n",
                "    \n",
                "    return -loglik\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Model Fitting\n",
                "# =============================================================================\n",
                "\n",
                "def fit_sr_dyna_model(user_df: pd.DataFrame,\n",
                "                      config: Optional[SRDynaConfig] = None,\n",
                "                      verbose: bool = True) -> Dict[str, Any]:\n",
                "    \"\"\"Fit SR Dyna model for one user.\"\"\"\n",
                "    if config is None:\n",
                "        config = SRDynaConfig()\n",
                "    \n",
                "    # Prepare data\n",
                "    sr_data = prepare_sr_dyna_data(user_df, config)\n",
                "    n_records = sr_data['n_records']\n",
                "    \n",
                "    if n_records < 2:\n",
                "        return {\n",
                "            'n_records': int(n_records),\n",
                "            'log_likelihood': np.nan,\n",
                "            'AIC': np.nan,\n",
                "            'BIC': np.nan,\n",
                "            'alpha': np.nan,\n",
                "            'beta': np.nan,\n",
                "            'epsilon': np.nan,\n",
                "            'phi': np.nan,\n",
                "            'converged': False,\n",
                "            'n_iterations': 0,\n",
                "            'optimization_message': 'Insufficient records',\n",
                "        }\n",
                "    \n",
                "    # Initial parameters\n",
                "    theta_init = pack_params_sr_dyna(\n",
                "        alpha=config.alpha_init,\n",
                "        beta=config.beta_init,\n",
                "        epsilon=config.epsilon_init,\n",
                "        phi=config.phi_init,\n",
                "    )\n",
                "    \n",
                "    # Optimize (no bounds - following Simple MF approach for better optimization)\n",
                "    with warnings.catch_warnings():\n",
                "        warnings.simplefilter('ignore')\n",
                "        result = minimize(\n",
                "            simulate_and_loglik_sr_dyna,\n",
                "            theta_init,\n",
                "            args=(sr_data, config),\n",
                "            method='L-BFGS-B',\n",
                "            options={'maxiter': config.maxiter, 'ftol': config.ftol, 'disp': False},\n",
                "        )\n",
                "    \n",
                "    # Extract results\n",
                "    fitted = unpack_params_sr_dyna(result.x)\n",
                "    log_likelihood = -float(result.fun)\n",
                "    \n",
                "    k_params = 4  # alpha, beta, epsilon, phi (no gamma)\n",
                "    aic = 2 * k_params - 2 * log_likelihood\n",
                "    bic = k_params * np.log(max(n_records, 1)) - 2 * log_likelihood\n",
                "    \n",
                "    summary = {\n",
                "        'n_records': int(n_records),\n",
                "        'log_likelihood': float(log_likelihood),\n",
                "        'AIC': float(aic),\n",
                "        'BIC': float(bic),\n",
                "        'alpha': float(fitted['alpha']),\n",
                "        'beta': float(fitted['beta']),\n",
                "        'epsilon': float(fitted['epsilon']),\n",
                "        'phi': float(fitted['phi']),\n",
                "        'converged': bool(result.success),\n",
                "        'n_iterations': int(result.nit),\n",
                "        'optimization_message': str(result.message),\n",
                "    }\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"SR Dyna model fitting {'converged' if result.success else 'did not converge'}.\")\n",
                "        print(f\"  Log-likelihood: {log_likelihood:.2f}, AIC: {aic:.2f}, BIC: {bic:.2f}\")\n",
                "        print(f\"  alpha (SR rate): {fitted['alpha']:.4f}\")\n",
                "        print(f\"  beta (softmax temp): {fitted['beta']:.4f}\")\n",
                "        print(f\"  epsilon (explore): {fitted['epsilon']:.4f}\")\n",
                "        print(f\"  phi (forgetting): {fitted['phi']:.4f}\")\n",
                "    \n",
                "    return summary\n",
                "\n",
                "\n",
                "def fit_sr_dyna_for_all_users(users_dict: Dict[int, Any],\n",
                "                              config: Optional[SRDynaConfig] = None,\n",
                "                              sample_size: Optional[int] = None,\n",
                "                              verbose: bool = True) -> pd.DataFrame:\n",
                "    \"\"\"Fit SR Dyna model for all users.\"\"\"\n",
                "    if config is None:\n",
                "        config = SRDynaConfig()\n",
                "    \n",
                "    user_ids = list(users_dict.keys())\n",
                "    if sample_size is not None:\n",
                "        user_ids = user_ids[:sample_size]\n",
                "    \n",
                "    results = []\n",
                "    for i, user_id in enumerate(user_ids):\n",
                "        if verbose:\n",
                "            print(f\"[{i+1}/{len(user_ids)}] Fitting SR Dyna for user {user_id}...\", end='')\n",
                "        \n",
                "        user = users_dict[user_id]\n",
                "        user_df = user.to_dataframe()\n",
                "        \n",
                "        try:\n",
                "            t0 = time.time()\n",
                "            result = fit_sr_dyna_model(user_df, config, verbose=False)\n",
                "            elapsed = time.time() - t0\n",
                "            if verbose:\n",
                "                print(f\"\\tUser {user_id} SR Dyna fit time: {elapsed:.2f}s\")\n",
                "            result['user_id'] = user_id\n",
                "            result['fit_time_seconds'] = elapsed\n",
                "            results.append(result)\n",
                "        except Exception as e:\n",
                "            if verbose:\n",
                "                print(f\"  Error: {e}\")\n",
                "            results.append({\n",
                "                'user_id': user_id,\n",
                "                'n_records': 0,\n",
                "                'log_likelihood': np.nan,\n",
                "                'AIC': np.nan,\n",
                "                'BIC': np.nan,\n",
                "                'alpha': np.nan,\n",
                "                'beta': np.nan,\n",
                "                'epsilon': np.nan,\n",
                "                'phi': np.nan,\n",
                "                'converged': False,\n",
                "                'n_iterations': None,\n",
                "                'optimization_message': str(e),\n",
                "                'fit_time_seconds': np.nan,\n",
                "            })\n",
                "    \n",
                "    return pd.DataFrame(results)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "geosp_cog_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
